{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45n3pZxX0UGt"
   },
   "source": [
    "# 9장 다중 분류 (Mulitnomial classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOaBpx5m0UGu"
   },
   "source": [
    "* \"부록3 매트플롯립 입문\"에서 한글 폰트를 올바르게 출력하기 위한 설치 방법을 설명했다. 설치 방법은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5244,
     "status": "ok",
     "timestamp": 1739156945734,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "TCSqLIo70UGu",
    "outputId": "8ed999e7-f59c-485c-e6b7-3908c09b2b49"
   },
   "outputs": [],
   "source": [
    "# 한글 폰트 설치\n",
    "\n",
    "# !sudo apt-get install -y fonts-nanum* | tail -n 1\n",
    "# !sudo fc-cache -fv\n",
    "# !rm -rf ~/.cache/matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5039,
     "status": "ok",
     "timestamp": 1739156950774,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "DwFpV0Ty0UGv",
    "outputId": "748faaf4-8805-473a-db65-df5a18a6fa75"
   },
   "outputs": [],
   "source": [
    "# 필요 라이브러리 설치\n",
    "\n",
    "# !pip install torchviz | tail -n 1\n",
    "# !pip install torchinfo | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuGAXGlc0UGv"
   },
   "source": [
    "* 모든 설치가 끝나면 한글 폰트를 바르게 출력하기 위해 **[런타임]** -> **[런타임 다시시작]**을 클릭한 다음, 아래 셀부터 코드를 실행해 주십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1739156964537,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "unQ90AL30UGw"
   },
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# 폰트 관련 용도\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Colab, Linux\n",
    "# 나눔 고딕 폰트의 경로 명시\n",
    "path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
    "font_name = fm.FontProperties(fname=path, size=10).get_name()\n",
    "\n",
    "# Window\n",
    "# font_name = \"NanumBarunGothic\"\n",
    "\n",
    "# Mac\n",
    "# font_name = \"AppleGothic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5836,
     "status": "ok",
     "timestamp": 1739156970374,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "oSnLCyDj0UGw"
   },
   "outputs": [],
   "source": [
    "# 파이토치 관련 라이브러리\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot\n",
    "from torchinfo import summary\n",
    "\n",
    "# Iris dataset\n",
    "import pandas  as pd\n",
    "# from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1739156970376,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "5hD2Kk3X0UGw"
   },
   "outputs": [],
   "source": [
    "# 기본 폰트 설정\n",
    "plt.rcParams['font.family'] = font_name\n",
    "\n",
    "# 기본 폰트 사이즈 변경\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# 기본 그래프 사이즈 변경\n",
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "# 기본 그리드 표시\n",
    "# 필요에 따라 설정할 때는, plt.grid()\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams[\"grid.linestyle\"] = \":\"\n",
    "\n",
    "# 마이너스 기호 정상 출력\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 넘파이 부동소수점 자릿수 표시\n",
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739156970380,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "CQH1RTNs0UGx"
   },
   "outputs": [],
   "source": [
    "# warning 표시 끄기\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5Vz8qB_0UGx"
   },
   "source": [
    "## Iris data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IerfeClh0UGx"
   },
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1739156970409,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "rdFrlFf60UGx",
    "outputId": "0180f621-c5d8-4d21-be33-7ee2ee417be6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris = \n",
      " {'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. dropdown:: References\\n\\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n    Mathematical Statistics\" (John Wiley, NY, 1950).\\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n    Structure and Classification Rule for Recognition in Partially Exposed\\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n    on Information Theory, May 1972, 431-433.\\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n    conceptual clustering system finds 3 classes in the data.\\n  - Many, many more ...\\n', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n",
      "iris keys = \n",
      " dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "target_names = \n",
      " ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# 학습용 데이터 준비\n",
    "\n",
    "# 라이브러리 임포트\n",
    "# from sklearn.datasets import load_iris\n",
    "\n",
    "# 데이터 불러오기\n",
    "iris = load_iris()\n",
    "print(\"iris = \\n\", iris)\n",
    "print('iris keys = \\n', iris.keys())\n",
    "print(\"target_names = \\n\", iris[\"target_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739156970416,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "aBItsOzc0UGy",
    "outputId": "e6860c7a-1ec5-474f-9fbb-9df82011ec3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터 타입 : <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "원본 데이터 크기 : (150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "# 입력 데이터와 정답 데이터\n",
    "x_org, y_org = iris.data, iris.target\n",
    "\n",
    "# 결과 확인\n",
    "print('원본 데이터 타입 :', type(x_org), type(y_org))\n",
    "print('원본 데이터 크기 :', x_org.shape, y_org.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QO8Jun3K0UGy"
   },
   "source": [
    "### 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739156970420,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "cdaPNRFs0UGy",
    "outputId": "c92e7bfc-56e6-437b-97ec-c099e14a219d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터 (150, 2) (150,)\n"
     ]
    }
   ],
   "source": [
    "# 입력 데이터로 sepal(꽃받침) length(0)와 petal(꽃잎) length(2)를 추출\n",
    "x_select = x_org[:,[0,2]]\n",
    "\n",
    "# 결과 확인\n",
    "print('원본 데이터', x_select.shape, y_org.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXB9Cocv0UGy"
   },
   "source": [
    "### 훈련 데이터와 검증 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1739156970439,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "iVCErzts0UGy",
    "outputId": "3ec3a1a4-d406-4387-e0e5-9d683320afd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 2) (75, 2) (75,) (75,)\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터와 검증 데이터로 분할(셔플도 동시에 실시함)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_select, y_org, train_size=75, test_size=75,\n",
    "    random_state=123)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "#총 데이터수가 150개 이상이어야 오류 안남 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE36kKJp0UGy"
   },
   "source": [
    "### 훈련 데이터의 산포도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1739156970468,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "B7qsmIKz0UGy"
   },
   "outputs": [],
   "source": [
    "# 데이터를 정답별로 분할\n",
    "\n",
    "x_t0 = x_train[y_train == 0]\n",
    "x_t1 = x_train[y_train == 1]\n",
    "x_t2 = x_train[y_train == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1739156970816,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "L85t42Lj0UGy",
    "outputId": "3d6b1a91-64c4-4059-e68f-ba313f473574"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 산포도 출력\n",
    "\n",
    "plt.scatter(x_t0[:,0], x_t0[:,1], marker='x', c='k', s=50, label='0 (setosa)')\n",
    "plt.scatter(x_t1[:,0], x_t1[:,1], marker='o', c='b', s=50, label='1 (versicolor)')\n",
    "plt.scatter(x_t2[:,0], x_t2[:,1], marker='^', c='r', s=50, label='2 (virginica)')\n",
    "plt.xlabel('sepal_length')\n",
    "plt.ylabel('petal_length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape = torch.Size([75, 2])\n",
      "labels shape = torch.Size([75])\n"
     ]
    }
   ],
   "source": [
    "inputs =torch.tensor(x_train).float()\n",
    "labels = torch.tensor(y_train).long()\n",
    "\n",
    "print(\"inputs shape =\",inputs.shape)\n",
    "print(\"labels shape =\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQctuvIC0UGz"
   },
   "source": [
    "### 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739156970821,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "i5kTXwyM0UGz",
    "outputId": "f26b8c6b-52a2-49da-af56-079d26b89135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_input: 2  n_output: 3\n"
     ]
    }
   ],
   "source": [
    "# 학습용 파라미터 설정\n",
    "\n",
    "# 입력 차원수\n",
    "n_input = x_train.shape[1]\n",
    "\n",
    "# 출력 차원수\n",
    "# 분류 클래스 수, 여기서는 3\n",
    "n_output = len(list(set(y_train)))\n",
    "\n",
    "# 결과 확인\n",
    "print(f'n_input: {n_input}  n_output: {n_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1739156970851,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "IJQbr8Ti0UGz"
   },
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "# 2입력 3출력 로지스틱 회귀 모델\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_input, n_output)\n",
    "        #self.softmax = nn.Softmax() #사용하지 않는 이유 알기 \n",
    "\n",
    "        # 초깃값을 모두 1로 함\n",
    "        # \"딥러닝을 위한 수학\"과 조건을 맞추기 위한 목적\n",
    "        self.l1.weight.data.fill_(1.0)\n",
    "        self.l1.bias.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.l1(x)\n",
    "        return x1\n",
    "\n",
    "# 인스턴스 생성\n",
    "net = Net(n_input, n_output)\n",
    "# list(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MZvuZ2j0UGz"
   },
   "source": [
    "### 모델 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1739156970979,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "EA4M3I8X0UGz",
    "outputId": "cb18b9c0-eb57-4179-b725-a5ea729afe39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l1.weight', Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], requires_grad=True))\n",
      "('l1.bias', Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# 모델 내부 파라미터 확인\n",
    "# l1.weight는 행렬, l1.bias는 벡터\n",
    "\n",
    "for parameter in net.named_parameters(): #MSELoss \n",
    "    print(parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739156970983,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "i4cFCiFo0UGz",
    "outputId": "b8244f9d-0993-4609-a074-103794452d55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (l1): Linear(in_features=2, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 개요 표시 1\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1739156971016,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "mtruwe5w0UGz",
    "outputId": "75d7259a-418b-41d9-9f29-c50c099a684e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Net                                      [100, 3]                  --\n",
       "├─Linear: 1-1                            [100, 3]                  9\n",
       "==========================================================================================\n",
       "Total params: 9\n",
       "Trainable params: 9\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 개요 표시 2\n",
    "\n",
    "summary(net, (100,2), device = 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xkr3R_gs0UG0"
   },
   "source": [
    "### 최적화 알고리즘과 손실 함수의 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 5514,
     "status": "ok",
     "timestamp": 1739156976544,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "ReX2VjQj0UG0"
   },
   "outputs": [],
   "source": [
    "# 손실 함수： 교차 엔트로피 함수\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 학습률\n",
    "lr = 0.01\n",
    "\n",
    "# 최적화 함수: 경사 하강법\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape=  torch.Size([75, 2])\n",
      "outputs shape=  torch.Size([75, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"inputs shape= \",inputs.shape)\n",
    "outputs=net(inputs)\n",
    "print(\"outputs shape= \",outputs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzj9K6Yi0UG0"
   },
   "source": [
    "### 경사 하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739156976548,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "4JxBc8Sl0UG0"
   },
   "outputs": [],
   "source": [
    "# 입력 데이터 x_train과 정답 데이터 y_train의 텐서 변수화\n",
    "\n",
    "inputs = torch.tensor(x_train).float()\n",
    "labels = torch.tensor(y_train).long()\n",
    "\n",
    "# 검증 데이터의 텐서 변수화\n",
    "\n",
    "inputs_test = torch.tensor(x_test).float()\n",
    "labels_test = torch.tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsn6a2qg0UG0"
   },
   "source": [
    "### 손실의 계산 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1739156976711,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "aIvNEv200UG0",
    "outputId": "cc0d0759-d50a-4d14-9197-6167fd34ee99"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"196pt\" height=\"391pt\"\n",
       " viewBox=\"0.00 0.00 196.00 391.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 387)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-387 192,-387 192,4 -4,4\"/>\n",
       "<!-- 128412470812560 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>128412470812560</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"121,-31 67,-31 67,0 121,0 121,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"94\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 128412281758112 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>128412281758112</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"144.5,-86 43.5,-86 43.5,-67 144.5,-67 144.5,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"94\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">NllLossBackward0</text>\n",
       "</g>\n",
       "<!-- 128412281758112&#45;&gt;128412470812560 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>128412281758112&#45;&gt;128412470812560</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M94,-66.79C94,-60.07 94,-50.4 94,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.5,-41.19 94,-31.19 90.5,-41.19 97.5,-41.19\"/>\n",
       "</g>\n",
       "<!-- 128412271818064 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>128412271818064</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"152,-141 36,-141 36,-122 152,-122 152,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"94\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">LogSoftmaxBackward0</text>\n",
       "</g>\n",
       "<!-- 128412271818064&#45;&gt;128412281758112 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>128412271818064&#45;&gt;128412281758112</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M94,-121.75C94,-114.8 94,-104.85 94,-96.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.5,-96.09 94,-86.09 90.5,-96.09 97.5,-96.09\"/>\n",
       "</g>\n",
       "<!-- 128412272065312 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>128412272065312</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"139,-196 49,-196 49,-177 139,-177 139,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"94\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 128412272065312&#45;&gt;128412271818064 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>128412272065312&#45;&gt;128412271818064</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M94,-176.75C94,-169.8 94,-159.85 94,-151.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.5,-151.09 94,-141.09 90.5,-151.09 97.5,-151.09\"/>\n",
       "</g>\n",
       "<!-- 128412272064400 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>128412272064400</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"90,-251 0,-251 0,-232 90,-232 90,-251\"/>\n",
       "<text text-anchor=\"middle\" x=\"45\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 128412272064400&#45;&gt;128412272065312 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>128412272064400&#45;&gt;128412272065312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.09,-231.75C60.16,-224.11 70.59,-212.82 79.15,-203.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.84,-205.81 86.06,-196.09 76.7,-201.06 81.84,-205.81\"/>\n",
       "</g>\n",
       "<!-- 128412477150848 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>128412477150848</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"72,-317 18,-317 18,-287 72,-287 72,-317\"/>\n",
       "<text text-anchor=\"middle\" x=\"45\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\">l1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"45\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n",
       "</g>\n",
       "<!-- 128412477150848&#45;&gt;128412272064400 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>128412477150848&#45;&gt;128412272064400</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45,-286.84C45,-279.21 45,-269.7 45,-261.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"48.5,-261.27 45,-251.27 41.5,-261.27 48.5,-261.27\"/>\n",
       "</g>\n",
       "<!-- 128412272057920 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>128412272057920</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"177.5,-251 108.5,-251 108.5,-232 177.5,-232 177.5,-251\"/>\n",
       "<text text-anchor=\"middle\" x=\"143\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 128412272057920&#45;&gt;128412272065312 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>128412272057920&#45;&gt;128412272065312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M134.91,-231.75C127.84,-224.11 117.41,-212.82 108.85,-203.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"111.3,-201.06 101.94,-196.09 106.16,-205.81 111.3,-201.06\"/>\n",
       "</g>\n",
       "<!-- 128412272063824 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>128412272063824</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"188,-311.5 98,-311.5 98,-292.5 188,-292.5 188,-311.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"143\" y=\"-299.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 128412272063824&#45;&gt;128412272057920 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>128412272063824&#45;&gt;128412272057920</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M143,-292.37C143,-284.25 143,-271.81 143,-261.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"146.5,-261.17 143,-251.17 139.5,-261.17 146.5,-261.17\"/>\n",
       "</g>\n",
       "<!-- 128412434966208 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>128412434966208</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"175,-383 111,-383 111,-353 175,-353 175,-383\"/>\n",
       "<text text-anchor=\"middle\" x=\"143\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\">l1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"143\" y=\"-360\" font-family=\"monospace\" font-size=\"10.00\"> (3, 2)</text>\n",
       "</g>\n",
       "<!-- 128412434966208&#45;&gt;128412272063824 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>128412434966208&#45;&gt;128412272063824</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M143,-352.8C143,-343.7 143,-331.79 143,-321.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"146.5,-321.84 143,-311.84 139.5,-321.84 146.5,-321.84\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x74ca4fdc7710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 예측 계산\n",
    "outputs = net(inputs)\n",
    "\n",
    "# 손실 계산\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "# 손실의 계산 그래프 시각화\n",
    "g = make_dot(loss, params=dict(net.named_parameters()))\n",
    "display(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2bSqlyQ0UG0"
   },
   "source": [
    "### 예측 라벨을 얻는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1739156976768,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "ZMTcXlVY0UG0",
    "outputId": "7b9d9e5d-2e80-4a35-fd2d-5b6f1f739e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([12.0000, 12.7000,  7.6000, 13.0000, 12.3000,  7.6000,  7.3000, 11.1000,\n",
      "        12.1000, 13.3000,  8.0000,  7.0000, 10.3000,  7.6000, 11.7000, 13.3000,\n",
      "         7.4000, 13.5000,  8.2000,  8.4000, 12.7000,  6.6000,  7.9000, 12.2000,\n",
      "        14.6000, 12.0000, 10.2000, 10.5000,  7.1000,  7.3000, 12.6000, 12.7000,\n",
      "         7.4000,  7.7000, 10.8000, 11.5000, 11.5000, 14.0000, 12.8000, 10.8000,\n",
      "        10.8000, 15.2000,  7.5000,  7.8000, 11.1000, 13.6000, 12.9000, 14.2000,\n",
      "        12.7000,  7.6000, 10.9000,  7.0000, 10.9000, 11.2000,  7.4000, 11.7000,\n",
      "        13.3000, 11.5000, 13.4000, 12.7000,  7.7000, 11.8000,  7.0000, 12.6000,\n",
      "        11.7000, 10.9000,  9.2000, 12.2000, 10.4000, 12.1000,  7.5000,  9.1000,\n",
      "        11.1000, 12.0000, 14.3000], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.max 함수 호출\n",
    "# 2번째 인수는 축을 의미함. 1이면 행별로 집계\n",
    "print(torch.max(outputs, 1))\n",
    "# print(torch.argmax(outputs, 1))\n",
    "\n",
    "# 예측 라벨 리스트를 취득\n",
    "torch.max(outputs, 1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqAsbZYN0UG1"
   },
   "source": [
    "### 반복 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1739156976803,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "aqZF-Hh50UG1"
   },
   "outputs": [],
   "source": [
    "# 학습률\n",
    "lr = 0.01\n",
    "\n",
    "# 초기화\n",
    "net = Net(n_input, n_output)\n",
    "\n",
    "# 손실 함수： 교차 엔트로피 함수\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 최적화 함수: 경사 하강법\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# 반복 횟수\n",
    "num_epochs = 10000\n",
    "\n",
    "# 평가 결과 기록\n",
    "history = np.zeros((0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8855,
     "status": "ok",
     "timestamp": 1739156985656,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "8Yyq8rrD0UG1",
    "outputId": "79d10cc8-56ba-47d4-8b8c-8a9ebb835d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10000], train loss: 1.0986 train acc: 0.3067 val_loss: 1.0926, val_acc: 0.2667\n",
      "Epoch [10/10000], train loss: 1.0358 train acc: 0.4000 val_loss: 1.0640, val_acc: 0.2667\n",
      "Epoch [20/10000], train loss: 1.0048 train acc: 0.4000 val_loss: 1.0335, val_acc: 0.2667\n",
      "Epoch [30/10000], train loss: 0.9767 train acc: 0.4000 val_loss: 1.0026, val_acc: 0.2667\n",
      "Epoch [40/10000], train loss: 0.9506 train acc: 0.4133 val_loss: 0.9735, val_acc: 0.2667\n",
      "Epoch [50/10000], train loss: 0.9262 train acc: 0.4800 val_loss: 0.9463, val_acc: 0.3867\n",
      "Epoch [60/10000], train loss: 0.9034 train acc: 0.6933 val_loss: 0.9210, val_acc: 0.5600\n",
      "Epoch [70/10000], train loss: 0.8821 train acc: 0.7067 val_loss: 0.8974, val_acc: 0.6000\n",
      "Epoch [80/10000], train loss: 0.8623 train acc: 0.7067 val_loss: 0.8755, val_acc: 0.6133\n",
      "Epoch [90/10000], train loss: 0.8437 train acc: 0.7067 val_loss: 0.8550, val_acc: 0.6267\n",
      "Epoch [100/10000], train loss: 0.8264 train acc: 0.7067 val_loss: 0.8359, val_acc: 0.6267\n",
      "Epoch [110/10000], train loss: 0.8102 train acc: 0.7200 val_loss: 0.8182, val_acc: 0.6267\n",
      "Epoch [120/10000], train loss: 0.7950 train acc: 0.7200 val_loss: 0.8015, val_acc: 0.6267\n",
      "Epoch [130/10000], train loss: 0.7808 train acc: 0.7333 val_loss: 0.7860, val_acc: 0.6267\n",
      "Epoch [140/10000], train loss: 0.7674 train acc: 0.7467 val_loss: 0.7714, val_acc: 0.6400\n",
      "Epoch [150/10000], train loss: 0.7548 train acc: 0.7467 val_loss: 0.7578, val_acc: 0.6533\n",
      "Epoch [160/10000], train loss: 0.7430 train acc: 0.7467 val_loss: 0.7449, val_acc: 0.6800\n",
      "Epoch [170/10000], train loss: 0.7319 train acc: 0.7600 val_loss: 0.7329, val_acc: 0.7067\n",
      "Epoch [180/10000], train loss: 0.7214 train acc: 0.7733 val_loss: 0.7215, val_acc: 0.7600\n",
      "Epoch [190/10000], train loss: 0.7115 train acc: 0.8267 val_loss: 0.7108, val_acc: 0.7867\n",
      "Epoch [200/10000], train loss: 0.7021 train acc: 0.8267 val_loss: 0.7007, val_acc: 0.7867\n",
      "Epoch [210/10000], train loss: 0.6931 train acc: 0.8400 val_loss: 0.6911, val_acc: 0.8000\n",
      "Epoch [220/10000], train loss: 0.6847 train acc: 0.8400 val_loss: 0.6820, val_acc: 0.8000\n",
      "Epoch [230/10000], train loss: 0.6767 train acc: 0.8667 val_loss: 0.6734, val_acc: 0.8133\n",
      "Epoch [240/10000], train loss: 0.6690 train acc: 0.8667 val_loss: 0.6652, val_acc: 0.8133\n",
      "Epoch [250/10000], train loss: 0.6618 train acc: 0.8667 val_loss: 0.6575, val_acc: 0.8267\n",
      "Epoch [260/10000], train loss: 0.6548 train acc: 0.8533 val_loss: 0.6501, val_acc: 0.8267\n",
      "Epoch [270/10000], train loss: 0.6482 train acc: 0.8533 val_loss: 0.6430, val_acc: 0.8267\n",
      "Epoch [280/10000], train loss: 0.6419 train acc: 0.8533 val_loss: 0.6362, val_acc: 0.8267\n",
      "Epoch [290/10000], train loss: 0.6358 train acc: 0.8667 val_loss: 0.6298, val_acc: 0.8267\n",
      "Epoch [300/10000], train loss: 0.6300 train acc: 0.8800 val_loss: 0.6236, val_acc: 0.8267\n",
      "Epoch [310/10000], train loss: 0.6244 train acc: 0.8933 val_loss: 0.6177, val_acc: 0.8267\n",
      "Epoch [320/10000], train loss: 0.6191 train acc: 0.8933 val_loss: 0.6121, val_acc: 0.8267\n",
      "Epoch [330/10000], train loss: 0.6139 train acc: 0.8933 val_loss: 0.6066, val_acc: 0.8267\n",
      "Epoch [340/10000], train loss: 0.6090 train acc: 0.8933 val_loss: 0.6014, val_acc: 0.8400\n",
      "Epoch [350/10000], train loss: 0.6042 train acc: 0.8933 val_loss: 0.5963, val_acc: 0.8400\n",
      "Epoch [360/10000], train loss: 0.5996 train acc: 0.9067 val_loss: 0.5915, val_acc: 0.8533\n",
      "Epoch [370/10000], train loss: 0.5952 train acc: 0.9200 val_loss: 0.5868, val_acc: 0.8667\n",
      "Epoch [380/10000], train loss: 0.5909 train acc: 0.9200 val_loss: 0.5823, val_acc: 0.8667\n",
      "Epoch [390/10000], train loss: 0.5868 train acc: 0.9200 val_loss: 0.5780, val_acc: 0.8667\n",
      "Epoch [400/10000], train loss: 0.5828 train acc: 0.9200 val_loss: 0.5738, val_acc: 0.8667\n",
      "Epoch [410/10000], train loss: 0.5789 train acc: 0.9200 val_loss: 0.5697, val_acc: 0.8667\n",
      "Epoch [420/10000], train loss: 0.5752 train acc: 0.9200 val_loss: 0.5658, val_acc: 0.8667\n",
      "Epoch [430/10000], train loss: 0.5715 train acc: 0.9067 val_loss: 0.5619, val_acc: 0.8667\n",
      "Epoch [440/10000], train loss: 0.5680 train acc: 0.9067 val_loss: 0.5582, val_acc: 0.8667\n",
      "Epoch [450/10000], train loss: 0.5646 train acc: 0.9067 val_loss: 0.5547, val_acc: 0.8667\n",
      "Epoch [460/10000], train loss: 0.5612 train acc: 0.9067 val_loss: 0.5512, val_acc: 0.8667\n",
      "Epoch [470/10000], train loss: 0.5580 train acc: 0.9067 val_loss: 0.5478, val_acc: 0.8800\n",
      "Epoch [480/10000], train loss: 0.5548 train acc: 0.9067 val_loss: 0.5445, val_acc: 0.8800\n",
      "Epoch [490/10000], train loss: 0.5518 train acc: 0.9067 val_loss: 0.5413, val_acc: 0.8800\n",
      "Epoch [500/10000], train loss: 0.5488 train acc: 0.9067 val_loss: 0.5382, val_acc: 0.8800\n",
      "Epoch [510/10000], train loss: 0.5459 train acc: 0.9067 val_loss: 0.5352, val_acc: 0.8800\n",
      "Epoch [520/10000], train loss: 0.5430 train acc: 0.9067 val_loss: 0.5322, val_acc: 0.8800\n",
      "Epoch [530/10000], train loss: 0.5403 train acc: 0.9067 val_loss: 0.5293, val_acc: 0.8800\n",
      "Epoch [540/10000], train loss: 0.5376 train acc: 0.9067 val_loss: 0.5265, val_acc: 0.8800\n",
      "Epoch [550/10000], train loss: 0.5349 train acc: 0.9067 val_loss: 0.5238, val_acc: 0.8800\n",
      "Epoch [560/10000], train loss: 0.5323 train acc: 0.9067 val_loss: 0.5211, val_acc: 0.8800\n",
      "Epoch [570/10000], train loss: 0.5298 train acc: 0.9067 val_loss: 0.5185, val_acc: 0.8800\n",
      "Epoch [580/10000], train loss: 0.5273 train acc: 0.9067 val_loss: 0.5159, val_acc: 0.8800\n",
      "Epoch [590/10000], train loss: 0.5249 train acc: 0.9067 val_loss: 0.5134, val_acc: 0.8800\n",
      "Epoch [600/10000], train loss: 0.5226 train acc: 0.9067 val_loss: 0.5110, val_acc: 0.8800\n",
      "Epoch [610/10000], train loss: 0.5202 train acc: 0.9067 val_loss: 0.5086, val_acc: 0.8800\n",
      "Epoch [620/10000], train loss: 0.5180 train acc: 0.9067 val_loss: 0.5062, val_acc: 0.8800\n",
      "Epoch [630/10000], train loss: 0.5158 train acc: 0.9067 val_loss: 0.5039, val_acc: 0.8800\n",
      "Epoch [640/10000], train loss: 0.5136 train acc: 0.9067 val_loss: 0.5017, val_acc: 0.8800\n",
      "Epoch [650/10000], train loss: 0.5114 train acc: 0.9067 val_loss: 0.4995, val_acc: 0.8800\n",
      "Epoch [660/10000], train loss: 0.5093 train acc: 0.9067 val_loss: 0.4973, val_acc: 0.8933\n",
      "Epoch [670/10000], train loss: 0.5073 train acc: 0.9067 val_loss: 0.4952, val_acc: 0.9067\n",
      "Epoch [680/10000], train loss: 0.5053 train acc: 0.9067 val_loss: 0.4931, val_acc: 0.9067\n",
      "Epoch [690/10000], train loss: 0.5033 train acc: 0.9067 val_loss: 0.4911, val_acc: 0.9067\n",
      "Epoch [700/10000], train loss: 0.5013 train acc: 0.9067 val_loss: 0.4891, val_acc: 0.9067\n",
      "Epoch [710/10000], train loss: 0.4994 train acc: 0.9067 val_loss: 0.4871, val_acc: 0.9067\n",
      "Epoch [720/10000], train loss: 0.4975 train acc: 0.9067 val_loss: 0.4852, val_acc: 0.9067\n",
      "Epoch [730/10000], train loss: 0.4957 train acc: 0.9067 val_loss: 0.4833, val_acc: 0.9067\n",
      "Epoch [740/10000], train loss: 0.4938 train acc: 0.9067 val_loss: 0.4814, val_acc: 0.9067\n",
      "Epoch [750/10000], train loss: 0.4921 train acc: 0.9067 val_loss: 0.4796, val_acc: 0.9067\n",
      "Epoch [760/10000], train loss: 0.4903 train acc: 0.9067 val_loss: 0.4778, val_acc: 0.9067\n",
      "Epoch [770/10000], train loss: 0.4886 train acc: 0.9067 val_loss: 0.4760, val_acc: 0.9067\n",
      "Epoch [780/10000], train loss: 0.4868 train acc: 0.9067 val_loss: 0.4742, val_acc: 0.9067\n",
      "Epoch [790/10000], train loss: 0.4852 train acc: 0.8933 val_loss: 0.4725, val_acc: 0.9200\n",
      "Epoch [800/10000], train loss: 0.4835 train acc: 0.8933 val_loss: 0.4708, val_acc: 0.9200\n",
      "Epoch [810/10000], train loss: 0.4819 train acc: 0.8933 val_loss: 0.4691, val_acc: 0.9200\n",
      "Epoch [820/10000], train loss: 0.4802 train acc: 0.8933 val_loss: 0.4675, val_acc: 0.9200\n",
      "Epoch [830/10000], train loss: 0.4787 train acc: 0.8933 val_loss: 0.4658, val_acc: 0.9200\n",
      "Epoch [840/10000], train loss: 0.4771 train acc: 0.8933 val_loss: 0.4642, val_acc: 0.9200\n",
      "Epoch [850/10000], train loss: 0.4755 train acc: 0.8933 val_loss: 0.4627, val_acc: 0.9200\n",
      "Epoch [860/10000], train loss: 0.4740 train acc: 0.8933 val_loss: 0.4611, val_acc: 0.9200\n",
      "Epoch [870/10000], train loss: 0.4725 train acc: 0.8933 val_loss: 0.4596, val_acc: 0.9200\n",
      "Epoch [880/10000], train loss: 0.4710 train acc: 0.8933 val_loss: 0.4581, val_acc: 0.9200\n",
      "Epoch [890/10000], train loss: 0.4696 train acc: 0.8933 val_loss: 0.4566, val_acc: 0.9200\n",
      "Epoch [900/10000], train loss: 0.4681 train acc: 0.8933 val_loss: 0.4551, val_acc: 0.9200\n",
      "Epoch [910/10000], train loss: 0.4667 train acc: 0.8933 val_loss: 0.4536, val_acc: 0.9200\n",
      "Epoch [920/10000], train loss: 0.4653 train acc: 0.8933 val_loss: 0.4522, val_acc: 0.9200\n",
      "Epoch [930/10000], train loss: 0.4639 train acc: 0.8933 val_loss: 0.4508, val_acc: 0.9200\n",
      "Epoch [940/10000], train loss: 0.4625 train acc: 0.8933 val_loss: 0.4494, val_acc: 0.9200\n",
      "Epoch [950/10000], train loss: 0.4611 train acc: 0.8933 val_loss: 0.4480, val_acc: 0.9200\n",
      "Epoch [960/10000], train loss: 0.4598 train acc: 0.8933 val_loss: 0.4466, val_acc: 0.9200\n",
      "Epoch [970/10000], train loss: 0.4585 train acc: 0.8933 val_loss: 0.4453, val_acc: 0.9200\n",
      "Epoch [980/10000], train loss: 0.4572 train acc: 0.8933 val_loss: 0.4440, val_acc: 0.9200\n",
      "Epoch [990/10000], train loss: 0.4559 train acc: 0.8933 val_loss: 0.4426, val_acc: 0.9200\n",
      "Epoch [1000/10000], train loss: 0.4546 train acc: 0.8933 val_loss: 0.4413, val_acc: 0.9200\n",
      "Epoch [1010/10000], train loss: 0.4533 train acc: 0.8933 val_loss: 0.4400, val_acc: 0.9200\n",
      "Epoch [1020/10000], train loss: 0.4521 train acc: 0.8933 val_loss: 0.4388, val_acc: 0.9200\n",
      "Epoch [1030/10000], train loss: 0.4508 train acc: 0.8933 val_loss: 0.4375, val_acc: 0.9200\n",
      "Epoch [1040/10000], train loss: 0.4496 train acc: 0.8933 val_loss: 0.4363, val_acc: 0.9200\n",
      "Epoch [1050/10000], train loss: 0.4484 train acc: 0.8933 val_loss: 0.4350, val_acc: 0.9200\n",
      "Epoch [1060/10000], train loss: 0.4472 train acc: 0.8933 val_loss: 0.4338, val_acc: 0.9200\n",
      "Epoch [1070/10000], train loss: 0.4460 train acc: 0.8933 val_loss: 0.4326, val_acc: 0.9200\n",
      "Epoch [1080/10000], train loss: 0.4448 train acc: 0.8933 val_loss: 0.4314, val_acc: 0.9200\n",
      "Epoch [1090/10000], train loss: 0.4436 train acc: 0.8933 val_loss: 0.4302, val_acc: 0.9200\n",
      "Epoch [1100/10000], train loss: 0.4425 train acc: 0.8933 val_loss: 0.4291, val_acc: 0.9200\n",
      "Epoch [1110/10000], train loss: 0.4413 train acc: 0.8933 val_loss: 0.4279, val_acc: 0.9200\n",
      "Epoch [1120/10000], train loss: 0.4402 train acc: 0.8933 val_loss: 0.4268, val_acc: 0.9200\n",
      "Epoch [1130/10000], train loss: 0.4391 train acc: 0.8933 val_loss: 0.4256, val_acc: 0.9200\n",
      "Epoch [1140/10000], train loss: 0.4380 train acc: 0.8933 val_loss: 0.4245, val_acc: 0.9200\n",
      "Epoch [1150/10000], train loss: 0.4369 train acc: 0.8933 val_loss: 0.4234, val_acc: 0.9200\n",
      "Epoch [1160/10000], train loss: 0.4358 train acc: 0.8933 val_loss: 0.4223, val_acc: 0.9200\n",
      "Epoch [1170/10000], train loss: 0.4347 train acc: 0.8933 val_loss: 0.4212, val_acc: 0.9200\n",
      "Epoch [1180/10000], train loss: 0.4336 train acc: 0.8933 val_loss: 0.4201, val_acc: 0.9200\n",
      "Epoch [1190/10000], train loss: 0.4326 train acc: 0.8933 val_loss: 0.4190, val_acc: 0.9200\n",
      "Epoch [1200/10000], train loss: 0.4315 train acc: 0.8933 val_loss: 0.4180, val_acc: 0.9200\n",
      "Epoch [1210/10000], train loss: 0.4305 train acc: 0.8933 val_loss: 0.4169, val_acc: 0.9200\n",
      "Epoch [1220/10000], train loss: 0.4295 train acc: 0.8933 val_loss: 0.4159, val_acc: 0.9200\n",
      "Epoch [1230/10000], train loss: 0.4284 train acc: 0.8933 val_loss: 0.4149, val_acc: 0.9200\n",
      "Epoch [1240/10000], train loss: 0.4274 train acc: 0.8933 val_loss: 0.4138, val_acc: 0.9200\n",
      "Epoch [1250/10000], train loss: 0.4264 train acc: 0.8933 val_loss: 0.4128, val_acc: 0.9200\n",
      "Epoch [1260/10000], train loss: 0.4254 train acc: 0.8933 val_loss: 0.4118, val_acc: 0.9200\n",
      "Epoch [1270/10000], train loss: 0.4244 train acc: 0.8933 val_loss: 0.4108, val_acc: 0.9200\n",
      "Epoch [1280/10000], train loss: 0.4235 train acc: 0.8933 val_loss: 0.4098, val_acc: 0.9200\n",
      "Epoch [1290/10000], train loss: 0.4225 train acc: 0.8933 val_loss: 0.4089, val_acc: 0.9200\n",
      "Epoch [1300/10000], train loss: 0.4215 train acc: 0.8933 val_loss: 0.4079, val_acc: 0.9200\n",
      "Epoch [1310/10000], train loss: 0.4206 train acc: 0.8933 val_loss: 0.4069, val_acc: 0.9200\n",
      "Epoch [1320/10000], train loss: 0.4196 train acc: 0.8933 val_loss: 0.4060, val_acc: 0.9200\n",
      "Epoch [1330/10000], train loss: 0.4187 train acc: 0.8933 val_loss: 0.4050, val_acc: 0.9333\n",
      "Epoch [1340/10000], train loss: 0.4177 train acc: 0.8933 val_loss: 0.4041, val_acc: 0.9333\n",
      "Epoch [1350/10000], train loss: 0.4168 train acc: 0.8933 val_loss: 0.4032, val_acc: 0.9333\n",
      "Epoch [1360/10000], train loss: 0.4159 train acc: 0.8933 val_loss: 0.4022, val_acc: 0.9333\n",
      "Epoch [1370/10000], train loss: 0.4150 train acc: 0.8933 val_loss: 0.4013, val_acc: 0.9333\n",
      "Epoch [1380/10000], train loss: 0.4141 train acc: 0.8933 val_loss: 0.4004, val_acc: 0.9333\n",
      "Epoch [1390/10000], train loss: 0.4132 train acc: 0.8933 val_loss: 0.3995, val_acc: 0.9333\n",
      "Epoch [1400/10000], train loss: 0.4123 train acc: 0.8933 val_loss: 0.3986, val_acc: 0.9333\n",
      "Epoch [1410/10000], train loss: 0.4114 train acc: 0.8933 val_loss: 0.3977, val_acc: 0.9333\n",
      "Epoch [1420/10000], train loss: 0.4106 train acc: 0.8933 val_loss: 0.3968, val_acc: 0.9333\n",
      "Epoch [1430/10000], train loss: 0.4097 train acc: 0.8933 val_loss: 0.3960, val_acc: 0.9333\n",
      "Epoch [1440/10000], train loss: 0.4088 train acc: 0.8933 val_loss: 0.3951, val_acc: 0.9333\n",
      "Epoch [1450/10000], train loss: 0.4080 train acc: 0.8933 val_loss: 0.3942, val_acc: 0.9333\n",
      "Epoch [1460/10000], train loss: 0.4071 train acc: 0.8933 val_loss: 0.3934, val_acc: 0.9333\n",
      "Epoch [1470/10000], train loss: 0.4063 train acc: 0.8933 val_loss: 0.3925, val_acc: 0.9333\n",
      "Epoch [1480/10000], train loss: 0.4054 train acc: 0.9067 val_loss: 0.3917, val_acc: 0.9333\n",
      "Epoch [1490/10000], train loss: 0.4046 train acc: 0.9067 val_loss: 0.3909, val_acc: 0.9333\n",
      "Epoch [1500/10000], train loss: 0.4038 train acc: 0.9067 val_loss: 0.3900, val_acc: 0.9333\n",
      "Epoch [1510/10000], train loss: 0.4030 train acc: 0.9067 val_loss: 0.3892, val_acc: 0.9333\n",
      "Epoch [1520/10000], train loss: 0.4021 train acc: 0.9067 val_loss: 0.3884, val_acc: 0.9333\n",
      "Epoch [1530/10000], train loss: 0.4013 train acc: 0.9067 val_loss: 0.3876, val_acc: 0.9333\n",
      "Epoch [1540/10000], train loss: 0.4005 train acc: 0.9067 val_loss: 0.3868, val_acc: 0.9333\n",
      "Epoch [1550/10000], train loss: 0.3997 train acc: 0.9067 val_loss: 0.3860, val_acc: 0.9333\n",
      "Epoch [1560/10000], train loss: 0.3989 train acc: 0.9067 val_loss: 0.3852, val_acc: 0.9467\n",
      "Epoch [1570/10000], train loss: 0.3982 train acc: 0.9067 val_loss: 0.3844, val_acc: 0.9467\n",
      "Epoch [1580/10000], train loss: 0.3974 train acc: 0.9067 val_loss: 0.3836, val_acc: 0.9467\n",
      "Epoch [1590/10000], train loss: 0.3966 train acc: 0.9067 val_loss: 0.3828, val_acc: 0.9467\n",
      "Epoch [1600/10000], train loss: 0.3958 train acc: 0.9067 val_loss: 0.3820, val_acc: 0.9467\n",
      "Epoch [1610/10000], train loss: 0.3951 train acc: 0.9067 val_loss: 0.3813, val_acc: 0.9467\n",
      "Epoch [1620/10000], train loss: 0.3943 train acc: 0.9067 val_loss: 0.3805, val_acc: 0.9467\n",
      "Epoch [1630/10000], train loss: 0.3936 train acc: 0.9067 val_loss: 0.3798, val_acc: 0.9467\n",
      "Epoch [1640/10000], train loss: 0.3928 train acc: 0.9067 val_loss: 0.3790, val_acc: 0.9467\n",
      "Epoch [1650/10000], train loss: 0.3921 train acc: 0.9067 val_loss: 0.3783, val_acc: 0.9467\n",
      "Epoch [1660/10000], train loss: 0.3913 train acc: 0.9067 val_loss: 0.3775, val_acc: 0.9467\n",
      "Epoch [1670/10000], train loss: 0.3906 train acc: 0.9067 val_loss: 0.3768, val_acc: 0.9467\n",
      "Epoch [1680/10000], train loss: 0.3899 train acc: 0.9067 val_loss: 0.3760, val_acc: 0.9467\n",
      "Epoch [1690/10000], train loss: 0.3891 train acc: 0.9067 val_loss: 0.3753, val_acc: 0.9467\n",
      "Epoch [1700/10000], train loss: 0.3884 train acc: 0.9067 val_loss: 0.3746, val_acc: 0.9467\n",
      "Epoch [1710/10000], train loss: 0.3877 train acc: 0.9067 val_loss: 0.3739, val_acc: 0.9467\n",
      "Epoch [1720/10000], train loss: 0.3870 train acc: 0.9067 val_loss: 0.3731, val_acc: 0.9467\n",
      "Epoch [1730/10000], train loss: 0.3863 train acc: 0.9067 val_loss: 0.3724, val_acc: 0.9467\n",
      "Epoch [1740/10000], train loss: 0.3856 train acc: 0.9067 val_loss: 0.3717, val_acc: 0.9467\n",
      "Epoch [1750/10000], train loss: 0.3849 train acc: 0.9067 val_loss: 0.3710, val_acc: 0.9467\n",
      "Epoch [1760/10000], train loss: 0.3842 train acc: 0.9067 val_loss: 0.3703, val_acc: 0.9467\n",
      "Epoch [1770/10000], train loss: 0.3835 train acc: 0.9067 val_loss: 0.3696, val_acc: 0.9467\n",
      "Epoch [1780/10000], train loss: 0.3828 train acc: 0.9067 val_loss: 0.3689, val_acc: 0.9467\n",
      "Epoch [1790/10000], train loss: 0.3821 train acc: 0.9067 val_loss: 0.3683, val_acc: 0.9467\n",
      "Epoch [1800/10000], train loss: 0.3814 train acc: 0.9067 val_loss: 0.3676, val_acc: 0.9467\n",
      "Epoch [1810/10000], train loss: 0.3808 train acc: 0.9067 val_loss: 0.3669, val_acc: 0.9467\n",
      "Epoch [1820/10000], train loss: 0.3801 train acc: 0.9067 val_loss: 0.3662, val_acc: 0.9467\n",
      "Epoch [1830/10000], train loss: 0.3794 train acc: 0.9067 val_loss: 0.3656, val_acc: 0.9467\n",
      "Epoch [1840/10000], train loss: 0.3788 train acc: 0.9067 val_loss: 0.3649, val_acc: 0.9467\n",
      "Epoch [1850/10000], train loss: 0.3781 train acc: 0.9067 val_loss: 0.3642, val_acc: 0.9467\n",
      "Epoch [1860/10000], train loss: 0.3775 train acc: 0.9067 val_loss: 0.3636, val_acc: 0.9467\n",
      "Epoch [1870/10000], train loss: 0.3768 train acc: 0.9067 val_loss: 0.3629, val_acc: 0.9467\n",
      "Epoch [1880/10000], train loss: 0.3762 train acc: 0.9067 val_loss: 0.3623, val_acc: 0.9467\n",
      "Epoch [1890/10000], train loss: 0.3755 train acc: 0.9067 val_loss: 0.3616, val_acc: 0.9467\n",
      "Epoch [1900/10000], train loss: 0.3749 train acc: 0.9067 val_loss: 0.3610, val_acc: 0.9467\n",
      "Epoch [1910/10000], train loss: 0.3742 train acc: 0.9067 val_loss: 0.3604, val_acc: 0.9467\n",
      "Epoch [1920/10000], train loss: 0.3736 train acc: 0.9067 val_loss: 0.3597, val_acc: 0.9467\n",
      "Epoch [1930/10000], train loss: 0.3730 train acc: 0.9067 val_loss: 0.3591, val_acc: 0.9467\n",
      "Epoch [1940/10000], train loss: 0.3724 train acc: 0.9067 val_loss: 0.3585, val_acc: 0.9467\n",
      "Epoch [1950/10000], train loss: 0.3717 train acc: 0.9067 val_loss: 0.3578, val_acc: 0.9467\n",
      "Epoch [1960/10000], train loss: 0.3711 train acc: 0.9067 val_loss: 0.3572, val_acc: 0.9467\n",
      "Epoch [1970/10000], train loss: 0.3705 train acc: 0.9067 val_loss: 0.3566, val_acc: 0.9467\n",
      "Epoch [1980/10000], train loss: 0.3699 train acc: 0.9067 val_loss: 0.3560, val_acc: 0.9467\n",
      "Epoch [1990/10000], train loss: 0.3693 train acc: 0.9067 val_loss: 0.3554, val_acc: 0.9467\n",
      "Epoch [2000/10000], train loss: 0.3687 train acc: 0.9067 val_loss: 0.3548, val_acc: 0.9467\n",
      "Epoch [2010/10000], train loss: 0.3681 train acc: 0.9067 val_loss: 0.3542, val_acc: 0.9467\n",
      "Epoch [2020/10000], train loss: 0.3675 train acc: 0.9067 val_loss: 0.3536, val_acc: 0.9467\n",
      "Epoch [2030/10000], train loss: 0.3669 train acc: 0.9067 val_loss: 0.3530, val_acc: 0.9467\n",
      "Epoch [2040/10000], train loss: 0.3663 train acc: 0.9067 val_loss: 0.3524, val_acc: 0.9467\n",
      "Epoch [2050/10000], train loss: 0.3657 train acc: 0.9067 val_loss: 0.3518, val_acc: 0.9467\n",
      "Epoch [2060/10000], train loss: 0.3651 train acc: 0.9067 val_loss: 0.3512, val_acc: 0.9467\n",
      "Epoch [2070/10000], train loss: 0.3646 train acc: 0.9067 val_loss: 0.3506, val_acc: 0.9467\n",
      "Epoch [2080/10000], train loss: 0.3640 train acc: 0.9067 val_loss: 0.3500, val_acc: 0.9467\n",
      "Epoch [2090/10000], train loss: 0.3634 train acc: 0.9067 val_loss: 0.3495, val_acc: 0.9467\n",
      "Epoch [2100/10000], train loss: 0.3628 train acc: 0.9067 val_loss: 0.3489, val_acc: 0.9467\n",
      "Epoch [2110/10000], train loss: 0.3623 train acc: 0.9067 val_loss: 0.3483, val_acc: 0.9467\n",
      "Epoch [2120/10000], train loss: 0.3617 train acc: 0.9067 val_loss: 0.3478, val_acc: 0.9467\n",
      "Epoch [2130/10000], train loss: 0.3611 train acc: 0.9067 val_loss: 0.3472, val_acc: 0.9467\n",
      "Epoch [2140/10000], train loss: 0.3606 train acc: 0.9067 val_loss: 0.3466, val_acc: 0.9467\n",
      "Epoch [2150/10000], train loss: 0.3600 train acc: 0.9067 val_loss: 0.3461, val_acc: 0.9467\n",
      "Epoch [2160/10000], train loss: 0.3595 train acc: 0.9067 val_loss: 0.3455, val_acc: 0.9467\n",
      "Epoch [2170/10000], train loss: 0.3589 train acc: 0.9067 val_loss: 0.3450, val_acc: 0.9467\n",
      "Epoch [2180/10000], train loss: 0.3584 train acc: 0.9067 val_loss: 0.3444, val_acc: 0.9467\n",
      "Epoch [2190/10000], train loss: 0.3578 train acc: 0.9067 val_loss: 0.3439, val_acc: 0.9467\n",
      "Epoch [2200/10000], train loss: 0.3573 train acc: 0.9067 val_loss: 0.3433, val_acc: 0.9467\n",
      "Epoch [2210/10000], train loss: 0.3567 train acc: 0.9067 val_loss: 0.3428, val_acc: 0.9467\n",
      "Epoch [2220/10000], train loss: 0.3562 train acc: 0.9067 val_loss: 0.3422, val_acc: 0.9467\n",
      "Epoch [2230/10000], train loss: 0.3557 train acc: 0.9067 val_loss: 0.3417, val_acc: 0.9467\n",
      "Epoch [2240/10000], train loss: 0.3551 train acc: 0.9067 val_loss: 0.3412, val_acc: 0.9467\n",
      "Epoch [2250/10000], train loss: 0.3546 train acc: 0.9067 val_loss: 0.3406, val_acc: 0.9467\n",
      "Epoch [2260/10000], train loss: 0.3541 train acc: 0.9067 val_loss: 0.3401, val_acc: 0.9467\n",
      "Epoch [2270/10000], train loss: 0.3536 train acc: 0.9067 val_loss: 0.3396, val_acc: 0.9467\n",
      "Epoch [2280/10000], train loss: 0.3530 train acc: 0.9067 val_loss: 0.3391, val_acc: 0.9467\n",
      "Epoch [2290/10000], train loss: 0.3525 train acc: 0.9067 val_loss: 0.3385, val_acc: 0.9467\n",
      "Epoch [2300/10000], train loss: 0.3520 train acc: 0.9067 val_loss: 0.3380, val_acc: 0.9467\n",
      "Epoch [2310/10000], train loss: 0.3515 train acc: 0.9067 val_loss: 0.3375, val_acc: 0.9467\n",
      "Epoch [2320/10000], train loss: 0.3510 train acc: 0.9067 val_loss: 0.3370, val_acc: 0.9467\n",
      "Epoch [2330/10000], train loss: 0.3505 train acc: 0.9067 val_loss: 0.3365, val_acc: 0.9467\n",
      "Epoch [2340/10000], train loss: 0.3500 train acc: 0.9067 val_loss: 0.3360, val_acc: 0.9467\n",
      "Epoch [2350/10000], train loss: 0.3495 train acc: 0.9067 val_loss: 0.3355, val_acc: 0.9467\n",
      "Epoch [2360/10000], train loss: 0.3490 train acc: 0.9067 val_loss: 0.3350, val_acc: 0.9467\n",
      "Epoch [2370/10000], train loss: 0.3485 train acc: 0.9067 val_loss: 0.3345, val_acc: 0.9467\n",
      "Epoch [2380/10000], train loss: 0.3480 train acc: 0.9067 val_loss: 0.3340, val_acc: 0.9467\n",
      "Epoch [2390/10000], train loss: 0.3475 train acc: 0.9067 val_loss: 0.3335, val_acc: 0.9467\n",
      "Epoch [2400/10000], train loss: 0.3470 train acc: 0.9067 val_loss: 0.3330, val_acc: 0.9467\n",
      "Epoch [2410/10000], train loss: 0.3465 train acc: 0.9067 val_loss: 0.3325, val_acc: 0.9467\n",
      "Epoch [2420/10000], train loss: 0.3460 train acc: 0.9067 val_loss: 0.3320, val_acc: 0.9467\n",
      "Epoch [2430/10000], train loss: 0.3455 train acc: 0.9067 val_loss: 0.3315, val_acc: 0.9467\n",
      "Epoch [2440/10000], train loss: 0.3451 train acc: 0.9067 val_loss: 0.3310, val_acc: 0.9467\n",
      "Epoch [2450/10000], train loss: 0.3446 train acc: 0.9067 val_loss: 0.3305, val_acc: 0.9467\n",
      "Epoch [2460/10000], train loss: 0.3441 train acc: 0.9067 val_loss: 0.3301, val_acc: 0.9467\n",
      "Epoch [2470/10000], train loss: 0.3436 train acc: 0.9067 val_loss: 0.3296, val_acc: 0.9467\n",
      "Epoch [2480/10000], train loss: 0.3432 train acc: 0.9067 val_loss: 0.3291, val_acc: 0.9467\n",
      "Epoch [2490/10000], train loss: 0.3427 train acc: 0.9067 val_loss: 0.3286, val_acc: 0.9467\n",
      "Epoch [2500/10000], train loss: 0.3422 train acc: 0.9067 val_loss: 0.3282, val_acc: 0.9467\n",
      "Epoch [2510/10000], train loss: 0.3418 train acc: 0.9067 val_loss: 0.3277, val_acc: 0.9467\n",
      "Epoch [2520/10000], train loss: 0.3413 train acc: 0.9067 val_loss: 0.3272, val_acc: 0.9467\n",
      "Epoch [2530/10000], train loss: 0.3408 train acc: 0.9067 val_loss: 0.3268, val_acc: 0.9467\n",
      "Epoch [2540/10000], train loss: 0.3404 train acc: 0.9067 val_loss: 0.3263, val_acc: 0.9467\n",
      "Epoch [2550/10000], train loss: 0.3399 train acc: 0.9067 val_loss: 0.3258, val_acc: 0.9467\n",
      "Epoch [2560/10000], train loss: 0.3395 train acc: 0.9067 val_loss: 0.3254, val_acc: 0.9467\n",
      "Epoch [2570/10000], train loss: 0.3390 train acc: 0.9067 val_loss: 0.3249, val_acc: 0.9467\n",
      "Epoch [2580/10000], train loss: 0.3386 train acc: 0.9067 val_loss: 0.3245, val_acc: 0.9467\n",
      "Epoch [2590/10000], train loss: 0.3381 train acc: 0.9067 val_loss: 0.3240, val_acc: 0.9467\n",
      "Epoch [2600/10000], train loss: 0.3377 train acc: 0.9067 val_loss: 0.3236, val_acc: 0.9467\n",
      "Epoch [2610/10000], train loss: 0.3372 train acc: 0.9067 val_loss: 0.3231, val_acc: 0.9467\n",
      "Epoch [2620/10000], train loss: 0.3368 train acc: 0.9067 val_loss: 0.3227, val_acc: 0.9467\n",
      "Epoch [2630/10000], train loss: 0.3363 train acc: 0.9067 val_loss: 0.3222, val_acc: 0.9467\n",
      "Epoch [2640/10000], train loss: 0.3359 train acc: 0.9067 val_loss: 0.3218, val_acc: 0.9467\n",
      "Epoch [2650/10000], train loss: 0.3355 train acc: 0.9067 val_loss: 0.3214, val_acc: 0.9467\n",
      "Epoch [2660/10000], train loss: 0.3350 train acc: 0.9067 val_loss: 0.3209, val_acc: 0.9467\n",
      "Epoch [2670/10000], train loss: 0.3346 train acc: 0.9067 val_loss: 0.3205, val_acc: 0.9467\n",
      "Epoch [2680/10000], train loss: 0.3342 train acc: 0.9067 val_loss: 0.3201, val_acc: 0.9467\n",
      "Epoch [2690/10000], train loss: 0.3337 train acc: 0.9067 val_loss: 0.3196, val_acc: 0.9467\n",
      "Epoch [2700/10000], train loss: 0.3333 train acc: 0.9067 val_loss: 0.3192, val_acc: 0.9467\n",
      "Epoch [2710/10000], train loss: 0.3329 train acc: 0.9067 val_loss: 0.3188, val_acc: 0.9467\n",
      "Epoch [2720/10000], train loss: 0.3325 train acc: 0.9067 val_loss: 0.3183, val_acc: 0.9467\n",
      "Epoch [2730/10000], train loss: 0.3321 train acc: 0.9067 val_loss: 0.3179, val_acc: 0.9467\n",
      "Epoch [2740/10000], train loss: 0.3316 train acc: 0.9067 val_loss: 0.3175, val_acc: 0.9467\n",
      "Epoch [2750/10000], train loss: 0.3312 train acc: 0.9067 val_loss: 0.3171, val_acc: 0.9467\n",
      "Epoch [2760/10000], train loss: 0.3308 train acc: 0.9067 val_loss: 0.3167, val_acc: 0.9467\n",
      "Epoch [2770/10000], train loss: 0.3304 train acc: 0.9067 val_loss: 0.3162, val_acc: 0.9467\n",
      "Epoch [2780/10000], train loss: 0.3300 train acc: 0.9067 val_loss: 0.3158, val_acc: 0.9467\n",
      "Epoch [2790/10000], train loss: 0.3296 train acc: 0.9067 val_loss: 0.3154, val_acc: 0.9467\n",
      "Epoch [2800/10000], train loss: 0.3292 train acc: 0.9067 val_loss: 0.3150, val_acc: 0.9467\n",
      "Epoch [2810/10000], train loss: 0.3288 train acc: 0.9067 val_loss: 0.3146, val_acc: 0.9467\n",
      "Epoch [2820/10000], train loss: 0.3284 train acc: 0.9067 val_loss: 0.3142, val_acc: 0.9467\n",
      "Epoch [2830/10000], train loss: 0.3280 train acc: 0.9067 val_loss: 0.3138, val_acc: 0.9467\n",
      "Epoch [2840/10000], train loss: 0.3276 train acc: 0.9067 val_loss: 0.3134, val_acc: 0.9467\n",
      "Epoch [2850/10000], train loss: 0.3272 train acc: 0.9067 val_loss: 0.3130, val_acc: 0.9467\n",
      "Epoch [2860/10000], train loss: 0.3268 train acc: 0.9067 val_loss: 0.3126, val_acc: 0.9467\n",
      "Epoch [2870/10000], train loss: 0.3264 train acc: 0.9067 val_loss: 0.3122, val_acc: 0.9467\n",
      "Epoch [2880/10000], train loss: 0.3260 train acc: 0.9067 val_loss: 0.3118, val_acc: 0.9467\n",
      "Epoch [2890/10000], train loss: 0.3256 train acc: 0.9067 val_loss: 0.3114, val_acc: 0.9467\n",
      "Epoch [2900/10000], train loss: 0.3252 train acc: 0.9067 val_loss: 0.3110, val_acc: 0.9467\n",
      "Epoch [2910/10000], train loss: 0.3248 train acc: 0.9067 val_loss: 0.3106, val_acc: 0.9467\n",
      "Epoch [2920/10000], train loss: 0.3244 train acc: 0.9067 val_loss: 0.3102, val_acc: 0.9467\n",
      "Epoch [2930/10000], train loss: 0.3240 train acc: 0.9067 val_loss: 0.3098, val_acc: 0.9467\n",
      "Epoch [2940/10000], train loss: 0.3236 train acc: 0.9067 val_loss: 0.3094, val_acc: 0.9467\n",
      "Epoch [2950/10000], train loss: 0.3233 train acc: 0.9067 val_loss: 0.3090, val_acc: 0.9467\n",
      "Epoch [2960/10000], train loss: 0.3229 train acc: 0.9067 val_loss: 0.3087, val_acc: 0.9467\n",
      "Epoch [2970/10000], train loss: 0.3225 train acc: 0.9067 val_loss: 0.3083, val_acc: 0.9467\n",
      "Epoch [2980/10000], train loss: 0.3221 train acc: 0.9067 val_loss: 0.3079, val_acc: 0.9467\n",
      "Epoch [2990/10000], train loss: 0.3217 train acc: 0.9067 val_loss: 0.3075, val_acc: 0.9467\n",
      "Epoch [3000/10000], train loss: 0.3214 train acc: 0.9067 val_loss: 0.3071, val_acc: 0.9467\n",
      "Epoch [3010/10000], train loss: 0.3210 train acc: 0.9067 val_loss: 0.3068, val_acc: 0.9467\n",
      "Epoch [3020/10000], train loss: 0.3206 train acc: 0.9067 val_loss: 0.3064, val_acc: 0.9467\n",
      "Epoch [3030/10000], train loss: 0.3203 train acc: 0.9067 val_loss: 0.3060, val_acc: 0.9467\n",
      "Epoch [3040/10000], train loss: 0.3199 train acc: 0.9067 val_loss: 0.3056, val_acc: 0.9467\n",
      "Epoch [3050/10000], train loss: 0.3195 train acc: 0.9067 val_loss: 0.3053, val_acc: 0.9467\n",
      "Epoch [3060/10000], train loss: 0.3192 train acc: 0.9067 val_loss: 0.3049, val_acc: 0.9467\n",
      "Epoch [3070/10000], train loss: 0.3188 train acc: 0.9067 val_loss: 0.3045, val_acc: 0.9467\n",
      "Epoch [3080/10000], train loss: 0.3184 train acc: 0.9067 val_loss: 0.3042, val_acc: 0.9467\n",
      "Epoch [3090/10000], train loss: 0.3181 train acc: 0.9067 val_loss: 0.3038, val_acc: 0.9467\n",
      "Epoch [3100/10000], train loss: 0.3177 train acc: 0.9067 val_loss: 0.3034, val_acc: 0.9467\n",
      "Epoch [3110/10000], train loss: 0.3174 train acc: 0.9067 val_loss: 0.3031, val_acc: 0.9467\n",
      "Epoch [3120/10000], train loss: 0.3170 train acc: 0.9067 val_loss: 0.3027, val_acc: 0.9467\n",
      "Epoch [3130/10000], train loss: 0.3166 train acc: 0.9067 val_loss: 0.3024, val_acc: 0.9467\n",
      "Epoch [3140/10000], train loss: 0.3163 train acc: 0.9067 val_loss: 0.3020, val_acc: 0.9467\n",
      "Epoch [3150/10000], train loss: 0.3159 train acc: 0.9067 val_loss: 0.3016, val_acc: 0.9467\n",
      "Epoch [3160/10000], train loss: 0.3156 train acc: 0.9067 val_loss: 0.3013, val_acc: 0.9467\n",
      "Epoch [3170/10000], train loss: 0.3152 train acc: 0.9067 val_loss: 0.3009, val_acc: 0.9467\n",
      "Epoch [3180/10000], train loss: 0.3149 train acc: 0.9067 val_loss: 0.3006, val_acc: 0.9467\n",
      "Epoch [3190/10000], train loss: 0.3146 train acc: 0.9067 val_loss: 0.3002, val_acc: 0.9467\n",
      "Epoch [3200/10000], train loss: 0.3142 train acc: 0.9067 val_loss: 0.2999, val_acc: 0.9467\n",
      "Epoch [3210/10000], train loss: 0.3139 train acc: 0.9067 val_loss: 0.2995, val_acc: 0.9467\n",
      "Epoch [3220/10000], train loss: 0.3135 train acc: 0.9067 val_loss: 0.2992, val_acc: 0.9467\n",
      "Epoch [3230/10000], train loss: 0.3132 train acc: 0.9067 val_loss: 0.2989, val_acc: 0.9467\n",
      "Epoch [3240/10000], train loss: 0.3128 train acc: 0.9067 val_loss: 0.2985, val_acc: 0.9467\n",
      "Epoch [3250/10000], train loss: 0.3125 train acc: 0.9067 val_loss: 0.2982, val_acc: 0.9467\n",
      "Epoch [3260/10000], train loss: 0.3122 train acc: 0.9067 val_loss: 0.2978, val_acc: 0.9467\n",
      "Epoch [3270/10000], train loss: 0.3118 train acc: 0.9067 val_loss: 0.2975, val_acc: 0.9467\n",
      "Epoch [3280/10000], train loss: 0.3115 train acc: 0.9067 val_loss: 0.2971, val_acc: 0.9467\n",
      "Epoch [3290/10000], train loss: 0.3112 train acc: 0.9067 val_loss: 0.2968, val_acc: 0.9467\n",
      "Epoch [3300/10000], train loss: 0.3108 train acc: 0.9067 val_loss: 0.2965, val_acc: 0.9467\n",
      "Epoch [3310/10000], train loss: 0.3105 train acc: 0.9067 val_loss: 0.2961, val_acc: 0.9467\n",
      "Epoch [3320/10000], train loss: 0.3102 train acc: 0.9067 val_loss: 0.2958, val_acc: 0.9467\n",
      "Epoch [3330/10000], train loss: 0.3099 train acc: 0.9067 val_loss: 0.2955, val_acc: 0.9467\n",
      "Epoch [3340/10000], train loss: 0.3095 train acc: 0.9067 val_loss: 0.2951, val_acc: 0.9467\n",
      "Epoch [3350/10000], train loss: 0.3092 train acc: 0.9067 val_loss: 0.2948, val_acc: 0.9467\n",
      "Epoch [3360/10000], train loss: 0.3089 train acc: 0.9067 val_loss: 0.2945, val_acc: 0.9467\n",
      "Epoch [3370/10000], train loss: 0.3086 train acc: 0.9067 val_loss: 0.2942, val_acc: 0.9467\n",
      "Epoch [3380/10000], train loss: 0.3082 train acc: 0.9067 val_loss: 0.2938, val_acc: 0.9467\n",
      "Epoch [3390/10000], train loss: 0.3079 train acc: 0.9067 val_loss: 0.2935, val_acc: 0.9467\n",
      "Epoch [3400/10000], train loss: 0.3076 train acc: 0.9067 val_loss: 0.2932, val_acc: 0.9467\n",
      "Epoch [3410/10000], train loss: 0.3073 train acc: 0.9067 val_loss: 0.2929, val_acc: 0.9467\n",
      "Epoch [3420/10000], train loss: 0.3070 train acc: 0.9067 val_loss: 0.2925, val_acc: 0.9467\n",
      "Epoch [3430/10000], train loss: 0.3066 train acc: 0.9067 val_loss: 0.2922, val_acc: 0.9467\n",
      "Epoch [3440/10000], train loss: 0.3063 train acc: 0.9067 val_loss: 0.2919, val_acc: 0.9467\n",
      "Epoch [3450/10000], train loss: 0.3060 train acc: 0.9067 val_loss: 0.2916, val_acc: 0.9467\n",
      "Epoch [3460/10000], train loss: 0.3057 train acc: 0.9067 val_loss: 0.2913, val_acc: 0.9467\n",
      "Epoch [3470/10000], train loss: 0.3054 train acc: 0.9067 val_loss: 0.2910, val_acc: 0.9467\n",
      "Epoch [3480/10000], train loss: 0.3051 train acc: 0.9067 val_loss: 0.2906, val_acc: 0.9467\n",
      "Epoch [3490/10000], train loss: 0.3048 train acc: 0.9067 val_loss: 0.2903, val_acc: 0.9467\n",
      "Epoch [3500/10000], train loss: 0.3045 train acc: 0.9067 val_loss: 0.2900, val_acc: 0.9467\n",
      "Epoch [3510/10000], train loss: 0.3042 train acc: 0.9067 val_loss: 0.2897, val_acc: 0.9467\n",
      "Epoch [3520/10000], train loss: 0.3039 train acc: 0.9067 val_loss: 0.2894, val_acc: 0.9467\n",
      "Epoch [3530/10000], train loss: 0.3036 train acc: 0.9067 val_loss: 0.2891, val_acc: 0.9467\n",
      "Epoch [3540/10000], train loss: 0.3033 train acc: 0.9067 val_loss: 0.2888, val_acc: 0.9467\n",
      "Epoch [3550/10000], train loss: 0.3030 train acc: 0.9067 val_loss: 0.2885, val_acc: 0.9467\n",
      "Epoch [3560/10000], train loss: 0.3027 train acc: 0.9067 val_loss: 0.2882, val_acc: 0.9467\n",
      "Epoch [3570/10000], train loss: 0.3024 train acc: 0.9067 val_loss: 0.2879, val_acc: 0.9467\n",
      "Epoch [3580/10000], train loss: 0.3021 train acc: 0.9067 val_loss: 0.2876, val_acc: 0.9467\n",
      "Epoch [3590/10000], train loss: 0.3018 train acc: 0.9067 val_loss: 0.2873, val_acc: 0.9467\n",
      "Epoch [3600/10000], train loss: 0.3015 train acc: 0.9067 val_loss: 0.2870, val_acc: 0.9467\n",
      "Epoch [3610/10000], train loss: 0.3012 train acc: 0.9067 val_loss: 0.2867, val_acc: 0.9467\n",
      "Epoch [3620/10000], train loss: 0.3009 train acc: 0.9067 val_loss: 0.2864, val_acc: 0.9600\n",
      "Epoch [3630/10000], train loss: 0.3006 train acc: 0.9067 val_loss: 0.2861, val_acc: 0.9600\n",
      "Epoch [3640/10000], train loss: 0.3003 train acc: 0.9067 val_loss: 0.2858, val_acc: 0.9600\n",
      "Epoch [3650/10000], train loss: 0.3000 train acc: 0.9067 val_loss: 0.2855, val_acc: 0.9600\n",
      "Epoch [3660/10000], train loss: 0.2997 train acc: 0.9067 val_loss: 0.2852, val_acc: 0.9600\n",
      "Epoch [3670/10000], train loss: 0.2994 train acc: 0.9067 val_loss: 0.2849, val_acc: 0.9600\n",
      "Epoch [3680/10000], train loss: 0.2992 train acc: 0.9067 val_loss: 0.2846, val_acc: 0.9600\n",
      "Epoch [3690/10000], train loss: 0.2989 train acc: 0.9067 val_loss: 0.2843, val_acc: 0.9600\n",
      "Epoch [3700/10000], train loss: 0.2986 train acc: 0.9067 val_loss: 0.2840, val_acc: 0.9600\n",
      "Epoch [3710/10000], train loss: 0.2983 train acc: 0.9067 val_loss: 0.2837, val_acc: 0.9600\n",
      "Epoch [3720/10000], train loss: 0.2980 train acc: 0.9067 val_loss: 0.2835, val_acc: 0.9600\n",
      "Epoch [3730/10000], train loss: 0.2977 train acc: 0.9067 val_loss: 0.2832, val_acc: 0.9600\n",
      "Epoch [3740/10000], train loss: 0.2975 train acc: 0.9067 val_loss: 0.2829, val_acc: 0.9600\n",
      "Epoch [3750/10000], train loss: 0.2972 train acc: 0.9067 val_loss: 0.2826, val_acc: 0.9600\n",
      "Epoch [3760/10000], train loss: 0.2969 train acc: 0.9067 val_loss: 0.2823, val_acc: 0.9600\n",
      "Epoch [3770/10000], train loss: 0.2966 train acc: 0.9067 val_loss: 0.2820, val_acc: 0.9600\n",
      "Epoch [3780/10000], train loss: 0.2963 train acc: 0.9067 val_loss: 0.2818, val_acc: 0.9600\n",
      "Epoch [3790/10000], train loss: 0.2961 train acc: 0.9067 val_loss: 0.2815, val_acc: 0.9600\n",
      "Epoch [3800/10000], train loss: 0.2958 train acc: 0.9067 val_loss: 0.2812, val_acc: 0.9600\n",
      "Epoch [3810/10000], train loss: 0.2955 train acc: 0.9067 val_loss: 0.2809, val_acc: 0.9600\n",
      "Epoch [3820/10000], train loss: 0.2953 train acc: 0.9067 val_loss: 0.2806, val_acc: 0.9600\n",
      "Epoch [3830/10000], train loss: 0.2950 train acc: 0.9067 val_loss: 0.2804, val_acc: 0.9600\n",
      "Epoch [3840/10000], train loss: 0.2947 train acc: 0.9067 val_loss: 0.2801, val_acc: 0.9600\n",
      "Epoch [3850/10000], train loss: 0.2944 train acc: 0.9067 val_loss: 0.2798, val_acc: 0.9600\n",
      "Epoch [3860/10000], train loss: 0.2942 train acc: 0.9067 val_loss: 0.2795, val_acc: 0.9600\n",
      "Epoch [3870/10000], train loss: 0.2939 train acc: 0.9067 val_loss: 0.2793, val_acc: 0.9600\n",
      "Epoch [3880/10000], train loss: 0.2936 train acc: 0.9067 val_loss: 0.2790, val_acc: 0.9600\n",
      "Epoch [3890/10000], train loss: 0.2934 train acc: 0.9067 val_loss: 0.2787, val_acc: 0.9600\n",
      "Epoch [3900/10000], train loss: 0.2931 train acc: 0.9067 val_loss: 0.2785, val_acc: 0.9600\n",
      "Epoch [3910/10000], train loss: 0.2928 train acc: 0.9067 val_loss: 0.2782, val_acc: 0.9600\n",
      "Epoch [3920/10000], train loss: 0.2926 train acc: 0.9067 val_loss: 0.2779, val_acc: 0.9600\n",
      "Epoch [3930/10000], train loss: 0.2923 train acc: 0.9067 val_loss: 0.2777, val_acc: 0.9600\n",
      "Epoch [3940/10000], train loss: 0.2921 train acc: 0.9067 val_loss: 0.2774, val_acc: 0.9600\n",
      "Epoch [3950/10000], train loss: 0.2918 train acc: 0.9067 val_loss: 0.2771, val_acc: 0.9600\n",
      "Epoch [3960/10000], train loss: 0.2915 train acc: 0.9067 val_loss: 0.2769, val_acc: 0.9600\n",
      "Epoch [3970/10000], train loss: 0.2913 train acc: 0.9067 val_loss: 0.2766, val_acc: 0.9600\n",
      "Epoch [3980/10000], train loss: 0.2910 train acc: 0.9067 val_loss: 0.2763, val_acc: 0.9600\n",
      "Epoch [3990/10000], train loss: 0.2908 train acc: 0.9067 val_loss: 0.2761, val_acc: 0.9600\n",
      "Epoch [4000/10000], train loss: 0.2905 train acc: 0.9067 val_loss: 0.2758, val_acc: 0.9600\n",
      "Epoch [4010/10000], train loss: 0.2903 train acc: 0.9067 val_loss: 0.2756, val_acc: 0.9600\n",
      "Epoch [4020/10000], train loss: 0.2900 train acc: 0.9067 val_loss: 0.2753, val_acc: 0.9600\n",
      "Epoch [4030/10000], train loss: 0.2898 train acc: 0.9067 val_loss: 0.2750, val_acc: 0.9600\n",
      "Epoch [4040/10000], train loss: 0.2895 train acc: 0.9067 val_loss: 0.2748, val_acc: 0.9600\n",
      "Epoch [4050/10000], train loss: 0.2893 train acc: 0.9067 val_loss: 0.2745, val_acc: 0.9600\n",
      "Epoch [4060/10000], train loss: 0.2890 train acc: 0.9067 val_loss: 0.2743, val_acc: 0.9600\n",
      "Epoch [4070/10000], train loss: 0.2888 train acc: 0.9067 val_loss: 0.2740, val_acc: 0.9600\n",
      "Epoch [4080/10000], train loss: 0.2885 train acc: 0.9067 val_loss: 0.2738, val_acc: 0.9600\n",
      "Epoch [4090/10000], train loss: 0.2883 train acc: 0.9067 val_loss: 0.2735, val_acc: 0.9600\n",
      "Epoch [4100/10000], train loss: 0.2880 train acc: 0.9067 val_loss: 0.2733, val_acc: 0.9600\n",
      "Epoch [4110/10000], train loss: 0.2878 train acc: 0.9067 val_loss: 0.2730, val_acc: 0.9600\n",
      "Epoch [4120/10000], train loss: 0.2875 train acc: 0.9067 val_loss: 0.2728, val_acc: 0.9600\n",
      "Epoch [4130/10000], train loss: 0.2873 train acc: 0.9067 val_loss: 0.2725, val_acc: 0.9600\n",
      "Epoch [4140/10000], train loss: 0.2870 train acc: 0.9067 val_loss: 0.2723, val_acc: 0.9600\n",
      "Epoch [4150/10000], train loss: 0.2868 train acc: 0.9067 val_loss: 0.2720, val_acc: 0.9600\n",
      "Epoch [4160/10000], train loss: 0.2865 train acc: 0.9067 val_loss: 0.2718, val_acc: 0.9600\n",
      "Epoch [4170/10000], train loss: 0.2863 train acc: 0.9067 val_loss: 0.2715, val_acc: 0.9600\n",
      "Epoch [4180/10000], train loss: 0.2861 train acc: 0.9067 val_loss: 0.2713, val_acc: 0.9600\n",
      "Epoch [4190/10000], train loss: 0.2858 train acc: 0.9067 val_loss: 0.2710, val_acc: 0.9600\n",
      "Epoch [4200/10000], train loss: 0.2856 train acc: 0.9067 val_loss: 0.2708, val_acc: 0.9600\n",
      "Epoch [4210/10000], train loss: 0.2853 train acc: 0.9067 val_loss: 0.2705, val_acc: 0.9600\n",
      "Epoch [4220/10000], train loss: 0.2851 train acc: 0.9067 val_loss: 0.2703, val_acc: 0.9600\n",
      "Epoch [4230/10000], train loss: 0.2849 train acc: 0.9067 val_loss: 0.2700, val_acc: 0.9600\n",
      "Epoch [4240/10000], train loss: 0.2846 train acc: 0.9067 val_loss: 0.2698, val_acc: 0.9600\n",
      "Epoch [4250/10000], train loss: 0.2844 train acc: 0.9067 val_loss: 0.2696, val_acc: 0.9600\n",
      "Epoch [4260/10000], train loss: 0.2842 train acc: 0.9067 val_loss: 0.2693, val_acc: 0.9600\n",
      "Epoch [4270/10000], train loss: 0.2839 train acc: 0.9067 val_loss: 0.2691, val_acc: 0.9600\n",
      "Epoch [4280/10000], train loss: 0.2837 train acc: 0.9067 val_loss: 0.2689, val_acc: 0.9600\n",
      "Epoch [4290/10000], train loss: 0.2835 train acc: 0.9067 val_loss: 0.2686, val_acc: 0.9600\n",
      "Epoch [4300/10000], train loss: 0.2832 train acc: 0.9067 val_loss: 0.2684, val_acc: 0.9600\n",
      "Epoch [4310/10000], train loss: 0.2830 train acc: 0.9067 val_loss: 0.2681, val_acc: 0.9600\n",
      "Epoch [4320/10000], train loss: 0.2828 train acc: 0.9067 val_loss: 0.2679, val_acc: 0.9600\n",
      "Epoch [4330/10000], train loss: 0.2826 train acc: 0.9067 val_loss: 0.2677, val_acc: 0.9600\n",
      "Epoch [4340/10000], train loss: 0.2823 train acc: 0.9067 val_loss: 0.2674, val_acc: 0.9600\n",
      "Epoch [4350/10000], train loss: 0.2821 train acc: 0.9067 val_loss: 0.2672, val_acc: 0.9600\n",
      "Epoch [4360/10000], train loss: 0.2819 train acc: 0.9067 val_loss: 0.2670, val_acc: 0.9600\n",
      "Epoch [4370/10000], train loss: 0.2816 train acc: 0.9067 val_loss: 0.2667, val_acc: 0.9600\n",
      "Epoch [4380/10000], train loss: 0.2814 train acc: 0.9067 val_loss: 0.2665, val_acc: 0.9600\n",
      "Epoch [4390/10000], train loss: 0.2812 train acc: 0.9067 val_loss: 0.2663, val_acc: 0.9600\n",
      "Epoch [4400/10000], train loss: 0.2810 train acc: 0.9067 val_loss: 0.2661, val_acc: 0.9600\n",
      "Epoch [4410/10000], train loss: 0.2808 train acc: 0.9067 val_loss: 0.2658, val_acc: 0.9600\n",
      "Epoch [4420/10000], train loss: 0.2805 train acc: 0.9067 val_loss: 0.2656, val_acc: 0.9600\n",
      "Epoch [4430/10000], train loss: 0.2803 train acc: 0.9067 val_loss: 0.2654, val_acc: 0.9600\n",
      "Epoch [4440/10000], train loss: 0.2801 train acc: 0.9067 val_loss: 0.2652, val_acc: 0.9600\n",
      "Epoch [4450/10000], train loss: 0.2799 train acc: 0.9067 val_loss: 0.2649, val_acc: 0.9600\n",
      "Epoch [4460/10000], train loss: 0.2797 train acc: 0.9067 val_loss: 0.2647, val_acc: 0.9600\n",
      "Epoch [4470/10000], train loss: 0.2794 train acc: 0.9067 val_loss: 0.2645, val_acc: 0.9600\n",
      "Epoch [4480/10000], train loss: 0.2792 train acc: 0.9067 val_loss: 0.2643, val_acc: 0.9600\n",
      "Epoch [4490/10000], train loss: 0.2790 train acc: 0.9067 val_loss: 0.2640, val_acc: 0.9600\n",
      "Epoch [4500/10000], train loss: 0.2788 train acc: 0.9067 val_loss: 0.2638, val_acc: 0.9600\n",
      "Epoch [4510/10000], train loss: 0.2786 train acc: 0.9067 val_loss: 0.2636, val_acc: 0.9600\n",
      "Epoch [4520/10000], train loss: 0.2784 train acc: 0.9067 val_loss: 0.2634, val_acc: 0.9600\n",
      "Epoch [4530/10000], train loss: 0.2781 train acc: 0.9067 val_loss: 0.2631, val_acc: 0.9600\n",
      "Epoch [4540/10000], train loss: 0.2779 train acc: 0.9067 val_loss: 0.2629, val_acc: 0.9600\n",
      "Epoch [4550/10000], train loss: 0.2777 train acc: 0.9067 val_loss: 0.2627, val_acc: 0.9600\n",
      "Epoch [4560/10000], train loss: 0.2775 train acc: 0.9067 val_loss: 0.2625, val_acc: 0.9600\n",
      "Epoch [4570/10000], train loss: 0.2773 train acc: 0.9067 val_loss: 0.2623, val_acc: 0.9600\n",
      "Epoch [4580/10000], train loss: 0.2771 train acc: 0.9067 val_loss: 0.2621, val_acc: 0.9600\n",
      "Epoch [4590/10000], train loss: 0.2769 train acc: 0.9067 val_loss: 0.2618, val_acc: 0.9600\n",
      "Epoch [4600/10000], train loss: 0.2767 train acc: 0.9067 val_loss: 0.2616, val_acc: 0.9600\n",
      "Epoch [4610/10000], train loss: 0.2764 train acc: 0.9067 val_loss: 0.2614, val_acc: 0.9600\n",
      "Epoch [4620/10000], train loss: 0.2762 train acc: 0.9067 val_loss: 0.2612, val_acc: 0.9600\n",
      "Epoch [4630/10000], train loss: 0.2760 train acc: 0.9067 val_loss: 0.2610, val_acc: 0.9600\n",
      "Epoch [4640/10000], train loss: 0.2758 train acc: 0.9067 val_loss: 0.2608, val_acc: 0.9600\n",
      "Epoch [4650/10000], train loss: 0.2756 train acc: 0.9067 val_loss: 0.2606, val_acc: 0.9600\n",
      "Epoch [4660/10000], train loss: 0.2754 train acc: 0.9067 val_loss: 0.2604, val_acc: 0.9600\n",
      "Epoch [4670/10000], train loss: 0.2752 train acc: 0.9067 val_loss: 0.2601, val_acc: 0.9600\n",
      "Epoch [4680/10000], train loss: 0.2750 train acc: 0.9067 val_loss: 0.2599, val_acc: 0.9600\n",
      "Epoch [4690/10000], train loss: 0.2748 train acc: 0.9067 val_loss: 0.2597, val_acc: 0.9600\n",
      "Epoch [4700/10000], train loss: 0.2746 train acc: 0.9067 val_loss: 0.2595, val_acc: 0.9600\n",
      "Epoch [4710/10000], train loss: 0.2744 train acc: 0.9067 val_loss: 0.2593, val_acc: 0.9600\n",
      "Epoch [4720/10000], train loss: 0.2742 train acc: 0.9067 val_loss: 0.2591, val_acc: 0.9600\n",
      "Epoch [4730/10000], train loss: 0.2740 train acc: 0.9067 val_loss: 0.2589, val_acc: 0.9600\n",
      "Epoch [4740/10000], train loss: 0.2738 train acc: 0.9067 val_loss: 0.2587, val_acc: 0.9600\n",
      "Epoch [4750/10000], train loss: 0.2736 train acc: 0.9067 val_loss: 0.2585, val_acc: 0.9600\n",
      "Epoch [4760/10000], train loss: 0.2734 train acc: 0.9067 val_loss: 0.2583, val_acc: 0.9600\n",
      "Epoch [4770/10000], train loss: 0.2732 train acc: 0.9067 val_loss: 0.2581, val_acc: 0.9600\n",
      "Epoch [4780/10000], train loss: 0.2730 train acc: 0.9067 val_loss: 0.2579, val_acc: 0.9600\n",
      "Epoch [4790/10000], train loss: 0.2728 train acc: 0.9067 val_loss: 0.2577, val_acc: 0.9600\n",
      "Epoch [4800/10000], train loss: 0.2726 train acc: 0.9067 val_loss: 0.2575, val_acc: 0.9600\n",
      "Epoch [4810/10000], train loss: 0.2724 train acc: 0.9067 val_loss: 0.2573, val_acc: 0.9600\n",
      "Epoch [4820/10000], train loss: 0.2722 train acc: 0.9067 val_loss: 0.2571, val_acc: 0.9600\n",
      "Epoch [4830/10000], train loss: 0.2720 train acc: 0.9067 val_loss: 0.2569, val_acc: 0.9600\n",
      "Epoch [4840/10000], train loss: 0.2718 train acc: 0.9067 val_loss: 0.2567, val_acc: 0.9600\n",
      "Epoch [4850/10000], train loss: 0.2716 train acc: 0.9067 val_loss: 0.2565, val_acc: 0.9600\n",
      "Epoch [4860/10000], train loss: 0.2714 train acc: 0.9067 val_loss: 0.2563, val_acc: 0.9600\n",
      "Epoch [4870/10000], train loss: 0.2712 train acc: 0.9067 val_loss: 0.2561, val_acc: 0.9600\n",
      "Epoch [4880/10000], train loss: 0.2710 train acc: 0.9067 val_loss: 0.2559, val_acc: 0.9600\n",
      "Epoch [4890/10000], train loss: 0.2709 train acc: 0.9067 val_loss: 0.2557, val_acc: 0.9600\n",
      "Epoch [4900/10000], train loss: 0.2707 train acc: 0.9067 val_loss: 0.2555, val_acc: 0.9600\n",
      "Epoch [4910/10000], train loss: 0.2705 train acc: 0.9067 val_loss: 0.2553, val_acc: 0.9600\n",
      "Epoch [4920/10000], train loss: 0.2703 train acc: 0.9067 val_loss: 0.2551, val_acc: 0.9600\n",
      "Epoch [4930/10000], train loss: 0.2701 train acc: 0.9067 val_loss: 0.2549, val_acc: 0.9600\n",
      "Epoch [4940/10000], train loss: 0.2699 train acc: 0.9067 val_loss: 0.2547, val_acc: 0.9600\n",
      "Epoch [4950/10000], train loss: 0.2697 train acc: 0.9067 val_loss: 0.2545, val_acc: 0.9600\n",
      "Epoch [4960/10000], train loss: 0.2695 train acc: 0.9067 val_loss: 0.2543, val_acc: 0.9600\n",
      "Epoch [4970/10000], train loss: 0.2693 train acc: 0.9067 val_loss: 0.2541, val_acc: 0.9600\n",
      "Epoch [4980/10000], train loss: 0.2692 train acc: 0.9067 val_loss: 0.2539, val_acc: 0.9600\n",
      "Epoch [4990/10000], train loss: 0.2690 train acc: 0.9067 val_loss: 0.2537, val_acc: 0.9600\n",
      "Epoch [5000/10000], train loss: 0.2688 train acc: 0.9067 val_loss: 0.2535, val_acc: 0.9600\n",
      "Epoch [5010/10000], train loss: 0.2686 train acc: 0.9067 val_loss: 0.2533, val_acc: 0.9600\n",
      "Epoch [5020/10000], train loss: 0.2684 train acc: 0.9067 val_loss: 0.2532, val_acc: 0.9600\n",
      "Epoch [5030/10000], train loss: 0.2682 train acc: 0.9067 val_loss: 0.2530, val_acc: 0.9600\n",
      "Epoch [5040/10000], train loss: 0.2681 train acc: 0.9067 val_loss: 0.2528, val_acc: 0.9600\n",
      "Epoch [5050/10000], train loss: 0.2679 train acc: 0.9067 val_loss: 0.2526, val_acc: 0.9600\n",
      "Epoch [5060/10000], train loss: 0.2677 train acc: 0.9067 val_loss: 0.2524, val_acc: 0.9600\n",
      "Epoch [5070/10000], train loss: 0.2675 train acc: 0.9067 val_loss: 0.2522, val_acc: 0.9600\n",
      "Epoch [5080/10000], train loss: 0.2673 train acc: 0.9067 val_loss: 0.2520, val_acc: 0.9600\n",
      "Epoch [5090/10000], train loss: 0.2671 train acc: 0.9067 val_loss: 0.2518, val_acc: 0.9600\n",
      "Epoch [5100/10000], train loss: 0.2670 train acc: 0.9067 val_loss: 0.2517, val_acc: 0.9600\n",
      "Epoch [5110/10000], train loss: 0.2668 train acc: 0.9067 val_loss: 0.2515, val_acc: 0.9600\n",
      "Epoch [5120/10000], train loss: 0.2666 train acc: 0.9067 val_loss: 0.2513, val_acc: 0.9600\n",
      "Epoch [5130/10000], train loss: 0.2664 train acc: 0.9067 val_loss: 0.2511, val_acc: 0.9600\n",
      "Epoch [5140/10000], train loss: 0.2663 train acc: 0.9067 val_loss: 0.2509, val_acc: 0.9600\n",
      "Epoch [5150/10000], train loss: 0.2661 train acc: 0.9067 val_loss: 0.2507, val_acc: 0.9600\n",
      "Epoch [5160/10000], train loss: 0.2659 train acc: 0.9067 val_loss: 0.2506, val_acc: 0.9600\n",
      "Epoch [5170/10000], train loss: 0.2657 train acc: 0.9067 val_loss: 0.2504, val_acc: 0.9600\n",
      "Epoch [5180/10000], train loss: 0.2655 train acc: 0.9067 val_loss: 0.2502, val_acc: 0.9600\n",
      "Epoch [5190/10000], train loss: 0.2654 train acc: 0.9067 val_loss: 0.2500, val_acc: 0.9600\n",
      "Epoch [5200/10000], train loss: 0.2652 train acc: 0.9067 val_loss: 0.2498, val_acc: 0.9600\n",
      "Epoch [5210/10000], train loss: 0.2650 train acc: 0.9067 val_loss: 0.2497, val_acc: 0.9600\n",
      "Epoch [5220/10000], train loss: 0.2648 train acc: 0.9067 val_loss: 0.2495, val_acc: 0.9600\n",
      "Epoch [5230/10000], train loss: 0.2647 train acc: 0.9067 val_loss: 0.2493, val_acc: 0.9600\n",
      "Epoch [5240/10000], train loss: 0.2645 train acc: 0.9067 val_loss: 0.2491, val_acc: 0.9600\n",
      "Epoch [5250/10000], train loss: 0.2643 train acc: 0.9067 val_loss: 0.2489, val_acc: 0.9600\n",
      "Epoch [5260/10000], train loss: 0.2642 train acc: 0.9067 val_loss: 0.2488, val_acc: 0.9600\n",
      "Epoch [5270/10000], train loss: 0.2640 train acc: 0.9067 val_loss: 0.2486, val_acc: 0.9600\n",
      "Epoch [5280/10000], train loss: 0.2638 train acc: 0.9067 val_loss: 0.2484, val_acc: 0.9600\n",
      "Epoch [5290/10000], train loss: 0.2636 train acc: 0.9067 val_loss: 0.2482, val_acc: 0.9600\n",
      "Epoch [5300/10000], train loss: 0.2635 train acc: 0.9067 val_loss: 0.2481, val_acc: 0.9600\n",
      "Epoch [5310/10000], train loss: 0.2633 train acc: 0.9067 val_loss: 0.2479, val_acc: 0.9600\n",
      "Epoch [5320/10000], train loss: 0.2631 train acc: 0.9067 val_loss: 0.2477, val_acc: 0.9600\n",
      "Epoch [5330/10000], train loss: 0.2630 train acc: 0.9067 val_loss: 0.2475, val_acc: 0.9600\n",
      "Epoch [5340/10000], train loss: 0.2628 train acc: 0.9067 val_loss: 0.2474, val_acc: 0.9600\n",
      "Epoch [5350/10000], train loss: 0.2626 train acc: 0.9067 val_loss: 0.2472, val_acc: 0.9600\n",
      "Epoch [5360/10000], train loss: 0.2625 train acc: 0.9067 val_loss: 0.2470, val_acc: 0.9600\n",
      "Epoch [5370/10000], train loss: 0.2623 train acc: 0.9067 val_loss: 0.2468, val_acc: 0.9600\n",
      "Epoch [5380/10000], train loss: 0.2621 train acc: 0.9067 val_loss: 0.2467, val_acc: 0.9600\n",
      "Epoch [5390/10000], train loss: 0.2620 train acc: 0.9067 val_loss: 0.2465, val_acc: 0.9600\n",
      "Epoch [5400/10000], train loss: 0.2618 train acc: 0.9067 val_loss: 0.2463, val_acc: 0.9600\n",
      "Epoch [5410/10000], train loss: 0.2616 train acc: 0.9067 val_loss: 0.2462, val_acc: 0.9600\n",
      "Epoch [5420/10000], train loss: 0.2615 train acc: 0.9067 val_loss: 0.2460, val_acc: 0.9600\n",
      "Epoch [5430/10000], train loss: 0.2613 train acc: 0.9067 val_loss: 0.2458, val_acc: 0.9600\n",
      "Epoch [5440/10000], train loss: 0.2612 train acc: 0.9067 val_loss: 0.2456, val_acc: 0.9600\n",
      "Epoch [5450/10000], train loss: 0.2610 train acc: 0.9067 val_loss: 0.2455, val_acc: 0.9600\n",
      "Epoch [5460/10000], train loss: 0.2608 train acc: 0.9067 val_loss: 0.2453, val_acc: 0.9600\n",
      "Epoch [5470/10000], train loss: 0.2607 train acc: 0.9067 val_loss: 0.2451, val_acc: 0.9600\n",
      "Epoch [5480/10000], train loss: 0.2605 train acc: 0.9067 val_loss: 0.2450, val_acc: 0.9600\n",
      "Epoch [5490/10000], train loss: 0.2603 train acc: 0.9067 val_loss: 0.2448, val_acc: 0.9600\n",
      "Epoch [5500/10000], train loss: 0.2602 train acc: 0.9067 val_loss: 0.2446, val_acc: 0.9600\n",
      "Epoch [5510/10000], train loss: 0.2600 train acc: 0.9067 val_loss: 0.2445, val_acc: 0.9600\n",
      "Epoch [5520/10000], train loss: 0.2599 train acc: 0.9067 val_loss: 0.2443, val_acc: 0.9600\n",
      "Epoch [5530/10000], train loss: 0.2597 train acc: 0.9067 val_loss: 0.2441, val_acc: 0.9600\n",
      "Epoch [5540/10000], train loss: 0.2595 train acc: 0.9067 val_loss: 0.2440, val_acc: 0.9600\n",
      "Epoch [5550/10000], train loss: 0.2594 train acc: 0.9067 val_loss: 0.2438, val_acc: 0.9600\n",
      "Epoch [5560/10000], train loss: 0.2592 train acc: 0.9067 val_loss: 0.2437, val_acc: 0.9600\n",
      "Epoch [5570/10000], train loss: 0.2591 train acc: 0.9067 val_loss: 0.2435, val_acc: 0.9600\n",
      "Epoch [5580/10000], train loss: 0.2589 train acc: 0.9067 val_loss: 0.2433, val_acc: 0.9600\n",
      "Epoch [5590/10000], train loss: 0.2588 train acc: 0.9067 val_loss: 0.2432, val_acc: 0.9600\n",
      "Epoch [5600/10000], train loss: 0.2586 train acc: 0.9067 val_loss: 0.2430, val_acc: 0.9600\n",
      "Epoch [5610/10000], train loss: 0.2584 train acc: 0.9067 val_loss: 0.2428, val_acc: 0.9600\n",
      "Epoch [5620/10000], train loss: 0.2583 train acc: 0.9067 val_loss: 0.2427, val_acc: 0.9600\n",
      "Epoch [5630/10000], train loss: 0.2581 train acc: 0.9067 val_loss: 0.2425, val_acc: 0.9600\n",
      "Epoch [5640/10000], train loss: 0.2580 train acc: 0.9067 val_loss: 0.2424, val_acc: 0.9600\n",
      "Epoch [5650/10000], train loss: 0.2578 train acc: 0.9067 val_loss: 0.2422, val_acc: 0.9600\n",
      "Epoch [5660/10000], train loss: 0.2577 train acc: 0.9067 val_loss: 0.2420, val_acc: 0.9600\n",
      "Epoch [5670/10000], train loss: 0.2575 train acc: 0.9067 val_loss: 0.2419, val_acc: 0.9600\n",
      "Epoch [5680/10000], train loss: 0.2574 train acc: 0.9067 val_loss: 0.2417, val_acc: 0.9600\n",
      "Epoch [5690/10000], train loss: 0.2572 train acc: 0.9067 val_loss: 0.2416, val_acc: 0.9600\n",
      "Epoch [5700/10000], train loss: 0.2571 train acc: 0.9067 val_loss: 0.2414, val_acc: 0.9600\n",
      "Epoch [5710/10000], train loss: 0.2569 train acc: 0.9067 val_loss: 0.2413, val_acc: 0.9600\n",
      "Epoch [5720/10000], train loss: 0.2568 train acc: 0.9067 val_loss: 0.2411, val_acc: 0.9600\n",
      "Epoch [5730/10000], train loss: 0.2566 train acc: 0.9067 val_loss: 0.2409, val_acc: 0.9600\n",
      "Epoch [5740/10000], train loss: 0.2565 train acc: 0.9067 val_loss: 0.2408, val_acc: 0.9600\n",
      "Epoch [5750/10000], train loss: 0.2563 train acc: 0.9067 val_loss: 0.2406, val_acc: 0.9600\n",
      "Epoch [5760/10000], train loss: 0.2562 train acc: 0.9067 val_loss: 0.2405, val_acc: 0.9600\n",
      "Epoch [5770/10000], train loss: 0.2560 train acc: 0.9067 val_loss: 0.2403, val_acc: 0.9600\n",
      "Epoch [5780/10000], train loss: 0.2559 train acc: 0.9067 val_loss: 0.2402, val_acc: 0.9600\n",
      "Epoch [5790/10000], train loss: 0.2557 train acc: 0.9067 val_loss: 0.2400, val_acc: 0.9600\n",
      "Epoch [5800/10000], train loss: 0.2556 train acc: 0.9067 val_loss: 0.2399, val_acc: 0.9600\n",
      "Epoch [5810/10000], train loss: 0.2554 train acc: 0.9067 val_loss: 0.2397, val_acc: 0.9600\n",
      "Epoch [5820/10000], train loss: 0.2553 train acc: 0.9067 val_loss: 0.2396, val_acc: 0.9600\n",
      "Epoch [5830/10000], train loss: 0.2551 train acc: 0.9067 val_loss: 0.2394, val_acc: 0.9600\n",
      "Epoch [5840/10000], train loss: 0.2550 train acc: 0.9067 val_loss: 0.2392, val_acc: 0.9600\n",
      "Epoch [5850/10000], train loss: 0.2548 train acc: 0.9067 val_loss: 0.2391, val_acc: 0.9600\n",
      "Epoch [5860/10000], train loss: 0.2547 train acc: 0.9067 val_loss: 0.2389, val_acc: 0.9600\n",
      "Epoch [5870/10000], train loss: 0.2545 train acc: 0.9067 val_loss: 0.2388, val_acc: 0.9600\n",
      "Epoch [5880/10000], train loss: 0.2544 train acc: 0.9067 val_loss: 0.2386, val_acc: 0.9600\n",
      "Epoch [5890/10000], train loss: 0.2543 train acc: 0.9067 val_loss: 0.2385, val_acc: 0.9600\n",
      "Epoch [5900/10000], train loss: 0.2541 train acc: 0.9067 val_loss: 0.2383, val_acc: 0.9600\n",
      "Epoch [5910/10000], train loss: 0.2540 train acc: 0.9067 val_loss: 0.2382, val_acc: 0.9600\n",
      "Epoch [5920/10000], train loss: 0.2538 train acc: 0.9067 val_loss: 0.2380, val_acc: 0.9600\n",
      "Epoch [5930/10000], train loss: 0.2537 train acc: 0.9067 val_loss: 0.2379, val_acc: 0.9600\n",
      "Epoch [5940/10000], train loss: 0.2535 train acc: 0.9067 val_loss: 0.2377, val_acc: 0.9600\n",
      "Epoch [5950/10000], train loss: 0.2534 train acc: 0.9067 val_loss: 0.2376, val_acc: 0.9600\n",
      "Epoch [5960/10000], train loss: 0.2533 train acc: 0.9067 val_loss: 0.2375, val_acc: 0.9600\n",
      "Epoch [5970/10000], train loss: 0.2531 train acc: 0.9067 val_loss: 0.2373, val_acc: 0.9600\n",
      "Epoch [5980/10000], train loss: 0.2530 train acc: 0.9067 val_loss: 0.2372, val_acc: 0.9600\n",
      "Epoch [5990/10000], train loss: 0.2528 train acc: 0.9067 val_loss: 0.2370, val_acc: 0.9600\n",
      "Epoch [6000/10000], train loss: 0.2527 train acc: 0.9067 val_loss: 0.2369, val_acc: 0.9600\n",
      "Epoch [6010/10000], train loss: 0.2526 train acc: 0.9067 val_loss: 0.2367, val_acc: 0.9600\n",
      "Epoch [6020/10000], train loss: 0.2524 train acc: 0.9067 val_loss: 0.2366, val_acc: 0.9600\n",
      "Epoch [6030/10000], train loss: 0.2523 train acc: 0.9067 val_loss: 0.2364, val_acc: 0.9600\n",
      "Epoch [6040/10000], train loss: 0.2521 train acc: 0.9067 val_loss: 0.2363, val_acc: 0.9600\n",
      "Epoch [6050/10000], train loss: 0.2520 train acc: 0.9067 val_loss: 0.2361, val_acc: 0.9600\n",
      "Epoch [6060/10000], train loss: 0.2519 train acc: 0.9067 val_loss: 0.2360, val_acc: 0.9600\n",
      "Epoch [6070/10000], train loss: 0.2517 train acc: 0.9067 val_loss: 0.2359, val_acc: 0.9600\n",
      "Epoch [6080/10000], train loss: 0.2516 train acc: 0.9067 val_loss: 0.2357, val_acc: 0.9600\n",
      "Epoch [6090/10000], train loss: 0.2514 train acc: 0.9067 val_loss: 0.2356, val_acc: 0.9600\n",
      "Epoch [6100/10000], train loss: 0.2513 train acc: 0.9067 val_loss: 0.2354, val_acc: 0.9600\n",
      "Epoch [6110/10000], train loss: 0.2512 train acc: 0.9067 val_loss: 0.2353, val_acc: 0.9600\n",
      "Epoch [6120/10000], train loss: 0.2510 train acc: 0.9067 val_loss: 0.2351, val_acc: 0.9600\n",
      "Epoch [6130/10000], train loss: 0.2509 train acc: 0.9067 val_loss: 0.2350, val_acc: 0.9600\n",
      "Epoch [6140/10000], train loss: 0.2508 train acc: 0.9067 val_loss: 0.2349, val_acc: 0.9600\n",
      "Epoch [6150/10000], train loss: 0.2506 train acc: 0.9067 val_loss: 0.2347, val_acc: 0.9600\n",
      "Epoch [6160/10000], train loss: 0.2505 train acc: 0.9067 val_loss: 0.2346, val_acc: 0.9600\n",
      "Epoch [6170/10000], train loss: 0.2504 train acc: 0.9067 val_loss: 0.2344, val_acc: 0.9600\n",
      "Epoch [6180/10000], train loss: 0.2502 train acc: 0.9067 val_loss: 0.2343, val_acc: 0.9600\n",
      "Epoch [6190/10000], train loss: 0.2501 train acc: 0.9067 val_loss: 0.2342, val_acc: 0.9600\n",
      "Epoch [6200/10000], train loss: 0.2500 train acc: 0.9067 val_loss: 0.2340, val_acc: 0.9600\n",
      "Epoch [6210/10000], train loss: 0.2498 train acc: 0.9067 val_loss: 0.2339, val_acc: 0.9600\n",
      "Epoch [6220/10000], train loss: 0.2497 train acc: 0.9067 val_loss: 0.2337, val_acc: 0.9600\n",
      "Epoch [6230/10000], train loss: 0.2496 train acc: 0.9067 val_loss: 0.2336, val_acc: 0.9600\n",
      "Epoch [6240/10000], train loss: 0.2494 train acc: 0.9067 val_loss: 0.2335, val_acc: 0.9600\n",
      "Epoch [6250/10000], train loss: 0.2493 train acc: 0.9067 val_loss: 0.2333, val_acc: 0.9600\n",
      "Epoch [6260/10000], train loss: 0.2492 train acc: 0.9067 val_loss: 0.2332, val_acc: 0.9600\n",
      "Epoch [6270/10000], train loss: 0.2490 train acc: 0.9067 val_loss: 0.2331, val_acc: 0.9600\n",
      "Epoch [6280/10000], train loss: 0.2489 train acc: 0.9067 val_loss: 0.2329, val_acc: 0.9600\n",
      "Epoch [6290/10000], train loss: 0.2488 train acc: 0.9067 val_loss: 0.2328, val_acc: 0.9600\n",
      "Epoch [6300/10000], train loss: 0.2486 train acc: 0.9067 val_loss: 0.2327, val_acc: 0.9600\n",
      "Epoch [6310/10000], train loss: 0.2485 train acc: 0.9067 val_loss: 0.2325, val_acc: 0.9600\n",
      "Epoch [6320/10000], train loss: 0.2484 train acc: 0.9067 val_loss: 0.2324, val_acc: 0.9600\n",
      "Epoch [6330/10000], train loss: 0.2483 train acc: 0.9067 val_loss: 0.2322, val_acc: 0.9600\n",
      "Epoch [6340/10000], train loss: 0.2481 train acc: 0.9067 val_loss: 0.2321, val_acc: 0.9600\n",
      "Epoch [6350/10000], train loss: 0.2480 train acc: 0.9067 val_loss: 0.2320, val_acc: 0.9600\n",
      "Epoch [6360/10000], train loss: 0.2479 train acc: 0.9067 val_loss: 0.2318, val_acc: 0.9600\n",
      "Epoch [6370/10000], train loss: 0.2477 train acc: 0.9067 val_loss: 0.2317, val_acc: 0.9600\n",
      "Epoch [6380/10000], train loss: 0.2476 train acc: 0.9067 val_loss: 0.2316, val_acc: 0.9600\n",
      "Epoch [6390/10000], train loss: 0.2475 train acc: 0.9067 val_loss: 0.2314, val_acc: 0.9600\n",
      "Epoch [6400/10000], train loss: 0.2474 train acc: 0.9067 val_loss: 0.2313, val_acc: 0.9600\n",
      "Epoch [6410/10000], train loss: 0.2472 train acc: 0.9067 val_loss: 0.2312, val_acc: 0.9600\n",
      "Epoch [6420/10000], train loss: 0.2471 train acc: 0.9067 val_loss: 0.2310, val_acc: 0.9600\n",
      "Epoch [6430/10000], train loss: 0.2470 train acc: 0.9067 val_loss: 0.2309, val_acc: 0.9600\n",
      "Epoch [6440/10000], train loss: 0.2469 train acc: 0.9067 val_loss: 0.2308, val_acc: 0.9600\n",
      "Epoch [6450/10000], train loss: 0.2467 train acc: 0.9067 val_loss: 0.2307, val_acc: 0.9600\n",
      "Epoch [6460/10000], train loss: 0.2466 train acc: 0.9067 val_loss: 0.2305, val_acc: 0.9600\n",
      "Epoch [6470/10000], train loss: 0.2465 train acc: 0.9067 val_loss: 0.2304, val_acc: 0.9600\n",
      "Epoch [6480/10000], train loss: 0.2464 train acc: 0.9067 val_loss: 0.2303, val_acc: 0.9600\n",
      "Epoch [6490/10000], train loss: 0.2462 train acc: 0.9067 val_loss: 0.2301, val_acc: 0.9600\n",
      "Epoch [6500/10000], train loss: 0.2461 train acc: 0.9067 val_loss: 0.2300, val_acc: 0.9600\n",
      "Epoch [6510/10000], train loss: 0.2460 train acc: 0.9067 val_loss: 0.2299, val_acc: 0.9600\n",
      "Epoch [6520/10000], train loss: 0.2459 train acc: 0.9067 val_loss: 0.2297, val_acc: 0.9600\n",
      "Epoch [6530/10000], train loss: 0.2457 train acc: 0.9067 val_loss: 0.2296, val_acc: 0.9600\n",
      "Epoch [6540/10000], train loss: 0.2456 train acc: 0.9067 val_loss: 0.2295, val_acc: 0.9600\n",
      "Epoch [6550/10000], train loss: 0.2455 train acc: 0.9067 val_loss: 0.2294, val_acc: 0.9600\n",
      "Epoch [6560/10000], train loss: 0.2454 train acc: 0.9067 val_loss: 0.2292, val_acc: 0.9600\n",
      "Epoch [6570/10000], train loss: 0.2453 train acc: 0.9067 val_loss: 0.2291, val_acc: 0.9600\n",
      "Epoch [6580/10000], train loss: 0.2451 train acc: 0.9067 val_loss: 0.2290, val_acc: 0.9600\n",
      "Epoch [6590/10000], train loss: 0.2450 train acc: 0.9067 val_loss: 0.2289, val_acc: 0.9600\n",
      "Epoch [6600/10000], train loss: 0.2449 train acc: 0.9067 val_loss: 0.2287, val_acc: 0.9600\n",
      "Epoch [6610/10000], train loss: 0.2448 train acc: 0.9067 val_loss: 0.2286, val_acc: 0.9600\n",
      "Epoch [6620/10000], train loss: 0.2447 train acc: 0.9067 val_loss: 0.2285, val_acc: 0.9600\n",
      "Epoch [6630/10000], train loss: 0.2445 train acc: 0.9067 val_loss: 0.2284, val_acc: 0.9600\n",
      "Epoch [6640/10000], train loss: 0.2444 train acc: 0.9067 val_loss: 0.2282, val_acc: 0.9600\n",
      "Epoch [6650/10000], train loss: 0.2443 train acc: 0.9067 val_loss: 0.2281, val_acc: 0.9600\n",
      "Epoch [6660/10000], train loss: 0.2442 train acc: 0.9067 val_loss: 0.2280, val_acc: 0.9600\n",
      "Epoch [6670/10000], train loss: 0.2441 train acc: 0.9067 val_loss: 0.2279, val_acc: 0.9600\n",
      "Epoch [6680/10000], train loss: 0.2439 train acc: 0.9067 val_loss: 0.2277, val_acc: 0.9600\n",
      "Epoch [6690/10000], train loss: 0.2438 train acc: 0.9067 val_loss: 0.2276, val_acc: 0.9600\n",
      "Epoch [6700/10000], train loss: 0.2437 train acc: 0.9067 val_loss: 0.2275, val_acc: 0.9600\n",
      "Epoch [6710/10000], train loss: 0.2436 train acc: 0.9067 val_loss: 0.2274, val_acc: 0.9600\n",
      "Epoch [6720/10000], train loss: 0.2435 train acc: 0.9067 val_loss: 0.2272, val_acc: 0.9600\n",
      "Epoch [6730/10000], train loss: 0.2434 train acc: 0.9067 val_loss: 0.2271, val_acc: 0.9600\n",
      "Epoch [6740/10000], train loss: 0.2432 train acc: 0.9067 val_loss: 0.2270, val_acc: 0.9600\n",
      "Epoch [6750/10000], train loss: 0.2431 train acc: 0.9067 val_loss: 0.2269, val_acc: 0.9600\n",
      "Epoch [6760/10000], train loss: 0.2430 train acc: 0.9067 val_loss: 0.2267, val_acc: 0.9600\n",
      "Epoch [6770/10000], train loss: 0.2429 train acc: 0.9067 val_loss: 0.2266, val_acc: 0.9600\n",
      "Epoch [6780/10000], train loss: 0.2428 train acc: 0.9067 val_loss: 0.2265, val_acc: 0.9600\n",
      "Epoch [6790/10000], train loss: 0.2427 train acc: 0.9067 val_loss: 0.2264, val_acc: 0.9600\n",
      "Epoch [6800/10000], train loss: 0.2425 train acc: 0.9067 val_loss: 0.2263, val_acc: 0.9600\n",
      "Epoch [6810/10000], train loss: 0.2424 train acc: 0.9067 val_loss: 0.2261, val_acc: 0.9600\n",
      "Epoch [6820/10000], train loss: 0.2423 train acc: 0.9067 val_loss: 0.2260, val_acc: 0.9600\n",
      "Epoch [6830/10000], train loss: 0.2422 train acc: 0.9067 val_loss: 0.2259, val_acc: 0.9600\n",
      "Epoch [6840/10000], train loss: 0.2421 train acc: 0.9067 val_loss: 0.2258, val_acc: 0.9600\n",
      "Epoch [6850/10000], train loss: 0.2420 train acc: 0.9067 val_loss: 0.2257, val_acc: 0.9600\n",
      "Epoch [6860/10000], train loss: 0.2419 train acc: 0.9067 val_loss: 0.2255, val_acc: 0.9600\n",
      "Epoch [6870/10000], train loss: 0.2417 train acc: 0.9067 val_loss: 0.2254, val_acc: 0.9600\n",
      "Epoch [6880/10000], train loss: 0.2416 train acc: 0.9067 val_loss: 0.2253, val_acc: 0.9600\n",
      "Epoch [6890/10000], train loss: 0.2415 train acc: 0.9067 val_loss: 0.2252, val_acc: 0.9600\n",
      "Epoch [6900/10000], train loss: 0.2414 train acc: 0.9067 val_loss: 0.2251, val_acc: 0.9600\n",
      "Epoch [6910/10000], train loss: 0.2413 train acc: 0.9067 val_loss: 0.2250, val_acc: 0.9600\n",
      "Epoch [6920/10000], train loss: 0.2412 train acc: 0.9067 val_loss: 0.2248, val_acc: 0.9600\n",
      "Epoch [6930/10000], train loss: 0.2411 train acc: 0.9067 val_loss: 0.2247, val_acc: 0.9600\n",
      "Epoch [6940/10000], train loss: 0.2410 train acc: 0.9067 val_loss: 0.2246, val_acc: 0.9600\n",
      "Epoch [6950/10000], train loss: 0.2408 train acc: 0.9067 val_loss: 0.2245, val_acc: 0.9600\n",
      "Epoch [6960/10000], train loss: 0.2407 train acc: 0.9067 val_loss: 0.2244, val_acc: 0.9600\n",
      "Epoch [6970/10000], train loss: 0.2406 train acc: 0.9067 val_loss: 0.2243, val_acc: 0.9600\n",
      "Epoch [6980/10000], train loss: 0.2405 train acc: 0.9067 val_loss: 0.2241, val_acc: 0.9600\n",
      "Epoch [6990/10000], train loss: 0.2404 train acc: 0.9067 val_loss: 0.2240, val_acc: 0.9600\n",
      "Epoch [7000/10000], train loss: 0.2403 train acc: 0.9067 val_loss: 0.2239, val_acc: 0.9600\n",
      "Epoch [7010/10000], train loss: 0.2402 train acc: 0.9067 val_loss: 0.2238, val_acc: 0.9600\n",
      "Epoch [7020/10000], train loss: 0.2401 train acc: 0.9067 val_loss: 0.2237, val_acc: 0.9600\n",
      "Epoch [7030/10000], train loss: 0.2400 train acc: 0.9067 val_loss: 0.2236, val_acc: 0.9600\n",
      "Epoch [7040/10000], train loss: 0.2399 train acc: 0.9067 val_loss: 0.2234, val_acc: 0.9600\n",
      "Epoch [7050/10000], train loss: 0.2398 train acc: 0.9067 val_loss: 0.2233, val_acc: 0.9600\n",
      "Epoch [7060/10000], train loss: 0.2396 train acc: 0.9067 val_loss: 0.2232, val_acc: 0.9600\n",
      "Epoch [7070/10000], train loss: 0.2395 train acc: 0.9067 val_loss: 0.2231, val_acc: 0.9600\n",
      "Epoch [7080/10000], train loss: 0.2394 train acc: 0.9067 val_loss: 0.2230, val_acc: 0.9600\n",
      "Epoch [7090/10000], train loss: 0.2393 train acc: 0.9067 val_loss: 0.2229, val_acc: 0.9600\n",
      "Epoch [7100/10000], train loss: 0.2392 train acc: 0.9067 val_loss: 0.2228, val_acc: 0.9600\n",
      "Epoch [7110/10000], train loss: 0.2391 train acc: 0.9067 val_loss: 0.2227, val_acc: 0.9600\n",
      "Epoch [7120/10000], train loss: 0.2390 train acc: 0.9067 val_loss: 0.2225, val_acc: 0.9600\n",
      "Epoch [7130/10000], train loss: 0.2389 train acc: 0.9067 val_loss: 0.2224, val_acc: 0.9600\n",
      "Epoch [7140/10000], train loss: 0.2388 train acc: 0.9067 val_loss: 0.2223, val_acc: 0.9600\n",
      "Epoch [7150/10000], train loss: 0.2387 train acc: 0.9067 val_loss: 0.2222, val_acc: 0.9600\n",
      "Epoch [7160/10000], train loss: 0.2386 train acc: 0.9067 val_loss: 0.2221, val_acc: 0.9600\n",
      "Epoch [7170/10000], train loss: 0.2385 train acc: 0.9067 val_loss: 0.2220, val_acc: 0.9600\n",
      "Epoch [7180/10000], train loss: 0.2384 train acc: 0.9067 val_loss: 0.2219, val_acc: 0.9600\n",
      "Epoch [7190/10000], train loss: 0.2383 train acc: 0.9067 val_loss: 0.2218, val_acc: 0.9600\n",
      "Epoch [7200/10000], train loss: 0.2382 train acc: 0.9067 val_loss: 0.2217, val_acc: 0.9600\n",
      "Epoch [7210/10000], train loss: 0.2380 train acc: 0.9067 val_loss: 0.2215, val_acc: 0.9600\n",
      "Epoch [7220/10000], train loss: 0.2379 train acc: 0.9067 val_loss: 0.2214, val_acc: 0.9600\n",
      "Epoch [7230/10000], train loss: 0.2378 train acc: 0.9067 val_loss: 0.2213, val_acc: 0.9600\n",
      "Epoch [7240/10000], train loss: 0.2377 train acc: 0.9067 val_loss: 0.2212, val_acc: 0.9600\n",
      "Epoch [7250/10000], train loss: 0.2376 train acc: 0.9067 val_loss: 0.2211, val_acc: 0.9600\n",
      "Epoch [7260/10000], train loss: 0.2375 train acc: 0.9067 val_loss: 0.2210, val_acc: 0.9600\n",
      "Epoch [7270/10000], train loss: 0.2374 train acc: 0.9067 val_loss: 0.2209, val_acc: 0.9600\n",
      "Epoch [7280/10000], train loss: 0.2373 train acc: 0.9067 val_loss: 0.2208, val_acc: 0.9600\n",
      "Epoch [7290/10000], train loss: 0.2372 train acc: 0.9067 val_loss: 0.2207, val_acc: 0.9600\n",
      "Epoch [7300/10000], train loss: 0.2371 train acc: 0.9067 val_loss: 0.2206, val_acc: 0.9600\n",
      "Epoch [7310/10000], train loss: 0.2370 train acc: 0.9067 val_loss: 0.2205, val_acc: 0.9600\n",
      "Epoch [7320/10000], train loss: 0.2369 train acc: 0.9067 val_loss: 0.2203, val_acc: 0.9600\n",
      "Epoch [7330/10000], train loss: 0.2368 train acc: 0.9067 val_loss: 0.2202, val_acc: 0.9600\n",
      "Epoch [7340/10000], train loss: 0.2367 train acc: 0.9067 val_loss: 0.2201, val_acc: 0.9600\n",
      "Epoch [7350/10000], train loss: 0.2366 train acc: 0.9067 val_loss: 0.2200, val_acc: 0.9600\n",
      "Epoch [7360/10000], train loss: 0.2365 train acc: 0.9067 val_loss: 0.2199, val_acc: 0.9600\n",
      "Epoch [7370/10000], train loss: 0.2364 train acc: 0.9067 val_loss: 0.2198, val_acc: 0.9600\n",
      "Epoch [7380/10000], train loss: 0.2363 train acc: 0.9067 val_loss: 0.2197, val_acc: 0.9600\n",
      "Epoch [7390/10000], train loss: 0.2362 train acc: 0.9067 val_loss: 0.2196, val_acc: 0.9600\n",
      "Epoch [7400/10000], train loss: 0.2361 train acc: 0.9067 val_loss: 0.2195, val_acc: 0.9600\n",
      "Epoch [7410/10000], train loss: 0.2360 train acc: 0.9067 val_loss: 0.2194, val_acc: 0.9600\n",
      "Epoch [7420/10000], train loss: 0.2359 train acc: 0.9067 val_loss: 0.2193, val_acc: 0.9600\n",
      "Epoch [7430/10000], train loss: 0.2358 train acc: 0.9067 val_loss: 0.2192, val_acc: 0.9600\n",
      "Epoch [7440/10000], train loss: 0.2357 train acc: 0.9067 val_loss: 0.2191, val_acc: 0.9600\n",
      "Epoch [7450/10000], train loss: 0.2356 train acc: 0.9067 val_loss: 0.2190, val_acc: 0.9600\n",
      "Epoch [7460/10000], train loss: 0.2355 train acc: 0.9067 val_loss: 0.2189, val_acc: 0.9600\n",
      "Epoch [7470/10000], train loss: 0.2354 train acc: 0.9067 val_loss: 0.2188, val_acc: 0.9600\n",
      "Epoch [7480/10000], train loss: 0.2353 train acc: 0.9067 val_loss: 0.2187, val_acc: 0.9600\n",
      "Epoch [7490/10000], train loss: 0.2352 train acc: 0.9067 val_loss: 0.2185, val_acc: 0.9600\n",
      "Epoch [7500/10000], train loss: 0.2351 train acc: 0.9067 val_loss: 0.2184, val_acc: 0.9600\n",
      "Epoch [7510/10000], train loss: 0.2350 train acc: 0.9067 val_loss: 0.2183, val_acc: 0.9600\n",
      "Epoch [7520/10000], train loss: 0.2349 train acc: 0.9067 val_loss: 0.2182, val_acc: 0.9600\n",
      "Epoch [7530/10000], train loss: 0.2348 train acc: 0.9067 val_loss: 0.2181, val_acc: 0.9600\n",
      "Epoch [7540/10000], train loss: 0.2347 train acc: 0.9067 val_loss: 0.2180, val_acc: 0.9600\n",
      "Epoch [7550/10000], train loss: 0.2346 train acc: 0.9067 val_loss: 0.2179, val_acc: 0.9600\n",
      "Epoch [7560/10000], train loss: 0.2345 train acc: 0.9067 val_loss: 0.2178, val_acc: 0.9600\n",
      "Epoch [7570/10000], train loss: 0.2344 train acc: 0.9067 val_loss: 0.2177, val_acc: 0.9600\n",
      "Epoch [7580/10000], train loss: 0.2343 train acc: 0.9067 val_loss: 0.2176, val_acc: 0.9600\n",
      "Epoch [7590/10000], train loss: 0.2342 train acc: 0.9067 val_loss: 0.2175, val_acc: 0.9600\n",
      "Epoch [7600/10000], train loss: 0.2341 train acc: 0.9067 val_loss: 0.2174, val_acc: 0.9600\n",
      "Epoch [7610/10000], train loss: 0.2340 train acc: 0.9067 val_loss: 0.2173, val_acc: 0.9600\n",
      "Epoch [7620/10000], train loss: 0.2340 train acc: 0.9067 val_loss: 0.2172, val_acc: 0.9600\n",
      "Epoch [7630/10000], train loss: 0.2339 train acc: 0.9067 val_loss: 0.2171, val_acc: 0.9600\n",
      "Epoch [7640/10000], train loss: 0.2338 train acc: 0.9067 val_loss: 0.2170, val_acc: 0.9600\n",
      "Epoch [7650/10000], train loss: 0.2337 train acc: 0.9067 val_loss: 0.2169, val_acc: 0.9600\n",
      "Epoch [7660/10000], train loss: 0.2336 train acc: 0.9067 val_loss: 0.2168, val_acc: 0.9600\n",
      "Epoch [7670/10000], train loss: 0.2335 train acc: 0.9067 val_loss: 0.2167, val_acc: 0.9600\n",
      "Epoch [7680/10000], train loss: 0.2334 train acc: 0.9067 val_loss: 0.2166, val_acc: 0.9600\n",
      "Epoch [7690/10000], train loss: 0.2333 train acc: 0.9067 val_loss: 0.2165, val_acc: 0.9600\n",
      "Epoch [7700/10000], train loss: 0.2332 train acc: 0.9067 val_loss: 0.2164, val_acc: 0.9600\n",
      "Epoch [7710/10000], train loss: 0.2331 train acc: 0.9067 val_loss: 0.2163, val_acc: 0.9600\n",
      "Epoch [7720/10000], train loss: 0.2330 train acc: 0.9067 val_loss: 0.2162, val_acc: 0.9600\n",
      "Epoch [7730/10000], train loss: 0.2329 train acc: 0.9067 val_loss: 0.2161, val_acc: 0.9600\n",
      "Epoch [7740/10000], train loss: 0.2328 train acc: 0.9067 val_loss: 0.2160, val_acc: 0.9600\n",
      "Epoch [7750/10000], train loss: 0.2327 train acc: 0.9067 val_loss: 0.2159, val_acc: 0.9600\n",
      "Epoch [7760/10000], train loss: 0.2326 train acc: 0.9067 val_loss: 0.2158, val_acc: 0.9600\n",
      "Epoch [7770/10000], train loss: 0.2325 train acc: 0.9067 val_loss: 0.2157, val_acc: 0.9600\n",
      "Epoch [7780/10000], train loss: 0.2324 train acc: 0.9067 val_loss: 0.2156, val_acc: 0.9600\n",
      "Epoch [7790/10000], train loss: 0.2324 train acc: 0.9067 val_loss: 0.2155, val_acc: 0.9600\n",
      "Epoch [7800/10000], train loss: 0.2323 train acc: 0.9067 val_loss: 0.2154, val_acc: 0.9600\n",
      "Epoch [7810/10000], train loss: 0.2322 train acc: 0.9067 val_loss: 0.2153, val_acc: 0.9600\n",
      "Epoch [7820/10000], train loss: 0.2321 train acc: 0.9067 val_loss: 0.2152, val_acc: 0.9600\n",
      "Epoch [7830/10000], train loss: 0.2320 train acc: 0.9067 val_loss: 0.2151, val_acc: 0.9600\n",
      "Epoch [7840/10000], train loss: 0.2319 train acc: 0.9067 val_loss: 0.2150, val_acc: 0.9600\n",
      "Epoch [7850/10000], train loss: 0.2318 train acc: 0.9067 val_loss: 0.2149, val_acc: 0.9600\n",
      "Epoch [7860/10000], train loss: 0.2317 train acc: 0.9067 val_loss: 0.2149, val_acc: 0.9600\n",
      "Epoch [7870/10000], train loss: 0.2316 train acc: 0.9067 val_loss: 0.2148, val_acc: 0.9600\n",
      "Epoch [7880/10000], train loss: 0.2315 train acc: 0.9067 val_loss: 0.2147, val_acc: 0.9600\n",
      "Epoch [7890/10000], train loss: 0.2314 train acc: 0.9067 val_loss: 0.2146, val_acc: 0.9600\n",
      "Epoch [7900/10000], train loss: 0.2314 train acc: 0.9067 val_loss: 0.2145, val_acc: 0.9600\n",
      "Epoch [7910/10000], train loss: 0.2313 train acc: 0.9067 val_loss: 0.2144, val_acc: 0.9600\n",
      "Epoch [7920/10000], train loss: 0.2312 train acc: 0.9067 val_loss: 0.2143, val_acc: 0.9600\n",
      "Epoch [7930/10000], train loss: 0.2311 train acc: 0.9067 val_loss: 0.2142, val_acc: 0.9600\n",
      "Epoch [7940/10000], train loss: 0.2310 train acc: 0.9067 val_loss: 0.2141, val_acc: 0.9600\n",
      "Epoch [7950/10000], train loss: 0.2309 train acc: 0.9067 val_loss: 0.2140, val_acc: 0.9600\n",
      "Epoch [7960/10000], train loss: 0.2308 train acc: 0.9067 val_loss: 0.2139, val_acc: 0.9600\n",
      "Epoch [7970/10000], train loss: 0.2307 train acc: 0.9067 val_loss: 0.2138, val_acc: 0.9600\n",
      "Epoch [7980/10000], train loss: 0.2306 train acc: 0.9067 val_loss: 0.2137, val_acc: 0.9600\n",
      "Epoch [7990/10000], train loss: 0.2306 train acc: 0.9067 val_loss: 0.2136, val_acc: 0.9600\n",
      "Epoch [8000/10000], train loss: 0.2305 train acc: 0.9067 val_loss: 0.2135, val_acc: 0.9600\n",
      "Epoch [8010/10000], train loss: 0.2304 train acc: 0.9067 val_loss: 0.2134, val_acc: 0.9600\n",
      "Epoch [8020/10000], train loss: 0.2303 train acc: 0.9067 val_loss: 0.2133, val_acc: 0.9600\n",
      "Epoch [8030/10000], train loss: 0.2302 train acc: 0.9067 val_loss: 0.2132, val_acc: 0.9600\n",
      "Epoch [8040/10000], train loss: 0.2301 train acc: 0.9067 val_loss: 0.2132, val_acc: 0.9600\n",
      "Epoch [8050/10000], train loss: 0.2300 train acc: 0.9067 val_loss: 0.2131, val_acc: 0.9600\n",
      "Epoch [8060/10000], train loss: 0.2299 train acc: 0.9067 val_loss: 0.2130, val_acc: 0.9600\n",
      "Epoch [8070/10000], train loss: 0.2298 train acc: 0.9067 val_loss: 0.2129, val_acc: 0.9600\n",
      "Epoch [8080/10000], train loss: 0.2298 train acc: 0.9067 val_loss: 0.2128, val_acc: 0.9600\n",
      "Epoch [8090/10000], train loss: 0.2297 train acc: 0.9067 val_loss: 0.2127, val_acc: 0.9600\n",
      "Epoch [8100/10000], train loss: 0.2296 train acc: 0.9067 val_loss: 0.2126, val_acc: 0.9600\n",
      "Epoch [8110/10000], train loss: 0.2295 train acc: 0.9067 val_loss: 0.2125, val_acc: 0.9600\n",
      "Epoch [8120/10000], train loss: 0.2294 train acc: 0.9067 val_loss: 0.2124, val_acc: 0.9600\n",
      "Epoch [8130/10000], train loss: 0.2293 train acc: 0.9067 val_loss: 0.2123, val_acc: 0.9600\n",
      "Epoch [8140/10000], train loss: 0.2292 train acc: 0.9067 val_loss: 0.2122, val_acc: 0.9600\n",
      "Epoch [8150/10000], train loss: 0.2292 train acc: 0.9067 val_loss: 0.2121, val_acc: 0.9600\n",
      "Epoch [8160/10000], train loss: 0.2291 train acc: 0.9067 val_loss: 0.2121, val_acc: 0.9600\n",
      "Epoch [8170/10000], train loss: 0.2290 train acc: 0.9067 val_loss: 0.2120, val_acc: 0.9600\n",
      "Epoch [8180/10000], train loss: 0.2289 train acc: 0.9067 val_loss: 0.2119, val_acc: 0.9600\n",
      "Epoch [8190/10000], train loss: 0.2288 train acc: 0.9067 val_loss: 0.2118, val_acc: 0.9600\n",
      "Epoch [8200/10000], train loss: 0.2287 train acc: 0.9067 val_loss: 0.2117, val_acc: 0.9600\n",
      "Epoch [8210/10000], train loss: 0.2286 train acc: 0.9067 val_loss: 0.2116, val_acc: 0.9600\n",
      "Epoch [8220/10000], train loss: 0.2286 train acc: 0.9067 val_loss: 0.2115, val_acc: 0.9600\n",
      "Epoch [8230/10000], train loss: 0.2285 train acc: 0.9067 val_loss: 0.2114, val_acc: 0.9600\n",
      "Epoch [8240/10000], train loss: 0.2284 train acc: 0.9067 val_loss: 0.2113, val_acc: 0.9600\n",
      "Epoch [8250/10000], train loss: 0.2283 train acc: 0.9067 val_loss: 0.2112, val_acc: 0.9600\n",
      "Epoch [8260/10000], train loss: 0.2282 train acc: 0.9067 val_loss: 0.2112, val_acc: 0.9600\n",
      "Epoch [8270/10000], train loss: 0.2281 train acc: 0.9067 val_loss: 0.2111, val_acc: 0.9600\n",
      "Epoch [8280/10000], train loss: 0.2281 train acc: 0.9067 val_loss: 0.2110, val_acc: 0.9600\n",
      "Epoch [8290/10000], train loss: 0.2280 train acc: 0.9067 val_loss: 0.2109, val_acc: 0.9600\n",
      "Epoch [8300/10000], train loss: 0.2279 train acc: 0.9067 val_loss: 0.2108, val_acc: 0.9600\n",
      "Epoch [8310/10000], train loss: 0.2278 train acc: 0.9067 val_loss: 0.2107, val_acc: 0.9600\n",
      "Epoch [8320/10000], train loss: 0.2277 train acc: 0.9067 val_loss: 0.2106, val_acc: 0.9600\n",
      "Epoch [8330/10000], train loss: 0.2276 train acc: 0.9067 val_loss: 0.2105, val_acc: 0.9600\n",
      "Epoch [8340/10000], train loss: 0.2276 train acc: 0.9067 val_loss: 0.2104, val_acc: 0.9600\n",
      "Epoch [8350/10000], train loss: 0.2275 train acc: 0.9067 val_loss: 0.2104, val_acc: 0.9600\n",
      "Epoch [8360/10000], train loss: 0.2274 train acc: 0.9067 val_loss: 0.2103, val_acc: 0.9600\n",
      "Epoch [8370/10000], train loss: 0.2273 train acc: 0.9067 val_loss: 0.2102, val_acc: 0.9600\n",
      "Epoch [8380/10000], train loss: 0.2272 train acc: 0.9067 val_loss: 0.2101, val_acc: 0.9600\n",
      "Epoch [8390/10000], train loss: 0.2272 train acc: 0.9067 val_loss: 0.2100, val_acc: 0.9600\n",
      "Epoch [8400/10000], train loss: 0.2271 train acc: 0.9067 val_loss: 0.2099, val_acc: 0.9600\n",
      "Epoch [8410/10000], train loss: 0.2270 train acc: 0.9067 val_loss: 0.2098, val_acc: 0.9600\n",
      "Epoch [8420/10000], train loss: 0.2269 train acc: 0.9067 val_loss: 0.2098, val_acc: 0.9600\n",
      "Epoch [8430/10000], train loss: 0.2268 train acc: 0.9067 val_loss: 0.2097, val_acc: 0.9600\n",
      "Epoch [8440/10000], train loss: 0.2268 train acc: 0.9067 val_loss: 0.2096, val_acc: 0.9600\n",
      "Epoch [8450/10000], train loss: 0.2267 train acc: 0.9067 val_loss: 0.2095, val_acc: 0.9600\n",
      "Epoch [8460/10000], train loss: 0.2266 train acc: 0.9067 val_loss: 0.2094, val_acc: 0.9600\n",
      "Epoch [8470/10000], train loss: 0.2265 train acc: 0.9067 val_loss: 0.2093, val_acc: 0.9600\n",
      "Epoch [8480/10000], train loss: 0.2264 train acc: 0.9067 val_loss: 0.2092, val_acc: 0.9600\n",
      "Epoch [8490/10000], train loss: 0.2263 train acc: 0.9067 val_loss: 0.2092, val_acc: 0.9600\n",
      "Epoch [8500/10000], train loss: 0.2263 train acc: 0.9067 val_loss: 0.2091, val_acc: 0.9600\n",
      "Epoch [8510/10000], train loss: 0.2262 train acc: 0.9067 val_loss: 0.2090, val_acc: 0.9600\n",
      "Epoch [8520/10000], train loss: 0.2261 train acc: 0.9067 val_loss: 0.2089, val_acc: 0.9600\n",
      "Epoch [8530/10000], train loss: 0.2260 train acc: 0.9067 val_loss: 0.2088, val_acc: 0.9600\n",
      "Epoch [8540/10000], train loss: 0.2260 train acc: 0.9067 val_loss: 0.2087, val_acc: 0.9600\n",
      "Epoch [8550/10000], train loss: 0.2259 train acc: 0.9067 val_loss: 0.2086, val_acc: 0.9600\n",
      "Epoch [8560/10000], train loss: 0.2258 train acc: 0.9067 val_loss: 0.2086, val_acc: 0.9600\n",
      "Epoch [8570/10000], train loss: 0.2257 train acc: 0.9067 val_loss: 0.2085, val_acc: 0.9600\n",
      "Epoch [8580/10000], train loss: 0.2256 train acc: 0.9067 val_loss: 0.2084, val_acc: 0.9600\n",
      "Epoch [8590/10000], train loss: 0.2256 train acc: 0.9067 val_loss: 0.2083, val_acc: 0.9600\n",
      "Epoch [8600/10000], train loss: 0.2255 train acc: 0.9067 val_loss: 0.2082, val_acc: 0.9600\n",
      "Epoch [8610/10000], train loss: 0.2254 train acc: 0.9067 val_loss: 0.2081, val_acc: 0.9600\n",
      "Epoch [8620/10000], train loss: 0.2253 train acc: 0.9067 val_loss: 0.2081, val_acc: 0.9600\n",
      "Epoch [8630/10000], train loss: 0.2252 train acc: 0.9067 val_loss: 0.2080, val_acc: 0.9600\n",
      "Epoch [8640/10000], train loss: 0.2252 train acc: 0.9067 val_loss: 0.2079, val_acc: 0.9600\n",
      "Epoch [8650/10000], train loss: 0.2251 train acc: 0.9067 val_loss: 0.2078, val_acc: 0.9600\n",
      "Epoch [8660/10000], train loss: 0.2250 train acc: 0.9067 val_loss: 0.2077, val_acc: 0.9600\n",
      "Epoch [8670/10000], train loss: 0.2249 train acc: 0.9067 val_loss: 0.2076, val_acc: 0.9600\n",
      "Epoch [8680/10000], train loss: 0.2249 train acc: 0.9067 val_loss: 0.2076, val_acc: 0.9600\n",
      "Epoch [8690/10000], train loss: 0.2248 train acc: 0.9067 val_loss: 0.2075, val_acc: 0.9600\n",
      "Epoch [8700/10000], train loss: 0.2247 train acc: 0.9067 val_loss: 0.2074, val_acc: 0.9600\n",
      "Epoch [8710/10000], train loss: 0.2246 train acc: 0.9067 val_loss: 0.2073, val_acc: 0.9600\n",
      "Epoch [8720/10000], train loss: 0.2246 train acc: 0.9067 val_loss: 0.2072, val_acc: 0.9600\n",
      "Epoch [8730/10000], train loss: 0.2245 train acc: 0.9067 val_loss: 0.2072, val_acc: 0.9600\n",
      "Epoch [8740/10000], train loss: 0.2244 train acc: 0.9067 val_loss: 0.2071, val_acc: 0.9600\n",
      "Epoch [8750/10000], train loss: 0.2243 train acc: 0.9067 val_loss: 0.2070, val_acc: 0.9600\n",
      "Epoch [8760/10000], train loss: 0.2242 train acc: 0.9067 val_loss: 0.2069, val_acc: 0.9600\n",
      "Epoch [8770/10000], train loss: 0.2242 train acc: 0.9067 val_loss: 0.2068, val_acc: 0.9600\n",
      "Epoch [8780/10000], train loss: 0.2241 train acc: 0.9067 val_loss: 0.2068, val_acc: 0.9600\n",
      "Epoch [8790/10000], train loss: 0.2240 train acc: 0.9067 val_loss: 0.2067, val_acc: 0.9600\n",
      "Epoch [8800/10000], train loss: 0.2239 train acc: 0.9067 val_loss: 0.2066, val_acc: 0.9600\n",
      "Epoch [8810/10000], train loss: 0.2239 train acc: 0.9067 val_loss: 0.2065, val_acc: 0.9600\n",
      "Epoch [8820/10000], train loss: 0.2238 train acc: 0.9067 val_loss: 0.2064, val_acc: 0.9600\n",
      "Epoch [8830/10000], train loss: 0.2237 train acc: 0.9067 val_loss: 0.2063, val_acc: 0.9600\n",
      "Epoch [8840/10000], train loss: 0.2236 train acc: 0.9067 val_loss: 0.2063, val_acc: 0.9600\n",
      "Epoch [8850/10000], train loss: 0.2236 train acc: 0.9067 val_loss: 0.2062, val_acc: 0.9600\n",
      "Epoch [8860/10000], train loss: 0.2235 train acc: 0.9067 val_loss: 0.2061, val_acc: 0.9600\n",
      "Epoch [8870/10000], train loss: 0.2234 train acc: 0.9067 val_loss: 0.2060, val_acc: 0.9600\n",
      "Epoch [8880/10000], train loss: 0.2233 train acc: 0.9067 val_loss: 0.2060, val_acc: 0.9600\n",
      "Epoch [8890/10000], train loss: 0.2233 train acc: 0.9067 val_loss: 0.2059, val_acc: 0.9600\n",
      "Epoch [8900/10000], train loss: 0.2232 train acc: 0.9067 val_loss: 0.2058, val_acc: 0.9600\n",
      "Epoch [8910/10000], train loss: 0.2231 train acc: 0.9067 val_loss: 0.2057, val_acc: 0.9600\n",
      "Epoch [8920/10000], train loss: 0.2231 train acc: 0.9067 val_loss: 0.2056, val_acc: 0.9600\n",
      "Epoch [8930/10000], train loss: 0.2230 train acc: 0.9067 val_loss: 0.2056, val_acc: 0.9600\n",
      "Epoch [8940/10000], train loss: 0.2229 train acc: 0.9067 val_loss: 0.2055, val_acc: 0.9600\n",
      "Epoch [8950/10000], train loss: 0.2228 train acc: 0.9067 val_loss: 0.2054, val_acc: 0.9600\n",
      "Epoch [8960/10000], train loss: 0.2228 train acc: 0.9067 val_loss: 0.2053, val_acc: 0.9600\n",
      "Epoch [8970/10000], train loss: 0.2227 train acc: 0.9067 val_loss: 0.2052, val_acc: 0.9600\n",
      "Epoch [8980/10000], train loss: 0.2226 train acc: 0.9067 val_loss: 0.2052, val_acc: 0.9600\n",
      "Epoch [8990/10000], train loss: 0.2225 train acc: 0.9067 val_loss: 0.2051, val_acc: 0.9600\n",
      "Epoch [9000/10000], train loss: 0.2225 train acc: 0.9067 val_loss: 0.2050, val_acc: 0.9600\n",
      "Epoch [9010/10000], train loss: 0.2224 train acc: 0.9067 val_loss: 0.2049, val_acc: 0.9600\n",
      "Epoch [9020/10000], train loss: 0.2223 train acc: 0.9067 val_loss: 0.2049, val_acc: 0.9600\n",
      "Epoch [9030/10000], train loss: 0.2223 train acc: 0.9067 val_loss: 0.2048, val_acc: 0.9600\n",
      "Epoch [9040/10000], train loss: 0.2222 train acc: 0.9067 val_loss: 0.2047, val_acc: 0.9600\n",
      "Epoch [9050/10000], train loss: 0.2221 train acc: 0.9067 val_loss: 0.2046, val_acc: 0.9600\n",
      "Epoch [9060/10000], train loss: 0.2220 train acc: 0.9067 val_loss: 0.2045, val_acc: 0.9600\n",
      "Epoch [9070/10000], train loss: 0.2220 train acc: 0.9067 val_loss: 0.2045, val_acc: 0.9600\n",
      "Epoch [9080/10000], train loss: 0.2219 train acc: 0.9067 val_loss: 0.2044, val_acc: 0.9600\n",
      "Epoch [9090/10000], train loss: 0.2218 train acc: 0.9067 val_loss: 0.2043, val_acc: 0.9600\n",
      "Epoch [9100/10000], train loss: 0.2218 train acc: 0.9067 val_loss: 0.2042, val_acc: 0.9600\n",
      "Epoch [9110/10000], train loss: 0.2217 train acc: 0.9067 val_loss: 0.2042, val_acc: 0.9600\n",
      "Epoch [9120/10000], train loss: 0.2216 train acc: 0.9067 val_loss: 0.2041, val_acc: 0.9600\n",
      "Epoch [9130/10000], train loss: 0.2215 train acc: 0.9067 val_loss: 0.2040, val_acc: 0.9600\n",
      "Epoch [9140/10000], train loss: 0.2215 train acc: 0.9067 val_loss: 0.2039, val_acc: 0.9600\n",
      "Epoch [9150/10000], train loss: 0.2214 train acc: 0.9067 val_loss: 0.2039, val_acc: 0.9600\n",
      "Epoch [9160/10000], train loss: 0.2213 train acc: 0.9067 val_loss: 0.2038, val_acc: 0.9600\n",
      "Epoch [9170/10000], train loss: 0.2213 train acc: 0.9067 val_loss: 0.2037, val_acc: 0.9600\n",
      "Epoch [9180/10000], train loss: 0.2212 train acc: 0.9067 val_loss: 0.2036, val_acc: 0.9600\n",
      "Epoch [9190/10000], train loss: 0.2211 train acc: 0.9067 val_loss: 0.2036, val_acc: 0.9600\n",
      "Epoch [9200/10000], train loss: 0.2210 train acc: 0.9067 val_loss: 0.2035, val_acc: 0.9600\n",
      "Epoch [9210/10000], train loss: 0.2210 train acc: 0.9067 val_loss: 0.2034, val_acc: 0.9600\n",
      "Epoch [9220/10000], train loss: 0.2209 train acc: 0.9067 val_loss: 0.2033, val_acc: 0.9600\n",
      "Epoch [9230/10000], train loss: 0.2208 train acc: 0.9067 val_loss: 0.2033, val_acc: 0.9600\n",
      "Epoch [9240/10000], train loss: 0.2208 train acc: 0.9067 val_loss: 0.2032, val_acc: 0.9600\n",
      "Epoch [9250/10000], train loss: 0.2207 train acc: 0.9067 val_loss: 0.2031, val_acc: 0.9600\n",
      "Epoch [9260/10000], train loss: 0.2206 train acc: 0.9067 val_loss: 0.2030, val_acc: 0.9600\n",
      "Epoch [9270/10000], train loss: 0.2206 train acc: 0.9067 val_loss: 0.2030, val_acc: 0.9600\n",
      "Epoch [9280/10000], train loss: 0.2205 train acc: 0.9067 val_loss: 0.2029, val_acc: 0.9600\n",
      "Epoch [9290/10000], train loss: 0.2204 train acc: 0.9067 val_loss: 0.2028, val_acc: 0.9600\n",
      "Epoch [9300/10000], train loss: 0.2204 train acc: 0.9067 val_loss: 0.2027, val_acc: 0.9600\n",
      "Epoch [9310/10000], train loss: 0.2203 train acc: 0.9067 val_loss: 0.2027, val_acc: 0.9600\n",
      "Epoch [9320/10000], train loss: 0.2202 train acc: 0.9067 val_loss: 0.2026, val_acc: 0.9600\n",
      "Epoch [9330/10000], train loss: 0.2201 train acc: 0.9067 val_loss: 0.2025, val_acc: 0.9600\n",
      "Epoch [9340/10000], train loss: 0.2201 train acc: 0.9067 val_loss: 0.2024, val_acc: 0.9600\n",
      "Epoch [9350/10000], train loss: 0.2200 train acc: 0.9067 val_loss: 0.2024, val_acc: 0.9600\n",
      "Epoch [9360/10000], train loss: 0.2199 train acc: 0.9067 val_loss: 0.2023, val_acc: 0.9600\n",
      "Epoch [9370/10000], train loss: 0.2199 train acc: 0.9067 val_loss: 0.2022, val_acc: 0.9600\n",
      "Epoch [9380/10000], train loss: 0.2198 train acc: 0.9067 val_loss: 0.2022, val_acc: 0.9600\n",
      "Epoch [9390/10000], train loss: 0.2197 train acc: 0.9067 val_loss: 0.2021, val_acc: 0.9600\n",
      "Epoch [9400/10000], train loss: 0.2197 train acc: 0.9067 val_loss: 0.2020, val_acc: 0.9600\n",
      "Epoch [9410/10000], train loss: 0.2196 train acc: 0.9067 val_loss: 0.2019, val_acc: 0.9600\n",
      "Epoch [9420/10000], train loss: 0.2195 train acc: 0.9067 val_loss: 0.2019, val_acc: 0.9600\n",
      "Epoch [9430/10000], train loss: 0.2195 train acc: 0.9067 val_loss: 0.2018, val_acc: 0.9600\n",
      "Epoch [9440/10000], train loss: 0.2194 train acc: 0.9067 val_loss: 0.2017, val_acc: 0.9600\n",
      "Epoch [9450/10000], train loss: 0.2193 train acc: 0.9067 val_loss: 0.2017, val_acc: 0.9600\n",
      "Epoch [9460/10000], train loss: 0.2193 train acc: 0.9067 val_loss: 0.2016, val_acc: 0.9600\n",
      "Epoch [9470/10000], train loss: 0.2192 train acc: 0.9067 val_loss: 0.2015, val_acc: 0.9600\n",
      "Epoch [9480/10000], train loss: 0.2191 train acc: 0.9067 val_loss: 0.2014, val_acc: 0.9600\n",
      "Epoch [9490/10000], train loss: 0.2191 train acc: 0.9067 val_loss: 0.2014, val_acc: 0.9600\n",
      "Epoch [9500/10000], train loss: 0.2190 train acc: 0.9067 val_loss: 0.2013, val_acc: 0.9600\n",
      "Epoch [9510/10000], train loss: 0.2189 train acc: 0.9067 val_loss: 0.2012, val_acc: 0.9600\n",
      "Epoch [9520/10000], train loss: 0.2189 train acc: 0.9067 val_loss: 0.2012, val_acc: 0.9600\n",
      "Epoch [9530/10000], train loss: 0.2188 train acc: 0.9067 val_loss: 0.2011, val_acc: 0.9600\n",
      "Epoch [9540/10000], train loss: 0.2187 train acc: 0.9067 val_loss: 0.2010, val_acc: 0.9600\n",
      "Epoch [9550/10000], train loss: 0.2187 train acc: 0.9067 val_loss: 0.2009, val_acc: 0.9600\n",
      "Epoch [9560/10000], train loss: 0.2186 train acc: 0.9067 val_loss: 0.2009, val_acc: 0.9600\n",
      "Epoch [9570/10000], train loss: 0.2185 train acc: 0.9067 val_loss: 0.2008, val_acc: 0.9600\n",
      "Epoch [9580/10000], train loss: 0.2185 train acc: 0.9067 val_loss: 0.2007, val_acc: 0.9600\n",
      "Epoch [9590/10000], train loss: 0.2184 train acc: 0.9067 val_loss: 0.2007, val_acc: 0.9600\n",
      "Epoch [9600/10000], train loss: 0.2183 train acc: 0.9067 val_loss: 0.2006, val_acc: 0.9600\n",
      "Epoch [9610/10000], train loss: 0.2183 train acc: 0.9067 val_loss: 0.2005, val_acc: 0.9600\n",
      "Epoch [9620/10000], train loss: 0.2182 train acc: 0.9067 val_loss: 0.2005, val_acc: 0.9600\n",
      "Epoch [9630/10000], train loss: 0.2182 train acc: 0.9067 val_loss: 0.2004, val_acc: 0.9600\n",
      "Epoch [9640/10000], train loss: 0.2181 train acc: 0.9067 val_loss: 0.2003, val_acc: 0.9600\n",
      "Epoch [9650/10000], train loss: 0.2180 train acc: 0.9067 val_loss: 0.2002, val_acc: 0.9600\n",
      "Epoch [9660/10000], train loss: 0.2180 train acc: 0.9067 val_loss: 0.2002, val_acc: 0.9600\n",
      "Epoch [9670/10000], train loss: 0.2179 train acc: 0.9067 val_loss: 0.2001, val_acc: 0.9600\n",
      "Epoch [9680/10000], train loss: 0.2178 train acc: 0.9067 val_loss: 0.2000, val_acc: 0.9600\n",
      "Epoch [9690/10000], train loss: 0.2178 train acc: 0.9067 val_loss: 0.2000, val_acc: 0.9600\n",
      "Epoch [9700/10000], train loss: 0.2177 train acc: 0.9067 val_loss: 0.1999, val_acc: 0.9600\n",
      "Epoch [9710/10000], train loss: 0.2176 train acc: 0.9067 val_loss: 0.1998, val_acc: 0.9600\n",
      "Epoch [9720/10000], train loss: 0.2176 train acc: 0.9067 val_loss: 0.1998, val_acc: 0.9600\n",
      "Epoch [9730/10000], train loss: 0.2175 train acc: 0.9067 val_loss: 0.1997, val_acc: 0.9600\n",
      "Epoch [9740/10000], train loss: 0.2174 train acc: 0.9067 val_loss: 0.1996, val_acc: 0.9600\n",
      "Epoch [9750/10000], train loss: 0.2174 train acc: 0.9067 val_loss: 0.1996, val_acc: 0.9600\n",
      "Epoch [9760/10000], train loss: 0.2173 train acc: 0.9067 val_loss: 0.1995, val_acc: 0.9600\n",
      "Epoch [9770/10000], train loss: 0.2173 train acc: 0.9067 val_loss: 0.1994, val_acc: 0.9600\n",
      "Epoch [9780/10000], train loss: 0.2172 train acc: 0.9067 val_loss: 0.1994, val_acc: 0.9600\n",
      "Epoch [9790/10000], train loss: 0.2171 train acc: 0.9067 val_loss: 0.1993, val_acc: 0.9600\n",
      "Epoch [9800/10000], train loss: 0.2171 train acc: 0.9067 val_loss: 0.1992, val_acc: 0.9600\n",
      "Epoch [9810/10000], train loss: 0.2170 train acc: 0.9067 val_loss: 0.1991, val_acc: 0.9600\n",
      "Epoch [9820/10000], train loss: 0.2169 train acc: 0.9067 val_loss: 0.1991, val_acc: 0.9600\n",
      "Epoch [9830/10000], train loss: 0.2169 train acc: 0.9067 val_loss: 0.1990, val_acc: 0.9600\n",
      "Epoch [9840/10000], train loss: 0.2168 train acc: 0.9067 val_loss: 0.1989, val_acc: 0.9600\n",
      "Epoch [9850/10000], train loss: 0.2168 train acc: 0.9067 val_loss: 0.1989, val_acc: 0.9600\n",
      "Epoch [9860/10000], train loss: 0.2167 train acc: 0.9067 val_loss: 0.1988, val_acc: 0.9600\n",
      "Epoch [9870/10000], train loss: 0.2166 train acc: 0.9067 val_loss: 0.1987, val_acc: 0.9600\n",
      "Epoch [9880/10000], train loss: 0.2166 train acc: 0.9067 val_loss: 0.1987, val_acc: 0.9600\n",
      "Epoch [9890/10000], train loss: 0.2165 train acc: 0.9067 val_loss: 0.1986, val_acc: 0.9600\n",
      "Epoch [9900/10000], train loss: 0.2164 train acc: 0.9067 val_loss: 0.1985, val_acc: 0.9600\n",
      "Epoch [9910/10000], train loss: 0.2164 train acc: 0.9067 val_loss: 0.1985, val_acc: 0.9600\n",
      "Epoch [9920/10000], train loss: 0.2163 train acc: 0.9067 val_loss: 0.1984, val_acc: 0.9600\n",
      "Epoch [9930/10000], train loss: 0.2163 train acc: 0.9067 val_loss: 0.1983, val_acc: 0.9600\n",
      "Epoch [9940/10000], train loss: 0.2162 train acc: 0.9067 val_loss: 0.1983, val_acc: 0.9600\n",
      "Epoch [9950/10000], train loss: 0.2161 train acc: 0.9067 val_loss: 0.1982, val_acc: 0.9600\n",
      "Epoch [9960/10000], train loss: 0.2161 train acc: 0.9067 val_loss: 0.1981, val_acc: 0.9600\n",
      "Epoch [9970/10000], train loss: 0.2160 train acc: 0.9067 val_loss: 0.1981, val_acc: 0.9600\n",
      "Epoch [9980/10000], train loss: 0.2160 train acc: 0.9067 val_loss: 0.1980, val_acc: 0.9600\n",
      "Epoch [9990/10000], train loss: 0.2159 train acc: 0.9067 val_loss: 0.1979, val_acc: 0.9600\n"
     ]
    }
   ],
   "source": [
    "# 반복 계산 메인 루프\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # 훈련 페이즈 #training \n",
    "\n",
    "    # 경사 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 예측 계산\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # 경사 계산\n",
    "    loss.backward()\n",
    "\n",
    "    # 파라미터 수정\n",
    "    optimizer.step()\n",
    "\n",
    "    # 예측 라벨 산출\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    # 손실과 정확도 계산\n",
    "    train_loss = loss.item()\n",
    "    train_acc = (predicted == labels).float().mean() \n",
    "\n",
    "    # 예측 페이즈\n",
    "\n",
    "    # 예측 계산 ###Test \n",
    "    outputs_test = net(inputs_test)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss_test = criterion(outputs_test, labels_test)\n",
    "\n",
    "    # 예측 라벨 산출\n",
    "    predicted_test = outputs_test.argmax(1)\n",
    "\n",
    "    # 손실과 정확도 계산\n",
    "    val_loss =  loss_test.item()\n",
    "    val_acc =  (predicted_test == labels_test).float().mean() \n",
    "\n",
    "    if ((epoch) % 10 == 0):\n",
    "        print (f'Epoch [{epoch}/{num_epochs}], train loss: {train_loss:.4f} train acc: {train_acc:.4f} val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}')\n",
    "        item = np.array([epoch, train_loss, train_acc, val_loss, val_acc])\n",
    "        history = np.vstack((history, item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmfiQ7Hu0UG1"
   },
   "source": [
    "### 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1739156985704,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "SPgA6BH10UG6",
    "outputId": "d93243b1-947c-46b0-fcbc-96f203dacbec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기상태 : 손실 : 1.09263  정확도 : 0.26667\n",
      "최종상태 : 손실 : 0.19795  정확도 : 0.96000\n"
     ]
    }
   ],
   "source": [
    "# 손실과 정확도 확인\n",
    "\n",
    "print(f'초기상태 : 손실 : {history[0,3]:.5f}  정확도 : {history[0,4]:.5f}' )\n",
    "print(f'최종상태 : 손실 : {history[-1,3]:.5f}  정확도 : {history[-1,4]:.5f}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1739156985976,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "md8-ut1H0UG7",
    "outputId": "dfd438f8-edab-49df-dd23-7453d1543eff"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 곡선 출력(손실)\n",
    "\n",
    "plt.plot(history[:,0], history[:,1], 'b', label='훈련trianing')\n",
    "plt.plot(history[:,0], history[:,3], 'k', label='검증test')\n",
    "plt.xlabel('반복 횟수')\n",
    "plt.ylabel('손실')\n",
    "plt.title('학습 곡선(손실)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history[:,0], history[:,2], 'b', label='훈련trianing')\n",
    "plt.plot(history[:,0], history[:,4], 'k', label='검증test')\n",
    "plt.xlabel('반복 횟수')\n",
    "plt.ylabel('손실')\n",
    "plt.title('학습 곡선(손실)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDQXO9L80UG7"
   },
   "source": [
    "### 모델 출력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1739156985990,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "b0mVnf0t0UG7",
    "outputId": "ca5c218a-42ba-4817-ad7e-0ae4ebf8fde4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 2])\n",
      "==================================================\n",
      "[[6.3 4.7]\n",
      " [5.  1.6]\n",
      " [6.4 5.6]]\n"
     ]
    }
   ],
   "source": [
    "# 정답 데이터의 0번째, 2번째, 3번째를 추출\n",
    "\n",
    "print(labels[[0,2,3]])\n",
    "\n",
    "# 이에 해당하는 입력값을 추출\n",
    "print(\"=\"*50)\n",
    "i3 = inputs[[0,2,3],:]\n",
    "print(i3.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739156985996,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "pRHdlZa30UG7",
    "outputId": "20d24967-be63-455f-a422-c774449e69bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.8071 14.1937 12.9986]\n",
      " [12.8262  9.8     0.1734]\n",
      " [ 6.7954 15.0928 17.1111]]\n",
      "[[0.0035 0.765  0.2315]\n",
      " [0.9537 0.0463 0.    ]\n",
      " [0.     0.1173 0.8827]]\n"
     ]
    }
   ],
   "source": [
    "# 출력값에 소프트맥스 함수를 적용한 결과를 취득\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "o3 = net(i3)\n",
    "k3 = softmax(o3)\n",
    "print(o3.data.numpy()) #linear output \n",
    "print(k3.data.numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAoktUT60UG7"
   },
   "source": [
    "### 가중치 행렬과 바이어스 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739156986002,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "WzAhs3G30UG8",
    "outputId": "3e6ad666-6d2e-4214-adf7-83a3c18bdc3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.0452, -2.5735],\n",
      "        [ 1.3573,  0.8481],\n",
      "        [-1.4026,  4.7253]])\n",
      "tensor([ 1.7178,  1.6563, -0.3741])\n"
     ]
    }
   ],
   "source": [
    "# 가중치 행렬\n",
    "print(net.l1.weight.data)\n",
    "\n",
    "# 바이어스\n",
    "print(net.l1.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NlAFyUt0UG8"
   },
   "source": [
    "### 입력 변수 4개 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1739156986040,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "3-KN91JK0UG8",
    "outputId": "48817019-1bab-4ad8-b983-4f40879f0ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 4) (75, 4) (75,) (75,)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터와 검증 데이터로 분할(셔플도 동시에 실시함)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_org, y_org, train_size=75, test_size=75,\n",
    "    random_state=123)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "print(len(set(y_org))) #class = 3\n",
    "\n",
    "# 입력 차원수\n",
    "n_input = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1739156986054,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "fU8bhg9i0UG8",
    "outputId": "baa7e98c-cf2f-43e8-fbf8-10c78d025690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터(x)\n",
      "[[6.3 3.3 4.7 1.6]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [6.3 2.5 5.  1.9]]\n",
      "입력 차원수: 4\n"
     ]
    }
   ],
   "source": [
    "print('입력 데이터(x)')\n",
    "print(x_train[:5,:])\n",
    "print(f'입력 차원수: {n_input}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739156986057,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "MS0s0Itv0UG8"
   },
   "outputs": [],
   "source": [
    "# 입력 데이터 x_train과 정답 데이터 y_train의 텐서 변수화\n",
    "inputs = torch.tensor(x_train).float()\n",
    "labels = torch.tensor(y_train).long() \n",
    "\n",
    "# 검증용 데이터의 텐서 변수화\n",
    "inputs_test = torch.tensor(x_test).float()\n",
    "labels_test = torch.tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax \n",
    "class Net(nn.Module):\n",
    "    def __init__(self,n_input,n_output):\n",
    "        super().__init__() \n",
    "\n",
    "        self.l1=nn.Linear(n_input,n_output)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.l1(x)\n",
    "        return x \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (l1): Linear(in_features=4, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.2970, -0.2828,  0.3225,  0.0884],\n",
      "        [-0.2934,  0.1284,  0.3224,  0.1670],\n",
      "        [ 0.1956, -0.2764, -0.0265,  0.3187]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3661, -0.0242,  0.2132], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "n_input=inputs.size()[1] #4\n",
    "n_output = len(set(y_train)) #3 \n",
    "\n",
    "net = Net(n_input, n_output)\n",
    "\n",
    "print(net)\n",
    "print()\n",
    "print(net.l1.weight)\n",
    "print(net.l1.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Net                                      [100, 3]                  --\n",
       "├─Linear: 1-1                            [100, 3]                  15\n",
       "==========================================================================================\n",
       "Total params: 15\n",
       "Trainable params: 15\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net, (100,4), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739156986083,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "FbEi6ai00UG8"
   },
   "outputs": [],
   "source": [
    "# 학습률\n",
    "lr = 0.01\n",
    "\n",
    "# 초기화\n",
    "net = Net(n_input, n_output)\n",
    "\n",
    "# 손실 함수： 교차 엔트로피 함수\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 최적화 알고리즘: 경사 하강법\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# 반복 횟수\n",
    "num_epochs = 10000\n",
    "\n",
    "# 평가 결과 기록\n",
    "history = np.zeros((0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9425,
     "status": "ok",
     "timestamp": 1739156995511,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "hvaliyZZ0UG9",
    "outputId": "b5ecdd2f-5b06-4627-ee54-8cc8941bd462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0, Train loss: 2.57865 Train acc: 0.30667 val_loss: 1.80062, val_acc: 0.36000\n",
      "Epoch [100, Train loss: 0.73957 Train acc: 0.85333 val_loss: 0.74004, val_acc: 0.78667\n",
      "Epoch [200, Train loss: 0.60808 Train acc: 0.88000 val_loss: 0.60373, val_acc: 0.82667\n",
      "Epoch [300, Train loss: 0.54238 Train acc: 0.90667 val_loss: 0.53634, val_acc: 0.84000\n",
      "Epoch [400, Train loss: 0.49987 Train acc: 0.94667 val_loss: 0.49325, val_acc: 0.89333\n",
      "Epoch [500, Train loss: 0.46825 Train acc: 0.96000 val_loss: 0.46153, val_acc: 0.90667\n",
      "Epoch [600, Train loss: 0.44281 Train acc: 0.96000 val_loss: 0.43625, val_acc: 0.90667\n",
      "Epoch [700, Train loss: 0.42136 Train acc: 0.97333 val_loss: 0.41511, val_acc: 0.90667\n",
      "Epoch [800, Train loss: 0.40275 Train acc: 0.97333 val_loss: 0.39688, val_acc: 0.92000\n",
      "Epoch [900, Train loss: 0.38627 Train acc: 0.97333 val_loss: 0.38083, val_acc: 0.93333\n",
      "Epoch [1000, Train loss: 0.37151 Train acc: 0.97333 val_loss: 0.36650, val_acc: 0.93333\n",
      "Epoch [1100, Train loss: 0.35814 Train acc: 0.97333 val_loss: 0.35359, val_acc: 0.93333\n",
      "Epoch [1200, Train loss: 0.34595 Train acc: 0.97333 val_loss: 0.34184, val_acc: 0.93333\n",
      "Epoch [1300, Train loss: 0.33478 Train acc: 0.97333 val_loss: 0.33111, val_acc: 0.93333\n",
      "Epoch [1400, Train loss: 0.32449 Train acc: 0.97333 val_loss: 0.32124, val_acc: 0.93333\n",
      "Epoch [1500, Train loss: 0.31497 Train acc: 0.97333 val_loss: 0.31213, val_acc: 0.93333\n",
      "Epoch [1600, Train loss: 0.30613 Train acc: 0.97333 val_loss: 0.30369, val_acc: 0.92000\n",
      "Epoch [1700, Train loss: 0.29791 Train acc: 0.97333 val_loss: 0.29586, val_acc: 0.92000\n",
      "Epoch [1800, Train loss: 0.29024 Train acc: 0.97333 val_loss: 0.28856, val_acc: 0.92000\n",
      "Epoch [1900, Train loss: 0.28307 Train acc: 0.97333 val_loss: 0.28174, val_acc: 0.92000\n",
      "Epoch [2000, Train loss: 0.27634 Train acc: 0.97333 val_loss: 0.27535, val_acc: 0.93333\n",
      "Epoch [2100, Train loss: 0.27002 Train acc: 0.97333 val_loss: 0.26937, val_acc: 0.94667\n",
      "Epoch [2200, Train loss: 0.26408 Train acc: 0.97333 val_loss: 0.26374, val_acc: 0.94667\n",
      "Epoch [2300, Train loss: 0.25847 Train acc: 0.97333 val_loss: 0.25844, val_acc: 0.94667\n",
      "Epoch [2400, Train loss: 0.25317 Train acc: 0.97333 val_loss: 0.25344, val_acc: 0.94667\n",
      "Epoch [2500, Train loss: 0.24817 Train acc: 0.97333 val_loss: 0.24872, val_acc: 0.94667\n",
      "Epoch [2600, Train loss: 0.24342 Train acc: 0.98667 val_loss: 0.24425, val_acc: 0.94667\n",
      "Epoch [2700, Train loss: 0.23892 Train acc: 0.98667 val_loss: 0.24001, val_acc: 0.94667\n",
      "Epoch [2800, Train loss: 0.23464 Train acc: 0.98667 val_loss: 0.23600, val_acc: 0.94667\n",
      "Epoch [2900, Train loss: 0.23057 Train acc: 0.98667 val_loss: 0.23218, val_acc: 0.94667\n",
      "Epoch [3000, Train loss: 0.22669 Train acc: 0.98667 val_loss: 0.22855, val_acc: 0.94667\n",
      "Epoch [3100, Train loss: 0.22300 Train acc: 0.98667 val_loss: 0.22509, val_acc: 0.94667\n",
      "Epoch [3200, Train loss: 0.21947 Train acc: 0.98667 val_loss: 0.22179, val_acc: 0.94667\n",
      "Epoch [3300, Train loss: 0.21610 Train acc: 0.98667 val_loss: 0.21864, val_acc: 0.94667\n",
      "Epoch [3400, Train loss: 0.21287 Train acc: 0.98667 val_loss: 0.21563, val_acc: 0.94667\n",
      "Epoch [3500, Train loss: 0.20978 Train acc: 0.98667 val_loss: 0.21275, val_acc: 0.94667\n",
      "Epoch [3600, Train loss: 0.20682 Train acc: 0.98667 val_loss: 0.21000, val_acc: 0.94667\n",
      "Epoch [3700, Train loss: 0.20398 Train acc: 0.98667 val_loss: 0.20736, val_acc: 0.94667\n",
      "Epoch [3800, Train loss: 0.20125 Train acc: 0.98667 val_loss: 0.20482, val_acc: 0.94667\n",
      "Epoch [3900, Train loss: 0.19863 Train acc: 0.98667 val_loss: 0.20239, val_acc: 0.96000\n",
      "Epoch [4000, Train loss: 0.19610 Train acc: 0.98667 val_loss: 0.20006, val_acc: 0.96000\n",
      "Epoch [4100, Train loss: 0.19368 Train acc: 0.98667 val_loss: 0.19781, val_acc: 0.96000\n",
      "Epoch [4200, Train loss: 0.19134 Train acc: 0.98667 val_loss: 0.19565, val_acc: 0.96000\n",
      "Epoch [4300, Train loss: 0.18908 Train acc: 0.98667 val_loss: 0.19357, val_acc: 0.96000\n",
      "Epoch [4400, Train loss: 0.18691 Train acc: 0.98667 val_loss: 0.19157, val_acc: 0.96000\n",
      "Epoch [4500, Train loss: 0.18481 Train acc: 0.98667 val_loss: 0.18963, val_acc: 0.96000\n",
      "Epoch [4600, Train loss: 0.18278 Train acc: 0.98667 val_loss: 0.18777, val_acc: 0.96000\n",
      "Epoch [4700, Train loss: 0.18082 Train acc: 0.98667 val_loss: 0.18597, val_acc: 0.96000\n",
      "Epoch [4800, Train loss: 0.17892 Train acc: 0.98667 val_loss: 0.18423, val_acc: 0.96000\n",
      "Epoch [4900, Train loss: 0.17708 Train acc: 0.98667 val_loss: 0.18255, val_acc: 0.96000\n",
      "Epoch [5000, Train loss: 0.17531 Train acc: 0.98667 val_loss: 0.18092, val_acc: 0.96000\n",
      "Epoch [5100, Train loss: 0.17358 Train acc: 0.98667 val_loss: 0.17935, val_acc: 0.96000\n",
      "Epoch [5200, Train loss: 0.17191 Train acc: 0.98667 val_loss: 0.17782, val_acc: 0.96000\n",
      "Epoch [5300, Train loss: 0.17030 Train acc: 0.98667 val_loss: 0.17635, val_acc: 0.96000\n",
      "Epoch [5400, Train loss: 0.16872 Train acc: 0.98667 val_loss: 0.17492, val_acc: 0.96000\n",
      "Epoch [5500, Train loss: 0.16720 Train acc: 0.98667 val_loss: 0.17353, val_acc: 0.96000\n",
      "Epoch [5600, Train loss: 0.16572 Train acc: 0.98667 val_loss: 0.17218, val_acc: 0.96000\n",
      "Epoch [5700, Train loss: 0.16428 Train acc: 0.98667 val_loss: 0.17088, val_acc: 0.96000\n",
      "Epoch [5800, Train loss: 0.16288 Train acc: 0.98667 val_loss: 0.16961, val_acc: 0.96000\n",
      "Epoch [5900, Train loss: 0.16152 Train acc: 0.98667 val_loss: 0.16838, val_acc: 0.96000\n",
      "Epoch [6000, Train loss: 0.16019 Train acc: 0.98667 val_loss: 0.16718, val_acc: 0.96000\n",
      "Epoch [6100, Train loss: 0.15890 Train acc: 0.98667 val_loss: 0.16602, val_acc: 0.96000\n",
      "Epoch [6200, Train loss: 0.15765 Train acc: 0.98667 val_loss: 0.16489, val_acc: 0.96000\n",
      "Epoch [6300, Train loss: 0.15642 Train acc: 0.98667 val_loss: 0.16379, val_acc: 0.96000\n",
      "Epoch [6400, Train loss: 0.15523 Train acc: 0.98667 val_loss: 0.16271, val_acc: 0.96000\n",
      "Epoch [6500, Train loss: 0.15407 Train acc: 0.98667 val_loss: 0.16167, val_acc: 0.96000\n",
      "Epoch [6600, Train loss: 0.15294 Train acc: 0.98667 val_loss: 0.16065, val_acc: 0.96000\n",
      "Epoch [6700, Train loss: 0.15183 Train acc: 0.98667 val_loss: 0.15966, val_acc: 0.96000\n",
      "Epoch [6800, Train loss: 0.15075 Train acc: 0.98667 val_loss: 0.15870, val_acc: 0.96000\n",
      "Epoch [6900, Train loss: 0.14970 Train acc: 0.98667 val_loss: 0.15776, val_acc: 0.96000\n",
      "Epoch [7000, Train loss: 0.14867 Train acc: 0.98667 val_loss: 0.15684, val_acc: 0.96000\n",
      "Epoch [7100, Train loss: 0.14767 Train acc: 0.98667 val_loss: 0.15594, val_acc: 0.96000\n",
      "Epoch [7200, Train loss: 0.14669 Train acc: 0.98667 val_loss: 0.15507, val_acc: 0.96000\n",
      "Epoch [7300, Train loss: 0.14573 Train acc: 0.98667 val_loss: 0.15422, val_acc: 0.96000\n",
      "Epoch [7400, Train loss: 0.14479 Train acc: 0.98667 val_loss: 0.15338, val_acc: 0.96000\n",
      "Epoch [7500, Train loss: 0.14387 Train acc: 0.98667 val_loss: 0.15257, val_acc: 0.96000\n",
      "Epoch [7600, Train loss: 0.14297 Train acc: 0.98667 val_loss: 0.15178, val_acc: 0.96000\n",
      "Epoch [7700, Train loss: 0.14210 Train acc: 0.98667 val_loss: 0.15100, val_acc: 0.96000\n",
      "Epoch [7800, Train loss: 0.14124 Train acc: 0.98667 val_loss: 0.15024, val_acc: 0.96000\n",
      "Epoch [7900, Train loss: 0.14040 Train acc: 0.98667 val_loss: 0.14950, val_acc: 0.96000\n",
      "Epoch [8000, Train loss: 0.13957 Train acc: 0.98667 val_loss: 0.14877, val_acc: 0.96000\n",
      "Epoch [8100, Train loss: 0.13877 Train acc: 0.98667 val_loss: 0.14806, val_acc: 0.96000\n",
      "Epoch [8200, Train loss: 0.13797 Train acc: 0.98667 val_loss: 0.14737, val_acc: 0.96000\n",
      "Epoch [8300, Train loss: 0.13720 Train acc: 0.98667 val_loss: 0.14669, val_acc: 0.96000\n",
      "Epoch [8400, Train loss: 0.13644 Train acc: 0.98667 val_loss: 0.14602, val_acc: 0.96000\n",
      "Epoch [8500, Train loss: 0.13570 Train acc: 0.98667 val_loss: 0.14537, val_acc: 0.96000\n",
      "Epoch [8600, Train loss: 0.13497 Train acc: 0.98667 val_loss: 0.14473, val_acc: 0.96000\n",
      "Epoch [8700, Train loss: 0.13425 Train acc: 0.98667 val_loss: 0.14411, val_acc: 0.96000\n",
      "Epoch [8800, Train loss: 0.13355 Train acc: 0.98667 val_loss: 0.14349, val_acc: 0.96000\n",
      "Epoch [8900, Train loss: 0.13286 Train acc: 0.98667 val_loss: 0.14289, val_acc: 0.96000\n",
      "Epoch [9000, Train loss: 0.13218 Train acc: 0.98667 val_loss: 0.14230, val_acc: 0.96000\n",
      "Epoch [9100, Train loss: 0.13152 Train acc: 0.98667 val_loss: 0.14173, val_acc: 0.96000\n",
      "Epoch [9200, Train loss: 0.13087 Train acc: 0.98667 val_loss: 0.14116, val_acc: 0.96000\n",
      "Epoch [9300, Train loss: 0.13023 Train acc: 0.98667 val_loss: 0.14061, val_acc: 0.96000\n",
      "Epoch [9400, Train loss: 0.12960 Train acc: 0.98667 val_loss: 0.14006, val_acc: 0.96000\n",
      "Epoch [9500, Train loss: 0.12898 Train acc: 0.98667 val_loss: 0.13953, val_acc: 0.96000\n",
      "Epoch [9600, Train loss: 0.12837 Train acc: 0.98667 val_loss: 0.13900, val_acc: 0.96000\n",
      "Epoch [9700, Train loss: 0.12778 Train acc: 0.98667 val_loss: 0.13849, val_acc: 0.94667\n",
      "Epoch [9800, Train loss: 0.12719 Train acc: 0.98667 val_loss: 0.13798, val_acc: 0.94667\n",
      "Epoch [9900, Train loss: 0.12661 Train acc: 0.98667 val_loss: 0.13749, val_acc: 0.94667\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # 훈련 페이즈\n",
    "\n",
    "    # 경사 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 예측 계산\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # 경사 계산\n",
    "    loss.backward()\n",
    "\n",
    "    # 파라미터 수정\n",
    "    optimizer.step()\n",
    "\n",
    "    # 예측 라벨 산출\n",
    "    predicted = outputs.argmax(-1)\n",
    "\n",
    "    # 손실과 정확도 계산\n",
    "    train_loss = loss.item()\n",
    "    train_acc = (predicted == labels).float().mean()\n",
    "\n",
    "    # 예측 페이즈\n",
    "\n",
    "    # 예측 계산\n",
    "    outputs_test = net(inputs_test)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss_test = criterion(outputs_test, labels_test)\n",
    "\n",
    "    # 예측 라벨 산출\n",
    "    predicted_test = torch.max(outputs_test, 1)[1] #outputs_test.argmax(-1)\n",
    "\n",
    "    # 손실과 정확도 계산\n",
    "    val_loss =  loss_test.item()\n",
    "    val_acc =  (predicted_test == labels_test).float().mean()\n",
    "\n",
    "    if ( epoch % 100 == 0):\n",
    "        print (f'Epoch [{epoch}, Train loss: {train_loss:.5f} Train acc: {train_acc:.5f} val_loss: {val_loss:.5f}, val_acc: {val_acc:.5f}')\n",
    "        item = np.array([epoch , train_loss, train_acc, val_loss, val_acc])\n",
    "        history = np.vstack((history, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739156995513,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "Bcttwz6M0UG9",
    "outputId": "a68d47ca-b3f4-4147-acb7-7841a47f1586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기상태 : 손실 : 1.80062  정확도 : 0.36000\n",
      "최종상태 : 손실 : 0.13749  정확도 : 0.94667\n"
     ]
    }
   ],
   "source": [
    "# 손실과 정확도 확인\n",
    "\n",
    "print(f'초기상태 : 손실 : {history[0,3]:.5f}  정확도 : {history[0,4]:.5f}' )\n",
    "print(f'최종상태 : 손실 : {history[-1,3]:.5f}  정확도 : {history[-1,4]:.5f}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1739156995729,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "H1k8qEYt0UG9",
    "outputId": "d20dbd25-de93-4d7c-83d2-0a8b8698c639"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 곡선 출력(손실)\n",
    "\n",
    "plt.plot(history[:,0], history[:,1], 'b', label='훈련')\n",
    "plt.plot(history[:,0], history[:,3], 'k', label='검증')\n",
    "plt.xlabel('반복 횟수')\n",
    "plt.ylabel('손실')\n",
    "plt.title('학습 곡선(손실)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1739156995956,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "PvE8lQbI0UG9",
    "outputId": "66d14f54-1f24-48f2-9333-aec9d35e37b2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 곡선 출력(정확도)\n",
    "\n",
    "plt.plot(history[:,0], history[:,2], 'b', label='훈련')\n",
    "plt.plot(history[:,0], history[:,4], 'k', label='검증')\n",
    "plt.xlabel('반복 횟수')\n",
    "plt.ylabel('정확도')\n",
    "plt.title('학습 곡선(정확도)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lXnvaek0UG9"
   },
   "source": [
    "### NLLLoss 함수 이해 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1739156995969,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "RJlEO9ET0UG-",
    "outputId": "164aa692-63be-4179-b07c-7456e5012596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "tensor([0, 1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "# 입력 변수 준비\n",
    "\n",
    "# 더미 출력 데이터\n",
    "outputs_np = np.array(range(1, 13)).reshape((4,3))\n",
    "# 더미 정답 데이터\n",
    "labels_np = np.array([0, 1, 2, 0]) \n",
    "\n",
    "# 텐서화\n",
    "outputs_dummy = torch.tensor(outputs_np).float()\n",
    "labels_dummy = torch.tensor(labels_np).long()\n",
    "\n",
    "# 결과 확인\n",
    "print(outputs_dummy.data)\n",
    "print(labels_dummy.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739156995974,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "HvDBFJ3v0UG-",
    "outputId": "1b58f415-2e50-47c8-8713-f4b818cde998"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.25\n"
     ]
    }
   ],
   "source": [
    "# NLLLoss 함수 호출\n",
    "\n",
    "nllloss = nn.NLLLoss()\n",
    "loss = nllloss(outputs_dummy, labels_dummy) # -(1 + 5 + 9 + 10)/4 = -6.25\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBnwEbie0UG-"
   },
   "source": [
    "### 모델 클래스측에 LogSoftmax 함수를 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739156995977,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "fHIZk9sx0UG-"
   },
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "# 2입력 3출력 로지스틱 회귀 모델\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_input, n_output)\n",
    "        # logsoftmax 함수 정의\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # 초깃값을 모두 1로 함\n",
    "        # \"딥러닝을 위한 수학\"과 조건을 맞추기 위한 목적\n",
    "        # self.l1.weight.data.fill_(1.0)\n",
    "        # self.l1.bias.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.l1(x)\n",
    "        x2 = self.logsoftmax(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739156995992,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "gGfS1v6X0UG-"
   },
   "outputs": [],
   "source": [
    "# 학습률\n",
    "lr = 0.01\n",
    "\n",
    "# 초기화\n",
    "net = Net(n_input, n_output)\n",
    "\n",
    "# 손실 함수： NLLLoss 함수\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# 최적화 함수: 경사 하강법\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1739156996039,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "v2ZEoqMy0UG-",
    "outputId": "59079a46-de1b-431f-eaaa-6c8f1b3fea1d"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"196pt\" height=\"391pt\"\n",
       " viewBox=\"0.00 0.00 196.00 391.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 387)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-387 192,-387 192,4 -4,4\"/>\n",
       "<!-- 128412470810960 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>128412470810960</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"121,-31 67,-31 67,0 121,0 121,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"94\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 128412245986656 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>128412245986656</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"144.5,-86 43.5,-86 43.5,-67 144.5,-67 144.5,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"94\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">NllLossBackward0</text>\n",
       "</g>\n",
       "<!-- 128412245986656&#45;&gt;128412470810960 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>128412245986656&#45;&gt;128412470810960</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M94,-66.79C94,-60.07 94,-50.4 94,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.5,-41.19 94,-31.19 90.5,-41.19 97.5,-41.19\"/>\n",
       "</g>\n",
       "<!-- 128412272066080 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>128412272066080</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"152,-141 36,-141 36,-122 152,-122 152,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"94\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">LogSoftmaxBackward0</text>\n",
       "</g>\n",
       "<!-- 128412272066080&#45;&gt;128412245986656 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>128412272066080&#45;&gt;128412245986656</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M94,-121.75C94,-114.8 94,-104.85 94,-96.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.5,-96.09 94,-86.09 90.5,-96.09 97.5,-96.09\"/>\n",
       "</g>\n",
       "<!-- 128412245896992 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>128412245896992</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"139,-196 49,-196 49,-177 139,-177 139,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"94\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 128412245896992&#45;&gt;128412272066080 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>128412245896992&#45;&gt;128412272066080</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M94,-176.75C94,-169.8 94,-159.85 94,-151.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.5,-151.09 94,-141.09 90.5,-151.09 97.5,-151.09\"/>\n",
       "</g>\n",
       "<!-- 128412244113008 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>128412244113008</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"90,-251 0,-251 0,-232 90,-232 90,-251\"/>\n",
       "<text text-anchor=\"middle\" x=\"45\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 128412244113008&#45;&gt;128412245896992 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>128412244113008&#45;&gt;128412245896992</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.09,-231.75C60.16,-224.11 70.59,-212.82 79.15,-203.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.84,-205.81 86.06,-196.09 76.7,-201.06 81.84,-205.81\"/>\n",
       "</g>\n",
       "<!-- 128412245599616 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>128412245599616</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"72,-317 18,-317 18,-287 72,-287 72,-317\"/>\n",
       "<text text-anchor=\"middle\" x=\"45\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\">l1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"45\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n",
       "</g>\n",
       "<!-- 128412245599616&#45;&gt;128412244113008 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>128412245599616&#45;&gt;128412244113008</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45,-286.84C45,-279.21 45,-269.7 45,-261.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"48.5,-261.27 45,-251.27 41.5,-261.27 48.5,-261.27\"/>\n",
       "</g>\n",
       "<!-- 128412244112336 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>128412244112336</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"177.5,-251 108.5,-251 108.5,-232 177.5,-232 177.5,-251\"/>\n",
       "<text text-anchor=\"middle\" x=\"143\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 128412244112336&#45;&gt;128412245896992 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>128412244112336&#45;&gt;128412245896992</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M134.91,-231.75C127.84,-224.11 117.41,-212.82 108.85,-203.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"111.3,-201.06 101.94,-196.09 106.16,-205.81 111.3,-201.06\"/>\n",
       "</g>\n",
       "<!-- 128412244113488 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>128412244113488</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"188,-311.5 98,-311.5 98,-292.5 188,-292.5 188,-311.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"143\" y=\"-299.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 128412244113488&#45;&gt;128412244112336 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>128412244113488&#45;&gt;128412244112336</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M143,-292.37C143,-284.25 143,-271.81 143,-261.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"146.5,-261.17 143,-251.17 139.5,-261.17 146.5,-261.17\"/>\n",
       "</g>\n",
       "<!-- 128412245597376 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>128412245597376</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"175,-383 111,-383 111,-353 175,-353 175,-383\"/>\n",
       "<text text-anchor=\"middle\" x=\"143\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\">l1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"143\" y=\"-360\" font-family=\"monospace\" font-size=\"10.00\"> (3, 4)</text>\n",
       "</g>\n",
       "<!-- 128412245597376&#45;&gt;128412244113488 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>128412245597376&#45;&gt;128412244113488</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M143,-352.8C143,-343.7 143,-331.79 143,-321.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"146.5,-321.84 143,-311.84 139.5,-321.84 146.5,-321.84\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x74ca4e31f4a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 예측 계산\n",
    "outputs = net(inputs)\n",
    "\n",
    "# 손실 계산\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "# 손실의 계산 그래프 시각화\n",
    "g = make_dot(loss, params=dict(net.named_parameters()))\n",
    "display(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1739156996077,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "ZEOiOKYX0UG_"
   },
   "outputs": [],
   "source": [
    "# 학습률\n",
    "lr = 0.01\n",
    "\n",
    "# 초기화\n",
    "net = Net(n_input, n_output)\n",
    "\n",
    "# 손실 함수： NLLLoss 함수\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# 최적화 함수: 경사 하강법\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# 반복 횟수\n",
    "num_epochs = 10000\n",
    "\n",
    "# 평가 결과 기록\n",
    "history = np.zeros((0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9273,
     "status": "ok",
     "timestamp": 1739157005349,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "d7lDx60y0UG_",
    "outputId": "b4ba1b50-116e-4018-bab0-0c48722cb378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10000], train loss: 2.12809 train acc: 0.30667 val_loss: 2.18588, val_acc: 0.36000\n",
      "Epoch [10/10000], train loss: 1.29681 train acc: 0.45333 val_loss: 1.41419, val_acc: 0.29333\n",
      "Epoch [20/10000], train loss: 1.15617 train acc: 0.40000 val_loss: 1.22674, val_acc: 0.25333\n",
      "Epoch [30/10000], train loss: 1.08701 train acc: 0.37333 val_loss: 1.14096, val_acc: 0.22667\n",
      "Epoch [40/10000], train loss: 1.02828 train acc: 0.41333 val_loss: 1.07250, val_acc: 0.30667\n",
      "Epoch [50/10000], train loss: 0.97725 train acc: 0.48000 val_loss: 1.01361, val_acc: 0.40000\n",
      "Epoch [60/10000], train loss: 0.93284 train acc: 0.61333 val_loss: 0.96256, val_acc: 0.46667\n",
      "Epoch [70/10000], train loss: 0.89407 train acc: 0.61333 val_loss: 0.91820, val_acc: 0.56000\n",
      "Epoch [80/10000], train loss: 0.86009 train acc: 0.64000 val_loss: 0.87949, val_acc: 0.54667\n",
      "Epoch [90/10000], train loss: 0.83017 train acc: 0.65333 val_loss: 0.84555, val_acc: 0.54667\n",
      "Epoch [100/10000], train loss: 0.80366 train acc: 0.65333 val_loss: 0.81563, val_acc: 0.54667\n",
      "Epoch [110/10000], train loss: 0.78006 train acc: 0.65333 val_loss: 0.78911, val_acc: 0.54667\n",
      "Epoch [120/10000], train loss: 0.75892 train acc: 0.65333 val_loss: 0.76547, val_acc: 0.56000\n",
      "Epoch [130/10000], train loss: 0.73988 train acc: 0.65333 val_loss: 0.74426, val_acc: 0.57333\n",
      "Epoch [140/10000], train loss: 0.72264 train acc: 0.65333 val_loss: 0.72515, val_acc: 0.58667\n",
      "Epoch [150/10000], train loss: 0.70695 train acc: 0.66667 val_loss: 0.70783, val_acc: 0.58667\n",
      "Epoch [160/10000], train loss: 0.69260 train acc: 0.68000 val_loss: 0.69205, val_acc: 0.61333\n",
      "Epoch [170/10000], train loss: 0.67941 train acc: 0.68000 val_loss: 0.67761, val_acc: 0.65333\n",
      "Epoch [180/10000], train loss: 0.66724 train acc: 0.66667 val_loss: 0.66435, val_acc: 0.65333\n",
      "Epoch [190/10000], train loss: 0.65597 train acc: 0.69333 val_loss: 0.65210, val_acc: 0.66667\n",
      "Epoch [200/10000], train loss: 0.64548 train acc: 0.70667 val_loss: 0.64076, val_acc: 0.66667\n",
      "Epoch [210/10000], train loss: 0.63569 train acc: 0.70667 val_loss: 0.63021, val_acc: 0.66667\n",
      "Epoch [220/10000], train loss: 0.62653 train acc: 0.72000 val_loss: 0.62036, val_acc: 0.68000\n",
      "Epoch [230/10000], train loss: 0.61792 train acc: 0.73333 val_loss: 0.61115, val_acc: 0.68000\n",
      "Epoch [240/10000], train loss: 0.60981 train acc: 0.73333 val_loss: 0.60249, val_acc: 0.68000\n",
      "Epoch [250/10000], train loss: 0.60214 train acc: 0.74667 val_loss: 0.59435, val_acc: 0.68000\n",
      "Epoch [260/10000], train loss: 0.59489 train acc: 0.74667 val_loss: 0.58666, val_acc: 0.68000\n",
      "Epoch [270/10000], train loss: 0.58800 train acc: 0.74667 val_loss: 0.57938, val_acc: 0.70667\n",
      "Epoch [280/10000], train loss: 0.58145 train acc: 0.76000 val_loss: 0.57248, val_acc: 0.70667\n",
      "Epoch [290/10000], train loss: 0.57520 train acc: 0.76000 val_loss: 0.56592, val_acc: 0.70667\n",
      "Epoch [300/10000], train loss: 0.56923 train acc: 0.77333 val_loss: 0.55967, val_acc: 0.72000\n",
      "Epoch [310/10000], train loss: 0.56352 train acc: 0.78667 val_loss: 0.55370, val_acc: 0.73333\n",
      "Epoch [320/10000], train loss: 0.55804 train acc: 0.80000 val_loss: 0.54800, val_acc: 0.77333\n",
      "Epoch [330/10000], train loss: 0.55279 train acc: 0.82667 val_loss: 0.54254, val_acc: 0.78667\n",
      "Epoch [340/10000], train loss: 0.54773 train acc: 0.84000 val_loss: 0.53730, val_acc: 0.78667\n",
      "Epoch [350/10000], train loss: 0.54286 train acc: 0.85333 val_loss: 0.53227, val_acc: 0.78667\n",
      "Epoch [360/10000], train loss: 0.53817 train acc: 0.88000 val_loss: 0.52742, val_acc: 0.80000\n",
      "Epoch [370/10000], train loss: 0.53363 train acc: 0.89333 val_loss: 0.52276, val_acc: 0.81333\n",
      "Epoch [380/10000], train loss: 0.52925 train acc: 0.92000 val_loss: 0.51826, val_acc: 0.82667\n",
      "Epoch [390/10000], train loss: 0.52500 train acc: 0.92000 val_loss: 0.51391, val_acc: 0.82667\n",
      "Epoch [400/10000], train loss: 0.52089 train acc: 0.92000 val_loss: 0.50971, val_acc: 0.82667\n",
      "Epoch [410/10000], train loss: 0.51691 train acc: 0.92000 val_loss: 0.50564, val_acc: 0.82667\n",
      "Epoch [420/10000], train loss: 0.51303 train acc: 0.92000 val_loss: 0.50170, val_acc: 0.86667\n",
      "Epoch [430/10000], train loss: 0.50927 train acc: 0.92000 val_loss: 0.49787, val_acc: 0.88000\n",
      "Epoch [440/10000], train loss: 0.50561 train acc: 0.92000 val_loss: 0.49416, val_acc: 0.89333\n",
      "Epoch [450/10000], train loss: 0.50205 train acc: 0.92000 val_loss: 0.49055, val_acc: 0.89333\n",
      "Epoch [460/10000], train loss: 0.49858 train acc: 0.92000 val_loss: 0.48705, val_acc: 0.90667\n",
      "Epoch [470/10000], train loss: 0.49519 train acc: 0.92000 val_loss: 0.48363, val_acc: 0.92000\n",
      "Epoch [480/10000], train loss: 0.49189 train acc: 0.92000 val_loss: 0.48031, val_acc: 0.92000\n",
      "Epoch [490/10000], train loss: 0.48867 train acc: 0.92000 val_loss: 0.47707, val_acc: 0.92000\n",
      "Epoch [500/10000], train loss: 0.48552 train acc: 0.93333 val_loss: 0.47391, val_acc: 0.93333\n",
      "Epoch [510/10000], train loss: 0.48244 train acc: 0.96000 val_loss: 0.47082, val_acc: 0.93333\n",
      "Epoch [520/10000], train loss: 0.47943 train acc: 0.96000 val_loss: 0.46781, val_acc: 0.93333\n",
      "Epoch [530/10000], train loss: 0.47648 train acc: 0.96000 val_loss: 0.46486, val_acc: 0.93333\n",
      "Epoch [540/10000], train loss: 0.47360 train acc: 0.94667 val_loss: 0.46198, val_acc: 0.93333\n",
      "Epoch [550/10000], train loss: 0.47077 train acc: 0.94667 val_loss: 0.45917, val_acc: 0.93333\n",
      "Epoch [560/10000], train loss: 0.46800 train acc: 0.94667 val_loss: 0.45641, val_acc: 0.93333\n",
      "Epoch [570/10000], train loss: 0.46529 train acc: 0.94667 val_loss: 0.45371, val_acc: 0.93333\n",
      "Epoch [580/10000], train loss: 0.46262 train acc: 0.94667 val_loss: 0.45107, val_acc: 0.93333\n",
      "Epoch [590/10000], train loss: 0.46001 train acc: 0.96000 val_loss: 0.44847, val_acc: 0.93333\n",
      "Epoch [600/10000], train loss: 0.45744 train acc: 0.96000 val_loss: 0.44593, val_acc: 0.93333\n",
      "Epoch [610/10000], train loss: 0.45492 train acc: 0.96000 val_loss: 0.44344, val_acc: 0.93333\n",
      "Epoch [620/10000], train loss: 0.45244 train acc: 0.96000 val_loss: 0.44099, val_acc: 0.93333\n",
      "Epoch [630/10000], train loss: 0.45001 train acc: 0.96000 val_loss: 0.43859, val_acc: 0.93333\n",
      "Epoch [640/10000], train loss: 0.44761 train acc: 0.96000 val_loss: 0.43623, val_acc: 0.93333\n",
      "Epoch [650/10000], train loss: 0.44526 train acc: 0.96000 val_loss: 0.43391, val_acc: 0.94667\n",
      "Epoch [660/10000], train loss: 0.44294 train acc: 0.97333 val_loss: 0.43163, val_acc: 0.94667\n",
      "Epoch [670/10000], train loss: 0.44066 train acc: 0.97333 val_loss: 0.42939, val_acc: 0.94667\n",
      "Epoch [680/10000], train loss: 0.43842 train acc: 0.97333 val_loss: 0.42718, val_acc: 0.94667\n",
      "Epoch [690/10000], train loss: 0.43621 train acc: 0.97333 val_loss: 0.42502, val_acc: 0.94667\n",
      "Epoch [700/10000], train loss: 0.43403 train acc: 0.97333 val_loss: 0.42288, val_acc: 0.94667\n",
      "Epoch [710/10000], train loss: 0.43189 train acc: 0.97333 val_loss: 0.42078, val_acc: 0.94667\n",
      "Epoch [720/10000], train loss: 0.42977 train acc: 0.97333 val_loss: 0.41872, val_acc: 0.94667\n",
      "Epoch [730/10000], train loss: 0.42769 train acc: 0.97333 val_loss: 0.41668, val_acc: 0.94667\n",
      "Epoch [740/10000], train loss: 0.42564 train acc: 0.97333 val_loss: 0.41468, val_acc: 0.94667\n",
      "Epoch [750/10000], train loss: 0.42361 train acc: 0.97333 val_loss: 0.41270, val_acc: 0.94667\n",
      "Epoch [760/10000], train loss: 0.42161 train acc: 0.97333 val_loss: 0.41075, val_acc: 0.94667\n",
      "Epoch [770/10000], train loss: 0.41964 train acc: 0.97333 val_loss: 0.40883, val_acc: 0.94667\n",
      "Epoch [780/10000], train loss: 0.41770 train acc: 0.97333 val_loss: 0.40694, val_acc: 0.94667\n",
      "Epoch [790/10000], train loss: 0.41578 train acc: 0.97333 val_loss: 0.40507, val_acc: 0.94667\n",
      "Epoch [800/10000], train loss: 0.41389 train acc: 0.97333 val_loss: 0.40323, val_acc: 0.94667\n",
      "Epoch [810/10000], train loss: 0.41202 train acc: 0.97333 val_loss: 0.40142, val_acc: 0.94667\n",
      "Epoch [820/10000], train loss: 0.41017 train acc: 0.97333 val_loss: 0.39963, val_acc: 0.94667\n",
      "Epoch [830/10000], train loss: 0.40835 train acc: 0.97333 val_loss: 0.39786, val_acc: 0.94667\n",
      "Epoch [840/10000], train loss: 0.40655 train acc: 0.97333 val_loss: 0.39611, val_acc: 0.94667\n",
      "Epoch [850/10000], train loss: 0.40477 train acc: 0.97333 val_loss: 0.39439, val_acc: 0.94667\n",
      "Epoch [860/10000], train loss: 0.40301 train acc: 0.97333 val_loss: 0.39269, val_acc: 0.94667\n",
      "Epoch [870/10000], train loss: 0.40128 train acc: 0.97333 val_loss: 0.39101, val_acc: 0.94667\n",
      "Epoch [880/10000], train loss: 0.39956 train acc: 0.97333 val_loss: 0.38935, val_acc: 0.94667\n",
      "Epoch [890/10000], train loss: 0.39786 train acc: 0.97333 val_loss: 0.38771, val_acc: 0.94667\n",
      "Epoch [900/10000], train loss: 0.39619 train acc: 0.97333 val_loss: 0.38609, val_acc: 0.94667\n",
      "Epoch [910/10000], train loss: 0.39453 train acc: 0.97333 val_loss: 0.38449, val_acc: 0.94667\n",
      "Epoch [920/10000], train loss: 0.39289 train acc: 0.97333 val_loss: 0.38291, val_acc: 0.94667\n",
      "Epoch [930/10000], train loss: 0.39127 train acc: 0.97333 val_loss: 0.38135, val_acc: 0.94667\n",
      "Epoch [940/10000], train loss: 0.38967 train acc: 0.97333 val_loss: 0.37980, val_acc: 0.94667\n",
      "Epoch [950/10000], train loss: 0.38808 train acc: 0.97333 val_loss: 0.37828, val_acc: 0.94667\n",
      "Epoch [960/10000], train loss: 0.38651 train acc: 0.97333 val_loss: 0.37677, val_acc: 0.94667\n",
      "Epoch [970/10000], train loss: 0.38496 train acc: 0.97333 val_loss: 0.37527, val_acc: 0.94667\n",
      "Epoch [980/10000], train loss: 0.38343 train acc: 0.97333 val_loss: 0.37380, val_acc: 0.94667\n",
      "Epoch [990/10000], train loss: 0.38191 train acc: 0.97333 val_loss: 0.37234, val_acc: 0.94667\n",
      "Epoch [1000/10000], train loss: 0.38041 train acc: 0.97333 val_loss: 0.37089, val_acc: 0.94667\n",
      "Epoch [1010/10000], train loss: 0.37892 train acc: 0.97333 val_loss: 0.36947, val_acc: 0.94667\n",
      "Epoch [1020/10000], train loss: 0.37745 train acc: 0.97333 val_loss: 0.36805, val_acc: 0.94667\n",
      "Epoch [1030/10000], train loss: 0.37599 train acc: 0.97333 val_loss: 0.36665, val_acc: 0.94667\n",
      "Epoch [1040/10000], train loss: 0.37455 train acc: 0.97333 val_loss: 0.36527, val_acc: 0.94667\n",
      "Epoch [1050/10000], train loss: 0.37312 train acc: 0.97333 val_loss: 0.36390, val_acc: 0.94667\n",
      "Epoch [1060/10000], train loss: 0.37170 train acc: 0.97333 val_loss: 0.36255, val_acc: 0.94667\n",
      "Epoch [1070/10000], train loss: 0.37030 train acc: 0.97333 val_loss: 0.36120, val_acc: 0.94667\n",
      "Epoch [1080/10000], train loss: 0.36892 train acc: 0.97333 val_loss: 0.35988, val_acc: 0.94667\n",
      "Epoch [1090/10000], train loss: 0.36755 train acc: 0.97333 val_loss: 0.35856, val_acc: 0.94667\n",
      "Epoch [1100/10000], train loss: 0.36619 train acc: 0.97333 val_loss: 0.35726, val_acc: 0.94667\n",
      "Epoch [1110/10000], train loss: 0.36484 train acc: 0.97333 val_loss: 0.35597, val_acc: 0.94667\n",
      "Epoch [1120/10000], train loss: 0.36350 train acc: 0.97333 val_loss: 0.35470, val_acc: 0.94667\n",
      "Epoch [1130/10000], train loss: 0.36218 train acc: 0.97333 val_loss: 0.35344, val_acc: 0.94667\n",
      "Epoch [1140/10000], train loss: 0.36087 train acc: 0.97333 val_loss: 0.35218, val_acc: 0.94667\n",
      "Epoch [1150/10000], train loss: 0.35958 train acc: 0.97333 val_loss: 0.35095, val_acc: 0.94667\n",
      "Epoch [1160/10000], train loss: 0.35829 train acc: 0.97333 val_loss: 0.34972, val_acc: 0.94667\n",
      "Epoch [1170/10000], train loss: 0.35702 train acc: 0.97333 val_loss: 0.34850, val_acc: 0.94667\n",
      "Epoch [1180/10000], train loss: 0.35576 train acc: 0.97333 val_loss: 0.34730, val_acc: 0.94667\n",
      "Epoch [1190/10000], train loss: 0.35451 train acc: 0.97333 val_loss: 0.34611, val_acc: 0.94667\n",
      "Epoch [1200/10000], train loss: 0.35327 train acc: 0.97333 val_loss: 0.34493, val_acc: 0.94667\n",
      "Epoch [1210/10000], train loss: 0.35204 train acc: 0.97333 val_loss: 0.34376, val_acc: 0.94667\n",
      "Epoch [1220/10000], train loss: 0.35082 train acc: 0.97333 val_loss: 0.34260, val_acc: 0.94667\n",
      "Epoch [1230/10000], train loss: 0.34962 train acc: 0.97333 val_loss: 0.34145, val_acc: 0.94667\n",
      "Epoch [1240/10000], train loss: 0.34842 train acc: 0.97333 val_loss: 0.34031, val_acc: 0.94667\n",
      "Epoch [1250/10000], train loss: 0.34723 train acc: 0.97333 val_loss: 0.33918, val_acc: 0.94667\n",
      "Epoch [1260/10000], train loss: 0.34606 train acc: 0.97333 val_loss: 0.33806, val_acc: 0.94667\n",
      "Epoch [1270/10000], train loss: 0.34489 train acc: 0.97333 val_loss: 0.33695, val_acc: 0.94667\n",
      "Epoch [1280/10000], train loss: 0.34374 train acc: 0.97333 val_loss: 0.33585, val_acc: 0.94667\n",
      "Epoch [1290/10000], train loss: 0.34259 train acc: 0.97333 val_loss: 0.33477, val_acc: 0.94667\n",
      "Epoch [1300/10000], train loss: 0.34146 train acc: 0.97333 val_loss: 0.33369, val_acc: 0.94667\n",
      "Epoch [1310/10000], train loss: 0.34033 train acc: 0.97333 val_loss: 0.33262, val_acc: 0.94667\n",
      "Epoch [1320/10000], train loss: 0.33922 train acc: 0.97333 val_loss: 0.33156, val_acc: 0.94667\n",
      "Epoch [1330/10000], train loss: 0.33811 train acc: 0.97333 val_loss: 0.33050, val_acc: 0.94667\n",
      "Epoch [1340/10000], train loss: 0.33701 train acc: 0.97333 val_loss: 0.32946, val_acc: 0.94667\n",
      "Epoch [1350/10000], train loss: 0.33592 train acc: 0.97333 val_loss: 0.32843, val_acc: 0.94667\n",
      "Epoch [1360/10000], train loss: 0.33484 train acc: 0.97333 val_loss: 0.32740, val_acc: 0.94667\n",
      "Epoch [1370/10000], train loss: 0.33377 train acc: 0.97333 val_loss: 0.32639, val_acc: 0.94667\n",
      "Epoch [1380/10000], train loss: 0.33271 train acc: 0.97333 val_loss: 0.32538, val_acc: 0.94667\n",
      "Epoch [1390/10000], train loss: 0.33165 train acc: 0.97333 val_loss: 0.32438, val_acc: 0.94667\n",
      "Epoch [1400/10000], train loss: 0.33061 train acc: 0.97333 val_loss: 0.32339, val_acc: 0.94667\n",
      "Epoch [1410/10000], train loss: 0.32957 train acc: 0.97333 val_loss: 0.32241, val_acc: 0.94667\n",
      "Epoch [1420/10000], train loss: 0.32854 train acc: 0.97333 val_loss: 0.32143, val_acc: 0.94667\n",
      "Epoch [1430/10000], train loss: 0.32752 train acc: 0.97333 val_loss: 0.32046, val_acc: 0.94667\n",
      "Epoch [1440/10000], train loss: 0.32651 train acc: 0.97333 val_loss: 0.31950, val_acc: 0.96000\n",
      "Epoch [1450/10000], train loss: 0.32551 train acc: 0.97333 val_loss: 0.31855, val_acc: 0.96000\n",
      "Epoch [1460/10000], train loss: 0.32451 train acc: 0.97333 val_loss: 0.31761, val_acc: 0.96000\n",
      "Epoch [1470/10000], train loss: 0.32352 train acc: 0.97333 val_loss: 0.31667, val_acc: 0.96000\n",
      "Epoch [1480/10000], train loss: 0.32254 train acc: 0.97333 val_loss: 0.31575, val_acc: 0.96000\n",
      "Epoch [1490/10000], train loss: 0.32157 train acc: 0.97333 val_loss: 0.31482, val_acc: 0.96000\n",
      "Epoch [1500/10000], train loss: 0.32060 train acc: 0.97333 val_loss: 0.31391, val_acc: 0.96000\n",
      "Epoch [1510/10000], train loss: 0.31964 train acc: 0.97333 val_loss: 0.31300, val_acc: 0.96000\n",
      "Epoch [1520/10000], train loss: 0.31869 train acc: 0.97333 val_loss: 0.31210, val_acc: 0.96000\n",
      "Epoch [1530/10000], train loss: 0.31774 train acc: 0.97333 val_loss: 0.31121, val_acc: 0.96000\n",
      "Epoch [1540/10000], train loss: 0.31681 train acc: 0.97333 val_loss: 0.31033, val_acc: 0.96000\n",
      "Epoch [1550/10000], train loss: 0.31588 train acc: 0.97333 val_loss: 0.30945, val_acc: 0.96000\n",
      "Epoch [1560/10000], train loss: 0.31495 train acc: 0.97333 val_loss: 0.30858, val_acc: 0.96000\n",
      "Epoch [1570/10000], train loss: 0.31404 train acc: 0.97333 val_loss: 0.30771, val_acc: 0.96000\n",
      "Epoch [1580/10000], train loss: 0.31313 train acc: 0.97333 val_loss: 0.30685, val_acc: 0.96000\n",
      "Epoch [1590/10000], train loss: 0.31223 train acc: 0.97333 val_loss: 0.30600, val_acc: 0.96000\n",
      "Epoch [1600/10000], train loss: 0.31133 train acc: 0.97333 val_loss: 0.30515, val_acc: 0.96000\n",
      "Epoch [1610/10000], train loss: 0.31044 train acc: 0.97333 val_loss: 0.30431, val_acc: 0.96000\n",
      "Epoch [1620/10000], train loss: 0.30956 train acc: 0.97333 val_loss: 0.30348, val_acc: 0.96000\n",
      "Epoch [1630/10000], train loss: 0.30868 train acc: 0.97333 val_loss: 0.30265, val_acc: 0.96000\n",
      "Epoch [1640/10000], train loss: 0.30781 train acc: 0.97333 val_loss: 0.30183, val_acc: 0.96000\n",
      "Epoch [1650/10000], train loss: 0.30695 train acc: 0.97333 val_loss: 0.30102, val_acc: 0.96000\n",
      "Epoch [1660/10000], train loss: 0.30609 train acc: 0.97333 val_loss: 0.30021, val_acc: 0.96000\n",
      "Epoch [1670/10000], train loss: 0.30524 train acc: 0.97333 val_loss: 0.29941, val_acc: 0.96000\n",
      "Epoch [1680/10000], train loss: 0.30439 train acc: 0.97333 val_loss: 0.29861, val_acc: 0.96000\n",
      "Epoch [1690/10000], train loss: 0.30355 train acc: 0.97333 val_loss: 0.29782, val_acc: 0.96000\n",
      "Epoch [1700/10000], train loss: 0.30272 train acc: 0.97333 val_loss: 0.29703, val_acc: 0.96000\n",
      "Epoch [1710/10000], train loss: 0.30189 train acc: 0.97333 val_loss: 0.29625, val_acc: 0.96000\n",
      "Epoch [1720/10000], train loss: 0.30107 train acc: 0.97333 val_loss: 0.29548, val_acc: 0.96000\n",
      "Epoch [1730/10000], train loss: 0.30025 train acc: 0.97333 val_loss: 0.29471, val_acc: 0.96000\n",
      "Epoch [1740/10000], train loss: 0.29944 train acc: 0.97333 val_loss: 0.29395, val_acc: 0.96000\n",
      "Epoch [1750/10000], train loss: 0.29864 train acc: 0.97333 val_loss: 0.29319, val_acc: 0.96000\n",
      "Epoch [1760/10000], train loss: 0.29784 train acc: 0.97333 val_loss: 0.29244, val_acc: 0.96000\n",
      "Epoch [1770/10000], train loss: 0.29704 train acc: 0.97333 val_loss: 0.29169, val_acc: 0.96000\n",
      "Epoch [1780/10000], train loss: 0.29626 train acc: 0.97333 val_loss: 0.29095, val_acc: 0.96000\n",
      "Epoch [1790/10000], train loss: 0.29547 train acc: 0.97333 val_loss: 0.29022, val_acc: 0.96000\n",
      "Epoch [1800/10000], train loss: 0.29470 train acc: 0.97333 val_loss: 0.28948, val_acc: 0.96000\n",
      "Epoch [1810/10000], train loss: 0.29392 train acc: 0.97333 val_loss: 0.28876, val_acc: 0.96000\n",
      "Epoch [1820/10000], train loss: 0.29316 train acc: 0.97333 val_loss: 0.28804, val_acc: 0.96000\n",
      "Epoch [1830/10000], train loss: 0.29239 train acc: 0.97333 val_loss: 0.28732, val_acc: 0.96000\n",
      "Epoch [1840/10000], train loss: 0.29164 train acc: 0.97333 val_loss: 0.28661, val_acc: 0.96000\n",
      "Epoch [1850/10000], train loss: 0.29089 train acc: 0.97333 val_loss: 0.28590, val_acc: 0.96000\n",
      "Epoch [1860/10000], train loss: 0.29014 train acc: 0.97333 val_loss: 0.28520, val_acc: 0.96000\n",
      "Epoch [1870/10000], train loss: 0.28940 train acc: 0.97333 val_loss: 0.28451, val_acc: 0.96000\n",
      "Epoch [1880/10000], train loss: 0.28866 train acc: 0.97333 val_loss: 0.28381, val_acc: 0.96000\n",
      "Epoch [1890/10000], train loss: 0.28793 train acc: 0.97333 val_loss: 0.28313, val_acc: 0.96000\n",
      "Epoch [1900/10000], train loss: 0.28720 train acc: 0.97333 val_loss: 0.28244, val_acc: 0.96000\n",
      "Epoch [1910/10000], train loss: 0.28648 train acc: 0.97333 val_loss: 0.28177, val_acc: 0.97333\n",
      "Epoch [1920/10000], train loss: 0.28576 train acc: 0.97333 val_loss: 0.28109, val_acc: 0.97333\n",
      "Epoch [1930/10000], train loss: 0.28505 train acc: 0.97333 val_loss: 0.28042, val_acc: 0.97333\n",
      "Epoch [1940/10000], train loss: 0.28434 train acc: 0.97333 val_loss: 0.27976, val_acc: 0.97333\n",
      "Epoch [1950/10000], train loss: 0.28364 train acc: 0.97333 val_loss: 0.27910, val_acc: 0.97333\n",
      "Epoch [1960/10000], train loss: 0.28294 train acc: 0.97333 val_loss: 0.27844, val_acc: 0.97333\n",
      "Epoch [1970/10000], train loss: 0.28225 train acc: 0.97333 val_loss: 0.27779, val_acc: 0.97333\n",
      "Epoch [1980/10000], train loss: 0.28156 train acc: 0.97333 val_loss: 0.27715, val_acc: 0.97333\n",
      "Epoch [1990/10000], train loss: 0.28087 train acc: 0.97333 val_loss: 0.27650, val_acc: 0.97333\n",
      "Epoch [2000/10000], train loss: 0.28019 train acc: 0.97333 val_loss: 0.27587, val_acc: 0.97333\n",
      "Epoch [2010/10000], train loss: 0.27951 train acc: 0.97333 val_loss: 0.27523, val_acc: 0.97333\n",
      "Epoch [2020/10000], train loss: 0.27884 train acc: 0.97333 val_loss: 0.27460, val_acc: 0.97333\n",
      "Epoch [2030/10000], train loss: 0.27817 train acc: 0.97333 val_loss: 0.27397, val_acc: 0.97333\n",
      "Epoch [2040/10000], train loss: 0.27751 train acc: 0.97333 val_loss: 0.27335, val_acc: 0.97333\n",
      "Epoch [2050/10000], train loss: 0.27685 train acc: 0.97333 val_loss: 0.27273, val_acc: 0.97333\n",
      "Epoch [2060/10000], train loss: 0.27619 train acc: 0.97333 val_loss: 0.27212, val_acc: 0.97333\n",
      "Epoch [2070/10000], train loss: 0.27554 train acc: 0.97333 val_loss: 0.27151, val_acc: 0.97333\n",
      "Epoch [2080/10000], train loss: 0.27489 train acc: 0.97333 val_loss: 0.27090, val_acc: 0.97333\n",
      "Epoch [2090/10000], train loss: 0.27425 train acc: 0.97333 val_loss: 0.27030, val_acc: 0.97333\n",
      "Epoch [2100/10000], train loss: 0.27361 train acc: 0.97333 val_loss: 0.26970, val_acc: 0.97333\n",
      "Epoch [2110/10000], train loss: 0.27297 train acc: 0.97333 val_loss: 0.26911, val_acc: 0.97333\n",
      "Epoch [2120/10000], train loss: 0.27234 train acc: 0.97333 val_loss: 0.26852, val_acc: 0.97333\n",
      "Epoch [2130/10000], train loss: 0.27171 train acc: 0.97333 val_loss: 0.26793, val_acc: 0.97333\n",
      "Epoch [2140/10000], train loss: 0.27109 train acc: 0.97333 val_loss: 0.26734, val_acc: 0.97333\n",
      "Epoch [2150/10000], train loss: 0.27047 train acc: 0.97333 val_loss: 0.26676, val_acc: 0.97333\n",
      "Epoch [2160/10000], train loss: 0.26985 train acc: 0.97333 val_loss: 0.26619, val_acc: 0.97333\n",
      "Epoch [2170/10000], train loss: 0.26924 train acc: 0.97333 val_loss: 0.26561, val_acc: 0.97333\n",
      "Epoch [2180/10000], train loss: 0.26863 train acc: 0.97333 val_loss: 0.26505, val_acc: 0.97333\n",
      "Epoch [2190/10000], train loss: 0.26802 train acc: 0.97333 val_loss: 0.26448, val_acc: 0.97333\n",
      "Epoch [2200/10000], train loss: 0.26742 train acc: 0.97333 val_loss: 0.26392, val_acc: 0.97333\n",
      "Epoch [2210/10000], train loss: 0.26682 train acc: 0.97333 val_loss: 0.26336, val_acc: 0.97333\n",
      "Epoch [2220/10000], train loss: 0.26623 train acc: 0.97333 val_loss: 0.26280, val_acc: 0.97333\n",
      "Epoch [2230/10000], train loss: 0.26564 train acc: 0.97333 val_loss: 0.26225, val_acc: 0.97333\n",
      "Epoch [2240/10000], train loss: 0.26505 train acc: 0.97333 val_loss: 0.26170, val_acc: 0.97333\n",
      "Epoch [2250/10000], train loss: 0.26446 train acc: 0.97333 val_loss: 0.26116, val_acc: 0.97333\n",
      "Epoch [2260/10000], train loss: 0.26388 train acc: 0.97333 val_loss: 0.26061, val_acc: 0.97333\n",
      "Epoch [2270/10000], train loss: 0.26331 train acc: 0.97333 val_loss: 0.26007, val_acc: 0.97333\n",
      "Epoch [2280/10000], train loss: 0.26273 train acc: 0.97333 val_loss: 0.25954, val_acc: 0.97333\n",
      "Epoch [2290/10000], train loss: 0.26216 train acc: 0.97333 val_loss: 0.25901, val_acc: 0.97333\n",
      "Epoch [2300/10000], train loss: 0.26159 train acc: 0.97333 val_loss: 0.25848, val_acc: 0.97333\n",
      "Epoch [2310/10000], train loss: 0.26103 train acc: 0.97333 val_loss: 0.25795, val_acc: 0.97333\n",
      "Epoch [2320/10000], train loss: 0.26047 train acc: 0.97333 val_loss: 0.25743, val_acc: 0.97333\n",
      "Epoch [2330/10000], train loss: 0.25991 train acc: 0.97333 val_loss: 0.25691, val_acc: 0.97333\n",
      "Epoch [2340/10000], train loss: 0.25936 train acc: 0.97333 val_loss: 0.25639, val_acc: 0.97333\n",
      "Epoch [2350/10000], train loss: 0.25881 train acc: 0.97333 val_loss: 0.25587, val_acc: 0.97333\n",
      "Epoch [2360/10000], train loss: 0.25826 train acc: 0.97333 val_loss: 0.25536, val_acc: 0.97333\n",
      "Epoch [2370/10000], train loss: 0.25771 train acc: 0.97333 val_loss: 0.25486, val_acc: 0.97333\n",
      "Epoch [2380/10000], train loss: 0.25717 train acc: 0.97333 val_loss: 0.25435, val_acc: 0.97333\n",
      "Epoch [2390/10000], train loss: 0.25663 train acc: 0.97333 val_loss: 0.25385, val_acc: 0.96000\n",
      "Epoch [2400/10000], train loss: 0.25610 train acc: 0.97333 val_loss: 0.25335, val_acc: 0.96000\n",
      "Epoch [2410/10000], train loss: 0.25556 train acc: 0.97333 val_loss: 0.25285, val_acc: 0.96000\n",
      "Epoch [2420/10000], train loss: 0.25503 train acc: 0.97333 val_loss: 0.25236, val_acc: 0.96000\n",
      "Epoch [2430/10000], train loss: 0.25451 train acc: 0.97333 val_loss: 0.25187, val_acc: 0.96000\n",
      "Epoch [2440/10000], train loss: 0.25398 train acc: 0.97333 val_loss: 0.25138, val_acc: 0.96000\n",
      "Epoch [2450/10000], train loss: 0.25346 train acc: 0.97333 val_loss: 0.25090, val_acc: 0.96000\n",
      "Epoch [2460/10000], train loss: 0.25294 train acc: 0.97333 val_loss: 0.25041, val_acc: 0.96000\n",
      "Epoch [2470/10000], train loss: 0.25243 train acc: 0.97333 val_loss: 0.24993, val_acc: 0.96000\n",
      "Epoch [2480/10000], train loss: 0.25192 train acc: 0.97333 val_loss: 0.24946, val_acc: 0.96000\n",
      "Epoch [2490/10000], train loss: 0.25141 train acc: 0.97333 val_loss: 0.24898, val_acc: 0.96000\n",
      "Epoch [2500/10000], train loss: 0.25090 train acc: 0.97333 val_loss: 0.24851, val_acc: 0.96000\n",
      "Epoch [2510/10000], train loss: 0.25040 train acc: 0.97333 val_loss: 0.24804, val_acc: 0.96000\n",
      "Epoch [2520/10000], train loss: 0.24990 train acc: 0.97333 val_loss: 0.24758, val_acc: 0.96000\n",
      "Epoch [2530/10000], train loss: 0.24940 train acc: 0.97333 val_loss: 0.24711, val_acc: 0.96000\n",
      "Epoch [2540/10000], train loss: 0.24890 train acc: 0.97333 val_loss: 0.24665, val_acc: 0.96000\n",
      "Epoch [2550/10000], train loss: 0.24841 train acc: 0.97333 val_loss: 0.24619, val_acc: 0.96000\n",
      "Epoch [2560/10000], train loss: 0.24792 train acc: 0.97333 val_loss: 0.24574, val_acc: 0.96000\n",
      "Epoch [2570/10000], train loss: 0.24743 train acc: 0.97333 val_loss: 0.24528, val_acc: 0.96000\n",
      "Epoch [2580/10000], train loss: 0.24695 train acc: 0.97333 val_loss: 0.24483, val_acc: 0.96000\n",
      "Epoch [2590/10000], train loss: 0.24646 train acc: 0.97333 val_loss: 0.24438, val_acc: 0.96000\n",
      "Epoch [2600/10000], train loss: 0.24598 train acc: 0.97333 val_loss: 0.24394, val_acc: 0.96000\n",
      "Epoch [2610/10000], train loss: 0.24551 train acc: 0.97333 val_loss: 0.24349, val_acc: 0.96000\n",
      "Epoch [2620/10000], train loss: 0.24503 train acc: 0.97333 val_loss: 0.24305, val_acc: 0.96000\n",
      "Epoch [2630/10000], train loss: 0.24456 train acc: 0.97333 val_loss: 0.24261, val_acc: 0.96000\n",
      "Epoch [2640/10000], train loss: 0.24409 train acc: 0.97333 val_loss: 0.24218, val_acc: 0.96000\n",
      "Epoch [2650/10000], train loss: 0.24362 train acc: 0.97333 val_loss: 0.24174, val_acc: 0.96000\n",
      "Epoch [2660/10000], train loss: 0.24316 train acc: 0.97333 val_loss: 0.24131, val_acc: 0.96000\n",
      "Epoch [2670/10000], train loss: 0.24269 train acc: 0.97333 val_loss: 0.24088, val_acc: 0.96000\n",
      "Epoch [2680/10000], train loss: 0.24223 train acc: 0.97333 val_loss: 0.24045, val_acc: 0.96000\n",
      "Epoch [2690/10000], train loss: 0.24178 train acc: 0.97333 val_loss: 0.24003, val_acc: 0.96000\n",
      "Epoch [2700/10000], train loss: 0.24132 train acc: 0.97333 val_loss: 0.23961, val_acc: 0.96000\n",
      "Epoch [2710/10000], train loss: 0.24087 train acc: 0.97333 val_loss: 0.23919, val_acc: 0.96000\n",
      "Epoch [2720/10000], train loss: 0.24042 train acc: 0.97333 val_loss: 0.23877, val_acc: 0.96000\n",
      "Epoch [2730/10000], train loss: 0.23997 train acc: 0.97333 val_loss: 0.23835, val_acc: 0.96000\n",
      "Epoch [2740/10000], train loss: 0.23952 train acc: 0.97333 val_loss: 0.23794, val_acc: 0.96000\n",
      "Epoch [2750/10000], train loss: 0.23908 train acc: 0.97333 val_loss: 0.23753, val_acc: 0.96000\n",
      "Epoch [2760/10000], train loss: 0.23864 train acc: 0.97333 val_loss: 0.23712, val_acc: 0.96000\n",
      "Epoch [2770/10000], train loss: 0.23820 train acc: 0.97333 val_loss: 0.23671, val_acc: 0.96000\n",
      "Epoch [2780/10000], train loss: 0.23776 train acc: 0.97333 val_loss: 0.23631, val_acc: 0.96000\n",
      "Epoch [2790/10000], train loss: 0.23733 train acc: 0.97333 val_loss: 0.23590, val_acc: 0.96000\n",
      "Epoch [2800/10000], train loss: 0.23689 train acc: 0.97333 val_loss: 0.23550, val_acc: 0.96000\n",
      "Epoch [2810/10000], train loss: 0.23646 train acc: 0.97333 val_loss: 0.23510, val_acc: 0.96000\n",
      "Epoch [2820/10000], train loss: 0.23604 train acc: 0.97333 val_loss: 0.23471, val_acc: 0.96000\n",
      "Epoch [2830/10000], train loss: 0.23561 train acc: 0.97333 val_loss: 0.23431, val_acc: 0.96000\n",
      "Epoch [2840/10000], train loss: 0.23519 train acc: 0.97333 val_loss: 0.23392, val_acc: 0.96000\n",
      "Epoch [2850/10000], train loss: 0.23477 train acc: 0.97333 val_loss: 0.23353, val_acc: 0.96000\n",
      "Epoch [2860/10000], train loss: 0.23435 train acc: 0.97333 val_loss: 0.23314, val_acc: 0.96000\n",
      "Epoch [2870/10000], train loss: 0.23393 train acc: 0.97333 val_loss: 0.23275, val_acc: 0.96000\n",
      "Epoch [2880/10000], train loss: 0.23351 train acc: 0.97333 val_loss: 0.23237, val_acc: 0.96000\n",
      "Epoch [2890/10000], train loss: 0.23310 train acc: 0.97333 val_loss: 0.23199, val_acc: 0.96000\n",
      "Epoch [2900/10000], train loss: 0.23269 train acc: 0.97333 val_loss: 0.23160, val_acc: 0.96000\n",
      "Epoch [2910/10000], train loss: 0.23228 train acc: 0.97333 val_loss: 0.23123, val_acc: 0.96000\n",
      "Epoch [2920/10000], train loss: 0.23187 train acc: 0.97333 val_loss: 0.23085, val_acc: 0.96000\n",
      "Epoch [2930/10000], train loss: 0.23147 train acc: 0.97333 val_loss: 0.23047, val_acc: 0.96000\n",
      "Epoch [2940/10000], train loss: 0.23106 train acc: 0.97333 val_loss: 0.23010, val_acc: 0.96000\n",
      "Epoch [2950/10000], train loss: 0.23066 train acc: 0.97333 val_loss: 0.22973, val_acc: 0.96000\n",
      "Epoch [2960/10000], train loss: 0.23026 train acc: 0.97333 val_loss: 0.22936, val_acc: 0.96000\n",
      "Epoch [2970/10000], train loss: 0.22986 train acc: 0.97333 val_loss: 0.22899, val_acc: 0.96000\n",
      "Epoch [2980/10000], train loss: 0.22947 train acc: 0.97333 val_loss: 0.22863, val_acc: 0.96000\n",
      "Epoch [2990/10000], train loss: 0.22908 train acc: 0.98667 val_loss: 0.22826, val_acc: 0.96000\n",
      "Epoch [3000/10000], train loss: 0.22868 train acc: 0.98667 val_loss: 0.22790, val_acc: 0.96000\n",
      "Epoch [3010/10000], train loss: 0.22829 train acc: 0.98667 val_loss: 0.22754, val_acc: 0.96000\n",
      "Epoch [3020/10000], train loss: 0.22791 train acc: 0.98667 val_loss: 0.22718, val_acc: 0.96000\n",
      "Epoch [3030/10000], train loss: 0.22752 train acc: 0.98667 val_loss: 0.22682, val_acc: 0.96000\n",
      "Epoch [3040/10000], train loss: 0.22714 train acc: 0.98667 val_loss: 0.22647, val_acc: 0.96000\n",
      "Epoch [3050/10000], train loss: 0.22675 train acc: 0.98667 val_loss: 0.22612, val_acc: 0.96000\n",
      "Epoch [3060/10000], train loss: 0.22637 train acc: 0.98667 val_loss: 0.22576, val_acc: 0.96000\n",
      "Epoch [3070/10000], train loss: 0.22599 train acc: 0.98667 val_loss: 0.22541, val_acc: 0.96000\n",
      "Epoch [3080/10000], train loss: 0.22562 train acc: 0.98667 val_loss: 0.22507, val_acc: 0.96000\n",
      "Epoch [3090/10000], train loss: 0.22524 train acc: 0.98667 val_loss: 0.22472, val_acc: 0.96000\n",
      "Epoch [3100/10000], train loss: 0.22487 train acc: 0.98667 val_loss: 0.22437, val_acc: 0.96000\n",
      "Epoch [3110/10000], train loss: 0.22450 train acc: 0.98667 val_loss: 0.22403, val_acc: 0.96000\n",
      "Epoch [3120/10000], train loss: 0.22413 train acc: 0.98667 val_loss: 0.22369, val_acc: 0.96000\n",
      "Epoch [3130/10000], train loss: 0.22376 train acc: 0.98667 val_loss: 0.22335, val_acc: 0.96000\n",
      "Epoch [3140/10000], train loss: 0.22339 train acc: 0.98667 val_loss: 0.22301, val_acc: 0.96000\n",
      "Epoch [3150/10000], train loss: 0.22303 train acc: 0.98667 val_loss: 0.22267, val_acc: 0.96000\n",
      "Epoch [3160/10000], train loss: 0.22266 train acc: 0.98667 val_loss: 0.22234, val_acc: 0.96000\n",
      "Epoch [3170/10000], train loss: 0.22230 train acc: 0.98667 val_loss: 0.22201, val_acc: 0.96000\n",
      "Epoch [3180/10000], train loss: 0.22194 train acc: 0.98667 val_loss: 0.22167, val_acc: 0.96000\n",
      "Epoch [3190/10000], train loss: 0.22158 train acc: 0.98667 val_loss: 0.22134, val_acc: 0.96000\n",
      "Epoch [3200/10000], train loss: 0.22123 train acc: 0.98667 val_loss: 0.22101, val_acc: 0.96000\n",
      "Epoch [3210/10000], train loss: 0.22087 train acc: 0.98667 val_loss: 0.22069, val_acc: 0.96000\n",
      "Epoch [3220/10000], train loss: 0.22052 train acc: 0.98667 val_loss: 0.22036, val_acc: 0.96000\n",
      "Epoch [3230/10000], train loss: 0.22017 train acc: 0.98667 val_loss: 0.22004, val_acc: 0.96000\n",
      "Epoch [3240/10000], train loss: 0.21982 train acc: 0.98667 val_loss: 0.21971, val_acc: 0.96000\n",
      "Epoch [3250/10000], train loss: 0.21947 train acc: 0.98667 val_loss: 0.21939, val_acc: 0.96000\n",
      "Epoch [3260/10000], train loss: 0.21912 train acc: 0.98667 val_loss: 0.21907, val_acc: 0.96000\n",
      "Epoch [3270/10000], train loss: 0.21878 train acc: 0.98667 val_loss: 0.21876, val_acc: 0.96000\n",
      "Epoch [3280/10000], train loss: 0.21843 train acc: 0.98667 val_loss: 0.21844, val_acc: 0.96000\n",
      "Epoch [3290/10000], train loss: 0.21809 train acc: 0.98667 val_loss: 0.21812, val_acc: 0.96000\n",
      "Epoch [3300/10000], train loss: 0.21775 train acc: 0.98667 val_loss: 0.21781, val_acc: 0.96000\n",
      "Epoch [3310/10000], train loss: 0.21741 train acc: 0.98667 val_loss: 0.21750, val_acc: 0.96000\n",
      "Epoch [3320/10000], train loss: 0.21707 train acc: 0.98667 val_loss: 0.21719, val_acc: 0.96000\n",
      "Epoch [3330/10000], train loss: 0.21674 train acc: 0.98667 val_loss: 0.21688, val_acc: 0.96000\n",
      "Epoch [3340/10000], train loss: 0.21640 train acc: 0.98667 val_loss: 0.21657, val_acc: 0.96000\n",
      "Epoch [3350/10000], train loss: 0.21607 train acc: 0.98667 val_loss: 0.21626, val_acc: 0.96000\n",
      "Epoch [3360/10000], train loss: 0.21574 train acc: 0.98667 val_loss: 0.21596, val_acc: 0.96000\n",
      "Epoch [3370/10000], train loss: 0.21541 train acc: 0.98667 val_loss: 0.21565, val_acc: 0.96000\n",
      "Epoch [3380/10000], train loss: 0.21508 train acc: 0.98667 val_loss: 0.21535, val_acc: 0.96000\n",
      "Epoch [3390/10000], train loss: 0.21475 train acc: 0.98667 val_loss: 0.21505, val_acc: 0.96000\n",
      "Epoch [3400/10000], train loss: 0.21443 train acc: 0.98667 val_loss: 0.21475, val_acc: 0.96000\n",
      "Epoch [3410/10000], train loss: 0.21410 train acc: 0.98667 val_loss: 0.21445, val_acc: 0.96000\n",
      "Epoch [3420/10000], train loss: 0.21378 train acc: 0.98667 val_loss: 0.21415, val_acc: 0.96000\n",
      "Epoch [3430/10000], train loss: 0.21346 train acc: 0.98667 val_loss: 0.21386, val_acc: 0.96000\n",
      "Epoch [3440/10000], train loss: 0.21314 train acc: 0.98667 val_loss: 0.21356, val_acc: 0.96000\n",
      "Epoch [3450/10000], train loss: 0.21282 train acc: 0.98667 val_loss: 0.21327, val_acc: 0.96000\n",
      "Epoch [3460/10000], train loss: 0.21250 train acc: 0.98667 val_loss: 0.21298, val_acc: 0.96000\n",
      "Epoch [3470/10000], train loss: 0.21219 train acc: 0.98667 val_loss: 0.21269, val_acc: 0.96000\n",
      "Epoch [3480/10000], train loss: 0.21187 train acc: 0.98667 val_loss: 0.21240, val_acc: 0.96000\n",
      "Epoch [3490/10000], train loss: 0.21156 train acc: 0.98667 val_loss: 0.21211, val_acc: 0.96000\n",
      "Epoch [3500/10000], train loss: 0.21125 train acc: 0.98667 val_loss: 0.21182, val_acc: 0.96000\n",
      "Epoch [3510/10000], train loss: 0.21093 train acc: 0.98667 val_loss: 0.21154, val_acc: 0.96000\n",
      "Epoch [3520/10000], train loss: 0.21063 train acc: 0.98667 val_loss: 0.21125, val_acc: 0.96000\n",
      "Epoch [3530/10000], train loss: 0.21032 train acc: 0.98667 val_loss: 0.21097, val_acc: 0.96000\n",
      "Epoch [3540/10000], train loss: 0.21001 train acc: 0.98667 val_loss: 0.21069, val_acc: 0.96000\n",
      "Epoch [3550/10000], train loss: 0.20971 train acc: 0.98667 val_loss: 0.21041, val_acc: 0.96000\n",
      "Epoch [3560/10000], train loss: 0.20940 train acc: 0.98667 val_loss: 0.21013, val_acc: 0.96000\n",
      "Epoch [3570/10000], train loss: 0.20910 train acc: 0.98667 val_loss: 0.20985, val_acc: 0.96000\n",
      "Epoch [3580/10000], train loss: 0.20880 train acc: 0.98667 val_loss: 0.20957, val_acc: 0.96000\n",
      "Epoch [3590/10000], train loss: 0.20850 train acc: 0.98667 val_loss: 0.20930, val_acc: 0.96000\n",
      "Epoch [3600/10000], train loss: 0.20820 train acc: 0.98667 val_loss: 0.20902, val_acc: 0.96000\n",
      "Epoch [3610/10000], train loss: 0.20790 train acc: 0.98667 val_loss: 0.20875, val_acc: 0.96000\n",
      "Epoch [3620/10000], train loss: 0.20760 train acc: 0.98667 val_loss: 0.20848, val_acc: 0.96000\n",
      "Epoch [3630/10000], train loss: 0.20731 train acc: 0.98667 val_loss: 0.20821, val_acc: 0.96000\n",
      "Epoch [3640/10000], train loss: 0.20701 train acc: 0.98667 val_loss: 0.20794, val_acc: 0.96000\n",
      "Epoch [3650/10000], train loss: 0.20672 train acc: 0.98667 val_loss: 0.20767, val_acc: 0.96000\n",
      "Epoch [3660/10000], train loss: 0.20643 train acc: 0.98667 val_loss: 0.20740, val_acc: 0.96000\n",
      "Epoch [3670/10000], train loss: 0.20614 train acc: 0.98667 val_loss: 0.20713, val_acc: 0.96000\n",
      "Epoch [3680/10000], train loss: 0.20585 train acc: 0.98667 val_loss: 0.20687, val_acc: 0.96000\n",
      "Epoch [3690/10000], train loss: 0.20556 train acc: 0.98667 val_loss: 0.20660, val_acc: 0.96000\n",
      "Epoch [3700/10000], train loss: 0.20527 train acc: 0.98667 val_loss: 0.20634, val_acc: 0.96000\n",
      "Epoch [3710/10000], train loss: 0.20499 train acc: 0.98667 val_loss: 0.20608, val_acc: 0.96000\n",
      "Epoch [3720/10000], train loss: 0.20470 train acc: 0.98667 val_loss: 0.20582, val_acc: 0.96000\n",
      "Epoch [3730/10000], train loss: 0.20442 train acc: 0.98667 val_loss: 0.20556, val_acc: 0.96000\n",
      "Epoch [3740/10000], train loss: 0.20414 train acc: 0.98667 val_loss: 0.20530, val_acc: 0.96000\n",
      "Epoch [3750/10000], train loss: 0.20386 train acc: 0.98667 val_loss: 0.20504, val_acc: 0.96000\n",
      "Epoch [3760/10000], train loss: 0.20358 train acc: 0.98667 val_loss: 0.20479, val_acc: 0.96000\n",
      "Epoch [3770/10000], train loss: 0.20330 train acc: 0.98667 val_loss: 0.20453, val_acc: 0.96000\n",
      "Epoch [3780/10000], train loss: 0.20302 train acc: 0.98667 val_loss: 0.20428, val_acc: 0.96000\n",
      "Epoch [3790/10000], train loss: 0.20274 train acc: 0.98667 val_loss: 0.20402, val_acc: 0.96000\n",
      "Epoch [3800/10000], train loss: 0.20247 train acc: 0.98667 val_loss: 0.20377, val_acc: 0.96000\n",
      "Epoch [3810/10000], train loss: 0.20219 train acc: 0.98667 val_loss: 0.20352, val_acc: 0.96000\n",
      "Epoch [3820/10000], train loss: 0.20192 train acc: 0.98667 val_loss: 0.20327, val_acc: 0.96000\n",
      "Epoch [3830/10000], train loss: 0.20165 train acc: 0.98667 val_loss: 0.20302, val_acc: 0.96000\n",
      "Epoch [3840/10000], train loss: 0.20138 train acc: 0.98667 val_loss: 0.20277, val_acc: 0.96000\n",
      "Epoch [3850/10000], train loss: 0.20111 train acc: 0.98667 val_loss: 0.20253, val_acc: 0.96000\n",
      "Epoch [3860/10000], train loss: 0.20084 train acc: 0.98667 val_loss: 0.20228, val_acc: 0.96000\n",
      "Epoch [3870/10000], train loss: 0.20057 train acc: 0.98667 val_loss: 0.20203, val_acc: 0.96000\n",
      "Epoch [3880/10000], train loss: 0.20030 train acc: 0.98667 val_loss: 0.20179, val_acc: 0.96000\n",
      "Epoch [3890/10000], train loss: 0.20004 train acc: 0.98667 val_loss: 0.20155, val_acc: 0.96000\n",
      "Epoch [3900/10000], train loss: 0.19977 train acc: 0.98667 val_loss: 0.20130, val_acc: 0.96000\n",
      "Epoch [3910/10000], train loss: 0.19951 train acc: 0.98667 val_loss: 0.20106, val_acc: 0.96000\n",
      "Epoch [3920/10000], train loss: 0.19925 train acc: 0.98667 val_loss: 0.20082, val_acc: 0.96000\n",
      "Epoch [3930/10000], train loss: 0.19899 train acc: 0.98667 val_loss: 0.20058, val_acc: 0.96000\n",
      "Epoch [3940/10000], train loss: 0.19873 train acc: 0.98667 val_loss: 0.20035, val_acc: 0.96000\n",
      "Epoch [3950/10000], train loss: 0.19847 train acc: 0.98667 val_loss: 0.20011, val_acc: 0.96000\n",
      "Epoch [3960/10000], train loss: 0.19821 train acc: 0.98667 val_loss: 0.19987, val_acc: 0.96000\n",
      "Epoch [3970/10000], train loss: 0.19795 train acc: 0.98667 val_loss: 0.19964, val_acc: 0.96000\n",
      "Epoch [3980/10000], train loss: 0.19769 train acc: 0.98667 val_loss: 0.19940, val_acc: 0.96000\n",
      "Epoch [3990/10000], train loss: 0.19744 train acc: 0.98667 val_loss: 0.19917, val_acc: 0.96000\n",
      "Epoch [4000/10000], train loss: 0.19718 train acc: 0.98667 val_loss: 0.19894, val_acc: 0.96000\n",
      "Epoch [4010/10000], train loss: 0.19693 train acc: 0.98667 val_loss: 0.19870, val_acc: 0.96000\n",
      "Epoch [4020/10000], train loss: 0.19668 train acc: 0.98667 val_loss: 0.19847, val_acc: 0.96000\n",
      "Epoch [4030/10000], train loss: 0.19643 train acc: 0.98667 val_loss: 0.19824, val_acc: 0.96000\n",
      "Epoch [4040/10000], train loss: 0.19617 train acc: 0.98667 val_loss: 0.19802, val_acc: 0.96000\n",
      "Epoch [4050/10000], train loss: 0.19593 train acc: 0.98667 val_loss: 0.19779, val_acc: 0.96000\n",
      "Epoch [4060/10000], train loss: 0.19568 train acc: 0.98667 val_loss: 0.19756, val_acc: 0.96000\n",
      "Epoch [4070/10000], train loss: 0.19543 train acc: 0.98667 val_loss: 0.19733, val_acc: 0.96000\n",
      "Epoch [4080/10000], train loss: 0.19518 train acc: 0.98667 val_loss: 0.19711, val_acc: 0.96000\n",
      "Epoch [4090/10000], train loss: 0.19494 train acc: 0.98667 val_loss: 0.19688, val_acc: 0.96000\n",
      "Epoch [4100/10000], train loss: 0.19469 train acc: 0.98667 val_loss: 0.19666, val_acc: 0.96000\n",
      "Epoch [4110/10000], train loss: 0.19445 train acc: 0.98667 val_loss: 0.19644, val_acc: 0.96000\n",
      "Epoch [4120/10000], train loss: 0.19420 train acc: 0.98667 val_loss: 0.19622, val_acc: 0.96000\n",
      "Epoch [4130/10000], train loss: 0.19396 train acc: 0.98667 val_loss: 0.19599, val_acc: 0.96000\n",
      "Epoch [4140/10000], train loss: 0.19372 train acc: 0.98667 val_loss: 0.19577, val_acc: 0.97333\n",
      "Epoch [4150/10000], train loss: 0.19348 train acc: 0.98667 val_loss: 0.19556, val_acc: 0.97333\n",
      "Epoch [4160/10000], train loss: 0.19324 train acc: 0.98667 val_loss: 0.19534, val_acc: 0.97333\n",
      "Epoch [4170/10000], train loss: 0.19300 train acc: 0.98667 val_loss: 0.19512, val_acc: 0.97333\n",
      "Epoch [4180/10000], train loss: 0.19276 train acc: 0.98667 val_loss: 0.19490, val_acc: 0.97333\n",
      "Epoch [4190/10000], train loss: 0.19253 train acc: 0.98667 val_loss: 0.19469, val_acc: 0.97333\n",
      "Epoch [4200/10000], train loss: 0.19229 train acc: 0.98667 val_loss: 0.19447, val_acc: 0.97333\n",
      "Epoch [4210/10000], train loss: 0.19206 train acc: 0.98667 val_loss: 0.19426, val_acc: 0.97333\n",
      "Epoch [4220/10000], train loss: 0.19182 train acc: 0.98667 val_loss: 0.19404, val_acc: 0.97333\n",
      "Epoch [4230/10000], train loss: 0.19159 train acc: 0.98667 val_loss: 0.19383, val_acc: 0.97333\n",
      "Epoch [4240/10000], train loss: 0.19135 train acc: 0.98667 val_loss: 0.19362, val_acc: 0.97333\n",
      "Epoch [4250/10000], train loss: 0.19112 train acc: 0.98667 val_loss: 0.19341, val_acc: 0.97333\n",
      "Epoch [4260/10000], train loss: 0.19089 train acc: 0.98667 val_loss: 0.19320, val_acc: 0.97333\n",
      "Epoch [4270/10000], train loss: 0.19066 train acc: 0.98667 val_loss: 0.19299, val_acc: 0.97333\n",
      "Epoch [4280/10000], train loss: 0.19043 train acc: 0.98667 val_loss: 0.19278, val_acc: 0.97333\n",
      "Epoch [4290/10000], train loss: 0.19020 train acc: 0.98667 val_loss: 0.19257, val_acc: 0.97333\n",
      "Epoch [4300/10000], train loss: 0.18998 train acc: 0.98667 val_loss: 0.19236, val_acc: 0.97333\n",
      "Epoch [4310/10000], train loss: 0.18975 train acc: 0.98667 val_loss: 0.19216, val_acc: 0.97333\n",
      "Epoch [4320/10000], train loss: 0.18952 train acc: 0.98667 val_loss: 0.19195, val_acc: 0.97333\n",
      "Epoch [4330/10000], train loss: 0.18930 train acc: 0.98667 val_loss: 0.19175, val_acc: 0.97333\n",
      "Epoch [4340/10000], train loss: 0.18908 train acc: 0.98667 val_loss: 0.19154, val_acc: 0.97333\n",
      "Epoch [4350/10000], train loss: 0.18885 train acc: 0.98667 val_loss: 0.19134, val_acc: 0.97333\n",
      "Epoch [4360/10000], train loss: 0.18863 train acc: 0.98667 val_loss: 0.19114, val_acc: 0.97333\n",
      "Epoch [4370/10000], train loss: 0.18841 train acc: 0.98667 val_loss: 0.19094, val_acc: 0.97333\n",
      "Epoch [4380/10000], train loss: 0.18819 train acc: 0.98667 val_loss: 0.19074, val_acc: 0.97333\n",
      "Epoch [4390/10000], train loss: 0.18797 train acc: 0.98667 val_loss: 0.19053, val_acc: 0.97333\n",
      "Epoch [4400/10000], train loss: 0.18775 train acc: 0.98667 val_loss: 0.19034, val_acc: 0.97333\n",
      "Epoch [4410/10000], train loss: 0.18753 train acc: 0.98667 val_loss: 0.19014, val_acc: 0.97333\n",
      "Epoch [4420/10000], train loss: 0.18731 train acc: 0.98667 val_loss: 0.18994, val_acc: 0.97333\n",
      "Epoch [4430/10000], train loss: 0.18709 train acc: 0.98667 val_loss: 0.18974, val_acc: 0.97333\n",
      "Epoch [4440/10000], train loss: 0.18688 train acc: 0.98667 val_loss: 0.18954, val_acc: 0.97333\n",
      "Epoch [4450/10000], train loss: 0.18666 train acc: 0.98667 val_loss: 0.18935, val_acc: 0.97333\n",
      "Epoch [4460/10000], train loss: 0.18645 train acc: 0.98667 val_loss: 0.18915, val_acc: 0.97333\n",
      "Epoch [4470/10000], train loss: 0.18623 train acc: 0.98667 val_loss: 0.18896, val_acc: 0.97333\n",
      "Epoch [4480/10000], train loss: 0.18602 train acc: 0.98667 val_loss: 0.18876, val_acc: 0.97333\n",
      "Epoch [4490/10000], train loss: 0.18581 train acc: 0.98667 val_loss: 0.18857, val_acc: 0.97333\n",
      "Epoch [4500/10000], train loss: 0.18559 train acc: 0.98667 val_loss: 0.18838, val_acc: 0.97333\n",
      "Epoch [4510/10000], train loss: 0.18538 train acc: 0.98667 val_loss: 0.18819, val_acc: 0.97333\n",
      "Epoch [4520/10000], train loss: 0.18517 train acc: 0.98667 val_loss: 0.18800, val_acc: 0.97333\n",
      "Epoch [4530/10000], train loss: 0.18496 train acc: 0.98667 val_loss: 0.18781, val_acc: 0.97333\n",
      "Epoch [4540/10000], train loss: 0.18475 train acc: 0.98667 val_loss: 0.18762, val_acc: 0.97333\n",
      "Epoch [4550/10000], train loss: 0.18455 train acc: 0.98667 val_loss: 0.18743, val_acc: 0.97333\n",
      "Epoch [4560/10000], train loss: 0.18434 train acc: 0.98667 val_loss: 0.18724, val_acc: 0.97333\n",
      "Epoch [4570/10000], train loss: 0.18413 train acc: 0.98667 val_loss: 0.18705, val_acc: 0.97333\n",
      "Epoch [4580/10000], train loss: 0.18393 train acc: 0.98667 val_loss: 0.18686, val_acc: 0.97333\n",
      "Epoch [4590/10000], train loss: 0.18372 train acc: 0.98667 val_loss: 0.18668, val_acc: 0.97333\n",
      "Epoch [4600/10000], train loss: 0.18352 train acc: 0.98667 val_loss: 0.18649, val_acc: 0.97333\n",
      "Epoch [4610/10000], train loss: 0.18331 train acc: 0.98667 val_loss: 0.18631, val_acc: 0.97333\n",
      "Epoch [4620/10000], train loss: 0.18311 train acc: 0.98667 val_loss: 0.18612, val_acc: 0.97333\n",
      "Epoch [4630/10000], train loss: 0.18291 train acc: 0.98667 val_loss: 0.18594, val_acc: 0.97333\n",
      "Epoch [4640/10000], train loss: 0.18270 train acc: 0.98667 val_loss: 0.18576, val_acc: 0.97333\n",
      "Epoch [4650/10000], train loss: 0.18250 train acc: 0.98667 val_loss: 0.18557, val_acc: 0.97333\n",
      "Epoch [4660/10000], train loss: 0.18230 train acc: 0.98667 val_loss: 0.18539, val_acc: 0.97333\n",
      "Epoch [4670/10000], train loss: 0.18210 train acc: 0.98667 val_loss: 0.18521, val_acc: 0.97333\n",
      "Epoch [4680/10000], train loss: 0.18190 train acc: 0.98667 val_loss: 0.18503, val_acc: 0.97333\n",
      "Epoch [4690/10000], train loss: 0.18171 train acc: 0.98667 val_loss: 0.18485, val_acc: 0.97333\n",
      "Epoch [4700/10000], train loss: 0.18151 train acc: 0.98667 val_loss: 0.18467, val_acc: 0.97333\n",
      "Epoch [4710/10000], train loss: 0.18131 train acc: 0.98667 val_loss: 0.18449, val_acc: 0.97333\n",
      "Epoch [4720/10000], train loss: 0.18111 train acc: 0.98667 val_loss: 0.18432, val_acc: 0.97333\n",
      "Epoch [4730/10000], train loss: 0.18092 train acc: 0.98667 val_loss: 0.18414, val_acc: 0.97333\n",
      "Epoch [4740/10000], train loss: 0.18072 train acc: 0.98667 val_loss: 0.18396, val_acc: 0.97333\n",
      "Epoch [4750/10000], train loss: 0.18053 train acc: 0.98667 val_loss: 0.18379, val_acc: 0.97333\n",
      "Epoch [4760/10000], train loss: 0.18033 train acc: 0.98667 val_loss: 0.18361, val_acc: 0.97333\n",
      "Epoch [4770/10000], train loss: 0.18014 train acc: 0.98667 val_loss: 0.18344, val_acc: 0.97333\n",
      "Epoch [4780/10000], train loss: 0.17995 train acc: 0.98667 val_loss: 0.18326, val_acc: 0.97333\n",
      "Epoch [4790/10000], train loss: 0.17976 train acc: 0.98667 val_loss: 0.18309, val_acc: 0.97333\n",
      "Epoch [4800/10000], train loss: 0.17957 train acc: 0.98667 val_loss: 0.18291, val_acc: 0.97333\n",
      "Epoch [4810/10000], train loss: 0.17937 train acc: 0.98667 val_loss: 0.18274, val_acc: 0.97333\n",
      "Epoch [4820/10000], train loss: 0.17919 train acc: 0.98667 val_loss: 0.18257, val_acc: 0.97333\n",
      "Epoch [4830/10000], train loss: 0.17900 train acc: 0.98667 val_loss: 0.18240, val_acc: 0.97333\n",
      "Epoch [4840/10000], train loss: 0.17881 train acc: 0.98667 val_loss: 0.18223, val_acc: 0.97333\n",
      "Epoch [4850/10000], train loss: 0.17862 train acc: 0.98667 val_loss: 0.18206, val_acc: 0.97333\n",
      "Epoch [4860/10000], train loss: 0.17843 train acc: 0.98667 val_loss: 0.18189, val_acc: 0.97333\n",
      "Epoch [4870/10000], train loss: 0.17824 train acc: 0.98667 val_loss: 0.18172, val_acc: 0.97333\n",
      "Epoch [4880/10000], train loss: 0.17806 train acc: 0.98667 val_loss: 0.18155, val_acc: 0.97333\n",
      "Epoch [4890/10000], train loss: 0.17787 train acc: 0.98667 val_loss: 0.18138, val_acc: 0.97333\n",
      "Epoch [4900/10000], train loss: 0.17769 train acc: 0.98667 val_loss: 0.18121, val_acc: 0.97333\n",
      "Epoch [4910/10000], train loss: 0.17750 train acc: 0.98667 val_loss: 0.18105, val_acc: 0.97333\n",
      "Epoch [4920/10000], train loss: 0.17732 train acc: 0.98667 val_loss: 0.18088, val_acc: 0.97333\n",
      "Epoch [4930/10000], train loss: 0.17714 train acc: 0.98667 val_loss: 0.18072, val_acc: 0.97333\n",
      "Epoch [4940/10000], train loss: 0.17695 train acc: 0.98667 val_loss: 0.18055, val_acc: 0.97333\n",
      "Epoch [4950/10000], train loss: 0.17677 train acc: 0.98667 val_loss: 0.18039, val_acc: 0.97333\n",
      "Epoch [4960/10000], train loss: 0.17659 train acc: 0.98667 val_loss: 0.18022, val_acc: 0.97333\n",
      "Epoch [4970/10000], train loss: 0.17641 train acc: 0.98667 val_loss: 0.18006, val_acc: 0.97333\n",
      "Epoch [4980/10000], train loss: 0.17623 train acc: 0.98667 val_loss: 0.17990, val_acc: 0.97333\n",
      "Epoch [4990/10000], train loss: 0.17605 train acc: 0.98667 val_loss: 0.17973, val_acc: 0.97333\n",
      "Epoch [5000/10000], train loss: 0.17587 train acc: 0.98667 val_loss: 0.17957, val_acc: 0.97333\n",
      "Epoch [5010/10000], train loss: 0.17569 train acc: 0.98667 val_loss: 0.17941, val_acc: 0.97333\n",
      "Epoch [5020/10000], train loss: 0.17551 train acc: 0.98667 val_loss: 0.17925, val_acc: 0.97333\n",
      "Epoch [5030/10000], train loss: 0.17533 train acc: 0.98667 val_loss: 0.17909, val_acc: 0.97333\n",
      "Epoch [5040/10000], train loss: 0.17516 train acc: 0.98667 val_loss: 0.17893, val_acc: 0.97333\n",
      "Epoch [5050/10000], train loss: 0.17498 train acc: 0.98667 val_loss: 0.17877, val_acc: 0.97333\n",
      "Epoch [5060/10000], train loss: 0.17481 train acc: 0.98667 val_loss: 0.17861, val_acc: 0.97333\n",
      "Epoch [5070/10000], train loss: 0.17463 train acc: 0.98667 val_loss: 0.17845, val_acc: 0.97333\n",
      "Epoch [5080/10000], train loss: 0.17446 train acc: 0.98667 val_loss: 0.17830, val_acc: 0.97333\n",
      "Epoch [5090/10000], train loss: 0.17428 train acc: 0.98667 val_loss: 0.17814, val_acc: 0.97333\n",
      "Epoch [5100/10000], train loss: 0.17411 train acc: 0.98667 val_loss: 0.17798, val_acc: 0.97333\n",
      "Epoch [5110/10000], train loss: 0.17393 train acc: 0.98667 val_loss: 0.17783, val_acc: 0.97333\n",
      "Epoch [5120/10000], train loss: 0.17376 train acc: 0.98667 val_loss: 0.17767, val_acc: 0.97333\n",
      "Epoch [5130/10000], train loss: 0.17359 train acc: 0.98667 val_loss: 0.17752, val_acc: 0.97333\n",
      "Epoch [5140/10000], train loss: 0.17342 train acc: 0.98667 val_loss: 0.17736, val_acc: 0.96000\n",
      "Epoch [5150/10000], train loss: 0.17325 train acc: 0.98667 val_loss: 0.17721, val_acc: 0.96000\n",
      "Epoch [5160/10000], train loss: 0.17308 train acc: 0.98667 val_loss: 0.17705, val_acc: 0.96000\n",
      "Epoch [5170/10000], train loss: 0.17291 train acc: 0.98667 val_loss: 0.17690, val_acc: 0.96000\n",
      "Epoch [5180/10000], train loss: 0.17274 train acc: 0.98667 val_loss: 0.17675, val_acc: 0.96000\n",
      "Epoch [5190/10000], train loss: 0.17257 train acc: 0.98667 val_loss: 0.17660, val_acc: 0.96000\n",
      "Epoch [5200/10000], train loss: 0.17240 train acc: 0.98667 val_loss: 0.17645, val_acc: 0.96000\n",
      "Epoch [5210/10000], train loss: 0.17223 train acc: 0.98667 val_loss: 0.17629, val_acc: 0.96000\n",
      "Epoch [5220/10000], train loss: 0.17207 train acc: 0.98667 val_loss: 0.17614, val_acc: 0.96000\n",
      "Epoch [5230/10000], train loss: 0.17190 train acc: 0.98667 val_loss: 0.17599, val_acc: 0.96000\n",
      "Epoch [5240/10000], train loss: 0.17173 train acc: 0.98667 val_loss: 0.17584, val_acc: 0.96000\n",
      "Epoch [5250/10000], train loss: 0.17157 train acc: 0.98667 val_loss: 0.17569, val_acc: 0.96000\n",
      "Epoch [5260/10000], train loss: 0.17140 train acc: 0.98667 val_loss: 0.17555, val_acc: 0.96000\n",
      "Epoch [5270/10000], train loss: 0.17124 train acc: 0.98667 val_loss: 0.17540, val_acc: 0.96000\n",
      "Epoch [5280/10000], train loss: 0.17107 train acc: 0.98667 val_loss: 0.17525, val_acc: 0.96000\n",
      "Epoch [5290/10000], train loss: 0.17091 train acc: 0.98667 val_loss: 0.17510, val_acc: 0.96000\n",
      "Epoch [5300/10000], train loss: 0.17075 train acc: 0.98667 val_loss: 0.17496, val_acc: 0.96000\n",
      "Epoch [5310/10000], train loss: 0.17058 train acc: 0.98667 val_loss: 0.17481, val_acc: 0.96000\n",
      "Epoch [5320/10000], train loss: 0.17042 train acc: 0.98667 val_loss: 0.17466, val_acc: 0.96000\n",
      "Epoch [5330/10000], train loss: 0.17026 train acc: 0.98667 val_loss: 0.17452, val_acc: 0.96000\n",
      "Epoch [5340/10000], train loss: 0.17010 train acc: 0.98667 val_loss: 0.17437, val_acc: 0.96000\n",
      "Epoch [5350/10000], train loss: 0.16994 train acc: 0.98667 val_loss: 0.17423, val_acc: 0.96000\n",
      "Epoch [5360/10000], train loss: 0.16978 train acc: 0.98667 val_loss: 0.17408, val_acc: 0.96000\n",
      "Epoch [5370/10000], train loss: 0.16962 train acc: 0.98667 val_loss: 0.17394, val_acc: 0.96000\n",
      "Epoch [5380/10000], train loss: 0.16946 train acc: 0.98667 val_loss: 0.17380, val_acc: 0.96000\n",
      "Epoch [5390/10000], train loss: 0.16930 train acc: 0.98667 val_loss: 0.17366, val_acc: 0.96000\n",
      "Epoch [5400/10000], train loss: 0.16914 train acc: 0.98667 val_loss: 0.17351, val_acc: 0.96000\n",
      "Epoch [5410/10000], train loss: 0.16898 train acc: 0.98667 val_loss: 0.17337, val_acc: 0.96000\n",
      "Epoch [5420/10000], train loss: 0.16883 train acc: 0.98667 val_loss: 0.17323, val_acc: 0.96000\n",
      "Epoch [5430/10000], train loss: 0.16867 train acc: 0.98667 val_loss: 0.17309, val_acc: 0.96000\n",
      "Epoch [5440/10000], train loss: 0.16851 train acc: 0.98667 val_loss: 0.17295, val_acc: 0.96000\n",
      "Epoch [5450/10000], train loss: 0.16836 train acc: 0.98667 val_loss: 0.17281, val_acc: 0.96000\n",
      "Epoch [5460/10000], train loss: 0.16820 train acc: 0.98667 val_loss: 0.17267, val_acc: 0.96000\n",
      "Epoch [5470/10000], train loss: 0.16805 train acc: 0.98667 val_loss: 0.17253, val_acc: 0.96000\n",
      "Epoch [5480/10000], train loss: 0.16789 train acc: 0.98667 val_loss: 0.17239, val_acc: 0.96000\n",
      "Epoch [5490/10000], train loss: 0.16774 train acc: 0.98667 val_loss: 0.17225, val_acc: 0.96000\n",
      "Epoch [5500/10000], train loss: 0.16758 train acc: 0.98667 val_loss: 0.17211, val_acc: 0.96000\n",
      "Epoch [5510/10000], train loss: 0.16743 train acc: 0.98667 val_loss: 0.17198, val_acc: 0.96000\n",
      "Epoch [5520/10000], train loss: 0.16728 train acc: 0.98667 val_loss: 0.17184, val_acc: 0.96000\n",
      "Epoch [5530/10000], train loss: 0.16713 train acc: 0.98667 val_loss: 0.17170, val_acc: 0.96000\n",
      "Epoch [5540/10000], train loss: 0.16697 train acc: 0.98667 val_loss: 0.17157, val_acc: 0.96000\n",
      "Epoch [5550/10000], train loss: 0.16682 train acc: 0.98667 val_loss: 0.17143, val_acc: 0.96000\n",
      "Epoch [5560/10000], train loss: 0.16667 train acc: 0.98667 val_loss: 0.17130, val_acc: 0.96000\n",
      "Epoch [5570/10000], train loss: 0.16652 train acc: 0.98667 val_loss: 0.17116, val_acc: 0.96000\n",
      "Epoch [5580/10000], train loss: 0.16637 train acc: 0.98667 val_loss: 0.17103, val_acc: 0.96000\n",
      "Epoch [5590/10000], train loss: 0.16622 train acc: 0.98667 val_loss: 0.17089, val_acc: 0.96000\n",
      "Epoch [5600/10000], train loss: 0.16607 train acc: 0.98667 val_loss: 0.17076, val_acc: 0.96000\n",
      "Epoch [5610/10000], train loss: 0.16592 train acc: 0.98667 val_loss: 0.17062, val_acc: 0.96000\n",
      "Epoch [5620/10000], train loss: 0.16577 train acc: 0.98667 val_loss: 0.17049, val_acc: 0.96000\n",
      "Epoch [5630/10000], train loss: 0.16563 train acc: 0.98667 val_loss: 0.17036, val_acc: 0.96000\n",
      "Epoch [5640/10000], train loss: 0.16548 train acc: 0.98667 val_loss: 0.17023, val_acc: 0.96000\n",
      "Epoch [5650/10000], train loss: 0.16533 train acc: 0.98667 val_loss: 0.17009, val_acc: 0.96000\n",
      "Epoch [5660/10000], train loss: 0.16518 train acc: 0.98667 val_loss: 0.16996, val_acc: 0.96000\n",
      "Epoch [5670/10000], train loss: 0.16504 train acc: 0.98667 val_loss: 0.16983, val_acc: 0.96000\n",
      "Epoch [5680/10000], train loss: 0.16489 train acc: 0.98667 val_loss: 0.16970, val_acc: 0.96000\n",
      "Epoch [5690/10000], train loss: 0.16475 train acc: 0.98667 val_loss: 0.16957, val_acc: 0.96000\n",
      "Epoch [5700/10000], train loss: 0.16460 train acc: 0.98667 val_loss: 0.16944, val_acc: 0.96000\n",
      "Epoch [5710/10000], train loss: 0.16446 train acc: 0.98667 val_loss: 0.16931, val_acc: 0.96000\n",
      "Epoch [5720/10000], train loss: 0.16431 train acc: 0.98667 val_loss: 0.16918, val_acc: 0.96000\n",
      "Epoch [5730/10000], train loss: 0.16417 train acc: 0.98667 val_loss: 0.16905, val_acc: 0.96000\n",
      "Epoch [5740/10000], train loss: 0.16403 train acc: 0.98667 val_loss: 0.16893, val_acc: 0.96000\n",
      "Epoch [5750/10000], train loss: 0.16388 train acc: 0.98667 val_loss: 0.16880, val_acc: 0.96000\n",
      "Epoch [5760/10000], train loss: 0.16374 train acc: 0.98667 val_loss: 0.16867, val_acc: 0.96000\n",
      "Epoch [5770/10000], train loss: 0.16360 train acc: 0.98667 val_loss: 0.16854, val_acc: 0.96000\n",
      "Epoch [5780/10000], train loss: 0.16346 train acc: 0.98667 val_loss: 0.16842, val_acc: 0.96000\n",
      "Epoch [5790/10000], train loss: 0.16331 train acc: 0.98667 val_loss: 0.16829, val_acc: 0.96000\n",
      "Epoch [5800/10000], train loss: 0.16317 train acc: 0.98667 val_loss: 0.16816, val_acc: 0.96000\n",
      "Epoch [5810/10000], train loss: 0.16303 train acc: 0.98667 val_loss: 0.16804, val_acc: 0.96000\n",
      "Epoch [5820/10000], train loss: 0.16289 train acc: 0.98667 val_loss: 0.16791, val_acc: 0.96000\n",
      "Epoch [5830/10000], train loss: 0.16275 train acc: 0.98667 val_loss: 0.16779, val_acc: 0.96000\n",
      "Epoch [5840/10000], train loss: 0.16261 train acc: 0.98667 val_loss: 0.16766, val_acc: 0.96000\n",
      "Epoch [5850/10000], train loss: 0.16247 train acc: 0.98667 val_loss: 0.16754, val_acc: 0.96000\n",
      "Epoch [5860/10000], train loss: 0.16234 train acc: 0.98667 val_loss: 0.16741, val_acc: 0.96000\n",
      "Epoch [5870/10000], train loss: 0.16220 train acc: 0.98667 val_loss: 0.16729, val_acc: 0.96000\n",
      "Epoch [5880/10000], train loss: 0.16206 train acc: 0.98667 val_loss: 0.16717, val_acc: 0.96000\n",
      "Epoch [5890/10000], train loss: 0.16192 train acc: 0.98667 val_loss: 0.16704, val_acc: 0.96000\n",
      "Epoch [5900/10000], train loss: 0.16178 train acc: 0.98667 val_loss: 0.16692, val_acc: 0.96000\n",
      "Epoch [5910/10000], train loss: 0.16165 train acc: 0.98667 val_loss: 0.16680, val_acc: 0.96000\n",
      "Epoch [5920/10000], train loss: 0.16151 train acc: 0.98667 val_loss: 0.16668, val_acc: 0.96000\n",
      "Epoch [5930/10000], train loss: 0.16138 train acc: 0.98667 val_loss: 0.16656, val_acc: 0.96000\n",
      "Epoch [5940/10000], train loss: 0.16124 train acc: 0.98667 val_loss: 0.16644, val_acc: 0.96000\n",
      "Epoch [5950/10000], train loss: 0.16110 train acc: 0.98667 val_loss: 0.16631, val_acc: 0.96000\n",
      "Epoch [5960/10000], train loss: 0.16097 train acc: 0.98667 val_loss: 0.16619, val_acc: 0.96000\n",
      "Epoch [5970/10000], train loss: 0.16084 train acc: 0.98667 val_loss: 0.16607, val_acc: 0.96000\n",
      "Epoch [5980/10000], train loss: 0.16070 train acc: 0.98667 val_loss: 0.16595, val_acc: 0.96000\n",
      "Epoch [5990/10000], train loss: 0.16057 train acc: 0.98667 val_loss: 0.16584, val_acc: 0.96000\n",
      "Epoch [6000/10000], train loss: 0.16043 train acc: 0.98667 val_loss: 0.16572, val_acc: 0.96000\n",
      "Epoch [6010/10000], train loss: 0.16030 train acc: 0.98667 val_loss: 0.16560, val_acc: 0.96000\n",
      "Epoch [6020/10000], train loss: 0.16017 train acc: 0.98667 val_loss: 0.16548, val_acc: 0.96000\n",
      "Epoch [6030/10000], train loss: 0.16004 train acc: 0.98667 val_loss: 0.16536, val_acc: 0.96000\n",
      "Epoch [6040/10000], train loss: 0.15990 train acc: 0.98667 val_loss: 0.16524, val_acc: 0.96000\n",
      "Epoch [6050/10000], train loss: 0.15977 train acc: 0.98667 val_loss: 0.16513, val_acc: 0.96000\n",
      "Epoch [6060/10000], train loss: 0.15964 train acc: 0.98667 val_loss: 0.16501, val_acc: 0.96000\n",
      "Epoch [6070/10000], train loss: 0.15951 train acc: 0.98667 val_loss: 0.16489, val_acc: 0.96000\n",
      "Epoch [6080/10000], train loss: 0.15938 train acc: 0.98667 val_loss: 0.16478, val_acc: 0.96000\n",
      "Epoch [6090/10000], train loss: 0.15925 train acc: 0.98667 val_loss: 0.16466, val_acc: 0.96000\n",
      "Epoch [6100/10000], train loss: 0.15912 train acc: 0.98667 val_loss: 0.16454, val_acc: 0.96000\n",
      "Epoch [6110/10000], train loss: 0.15899 train acc: 0.98667 val_loss: 0.16443, val_acc: 0.96000\n",
      "Epoch [6120/10000], train loss: 0.15886 train acc: 0.98667 val_loss: 0.16431, val_acc: 0.96000\n",
      "Epoch [6130/10000], train loss: 0.15873 train acc: 0.98667 val_loss: 0.16420, val_acc: 0.96000\n",
      "Epoch [6140/10000], train loss: 0.15860 train acc: 0.98667 val_loss: 0.16408, val_acc: 0.96000\n",
      "Epoch [6150/10000], train loss: 0.15848 train acc: 0.98667 val_loss: 0.16397, val_acc: 0.96000\n",
      "Epoch [6160/10000], train loss: 0.15835 train acc: 0.98667 val_loss: 0.16386, val_acc: 0.96000\n",
      "Epoch [6170/10000], train loss: 0.15822 train acc: 0.98667 val_loss: 0.16374, val_acc: 0.96000\n",
      "Epoch [6180/10000], train loss: 0.15809 train acc: 0.98667 val_loss: 0.16363, val_acc: 0.96000\n",
      "Epoch [6190/10000], train loss: 0.15797 train acc: 0.98667 val_loss: 0.16352, val_acc: 0.96000\n",
      "Epoch [6200/10000], train loss: 0.15784 train acc: 0.98667 val_loss: 0.16340, val_acc: 0.96000\n",
      "Epoch [6210/10000], train loss: 0.15771 train acc: 0.98667 val_loss: 0.16329, val_acc: 0.96000\n",
      "Epoch [6220/10000], train loss: 0.15759 train acc: 0.98667 val_loss: 0.16318, val_acc: 0.96000\n",
      "Epoch [6230/10000], train loss: 0.15746 train acc: 0.98667 val_loss: 0.16307, val_acc: 0.96000\n",
      "Epoch [6240/10000], train loss: 0.15734 train acc: 0.98667 val_loss: 0.16296, val_acc: 0.96000\n",
      "Epoch [6250/10000], train loss: 0.15721 train acc: 0.98667 val_loss: 0.16285, val_acc: 0.96000\n",
      "Epoch [6260/10000], train loss: 0.15709 train acc: 0.98667 val_loss: 0.16274, val_acc: 0.96000\n",
      "Epoch [6270/10000], train loss: 0.15696 train acc: 0.98667 val_loss: 0.16263, val_acc: 0.96000\n",
      "Epoch [6280/10000], train loss: 0.15684 train acc: 0.98667 val_loss: 0.16252, val_acc: 0.96000\n",
      "Epoch [6290/10000], train loss: 0.15672 train acc: 0.98667 val_loss: 0.16241, val_acc: 0.96000\n",
      "Epoch [6300/10000], train loss: 0.15659 train acc: 0.98667 val_loss: 0.16230, val_acc: 0.96000\n",
      "Epoch [6310/10000], train loss: 0.15647 train acc: 0.98667 val_loss: 0.16219, val_acc: 0.96000\n",
      "Epoch [6320/10000], train loss: 0.15635 train acc: 0.98667 val_loss: 0.16208, val_acc: 0.96000\n",
      "Epoch [6330/10000], train loss: 0.15623 train acc: 0.98667 val_loss: 0.16197, val_acc: 0.96000\n",
      "Epoch [6340/10000], train loss: 0.15610 train acc: 0.98667 val_loss: 0.16186, val_acc: 0.96000\n",
      "Epoch [6350/10000], train loss: 0.15598 train acc: 0.98667 val_loss: 0.16175, val_acc: 0.96000\n",
      "Epoch [6360/10000], train loss: 0.15586 train acc: 0.98667 val_loss: 0.16165, val_acc: 0.96000\n",
      "Epoch [6370/10000], train loss: 0.15574 train acc: 0.98667 val_loss: 0.16154, val_acc: 0.96000\n",
      "Epoch [6380/10000], train loss: 0.15562 train acc: 0.98667 val_loss: 0.16143, val_acc: 0.96000\n",
      "Epoch [6390/10000], train loss: 0.15550 train acc: 0.98667 val_loss: 0.16132, val_acc: 0.96000\n",
      "Epoch [6400/10000], train loss: 0.15538 train acc: 0.98667 val_loss: 0.16122, val_acc: 0.96000\n",
      "Epoch [6410/10000], train loss: 0.15526 train acc: 0.98667 val_loss: 0.16111, val_acc: 0.96000\n",
      "Epoch [6420/10000], train loss: 0.15514 train acc: 0.98667 val_loss: 0.16101, val_acc: 0.96000\n",
      "Epoch [6430/10000], train loss: 0.15502 train acc: 0.98667 val_loss: 0.16090, val_acc: 0.96000\n",
      "Epoch [6440/10000], train loss: 0.15490 train acc: 0.98667 val_loss: 0.16079, val_acc: 0.96000\n",
      "Epoch [6450/10000], train loss: 0.15478 train acc: 0.98667 val_loss: 0.16069, val_acc: 0.96000\n",
      "Epoch [6460/10000], train loss: 0.15467 train acc: 0.98667 val_loss: 0.16058, val_acc: 0.96000\n",
      "Epoch [6470/10000], train loss: 0.15455 train acc: 0.98667 val_loss: 0.16048, val_acc: 0.96000\n",
      "Epoch [6480/10000], train loss: 0.15443 train acc: 0.98667 val_loss: 0.16038, val_acc: 0.96000\n",
      "Epoch [6490/10000], train loss: 0.15431 train acc: 0.98667 val_loss: 0.16027, val_acc: 0.96000\n",
      "Epoch [6500/10000], train loss: 0.15420 train acc: 0.98667 val_loss: 0.16017, val_acc: 0.96000\n",
      "Epoch [6510/10000], train loss: 0.15408 train acc: 0.98667 val_loss: 0.16006, val_acc: 0.96000\n",
      "Epoch [6520/10000], train loss: 0.15396 train acc: 0.98667 val_loss: 0.15996, val_acc: 0.96000\n",
      "Epoch [6530/10000], train loss: 0.15385 train acc: 0.98667 val_loss: 0.15986, val_acc: 0.96000\n",
      "Epoch [6540/10000], train loss: 0.15373 train acc: 0.98667 val_loss: 0.15976, val_acc: 0.96000\n",
      "Epoch [6550/10000], train loss: 0.15362 train acc: 0.98667 val_loss: 0.15965, val_acc: 0.96000\n",
      "Epoch [6560/10000], train loss: 0.15350 train acc: 0.98667 val_loss: 0.15955, val_acc: 0.96000\n",
      "Epoch [6570/10000], train loss: 0.15339 train acc: 0.98667 val_loss: 0.15945, val_acc: 0.96000\n",
      "Epoch [6580/10000], train loss: 0.15327 train acc: 0.98667 val_loss: 0.15935, val_acc: 0.96000\n",
      "Epoch [6590/10000], train loss: 0.15316 train acc: 0.98667 val_loss: 0.15925, val_acc: 0.96000\n",
      "Epoch [6600/10000], train loss: 0.15304 train acc: 0.98667 val_loss: 0.15915, val_acc: 0.96000\n",
      "Epoch [6610/10000], train loss: 0.15293 train acc: 0.98667 val_loss: 0.15904, val_acc: 0.96000\n",
      "Epoch [6620/10000], train loss: 0.15282 train acc: 0.98667 val_loss: 0.15894, val_acc: 0.96000\n",
      "Epoch [6630/10000], train loss: 0.15270 train acc: 0.98667 val_loss: 0.15884, val_acc: 0.96000\n",
      "Epoch [6640/10000], train loss: 0.15259 train acc: 0.98667 val_loss: 0.15874, val_acc: 0.96000\n",
      "Epoch [6650/10000], train loss: 0.15248 train acc: 0.98667 val_loss: 0.15864, val_acc: 0.96000\n",
      "Epoch [6660/10000], train loss: 0.15236 train acc: 0.98667 val_loss: 0.15854, val_acc: 0.96000\n",
      "Epoch [6670/10000], train loss: 0.15225 train acc: 0.98667 val_loss: 0.15845, val_acc: 0.96000\n",
      "Epoch [6680/10000], train loss: 0.15214 train acc: 0.98667 val_loss: 0.15835, val_acc: 0.96000\n",
      "Epoch [6690/10000], train loss: 0.15203 train acc: 0.98667 val_loss: 0.15825, val_acc: 0.96000\n",
      "Epoch [6700/10000], train loss: 0.15192 train acc: 0.98667 val_loss: 0.15815, val_acc: 0.96000\n",
      "Epoch [6710/10000], train loss: 0.15181 train acc: 0.98667 val_loss: 0.15805, val_acc: 0.96000\n",
      "Epoch [6720/10000], train loss: 0.15170 train acc: 0.98667 val_loss: 0.15795, val_acc: 0.96000\n",
      "Epoch [6730/10000], train loss: 0.15159 train acc: 0.98667 val_loss: 0.15786, val_acc: 0.96000\n",
      "Epoch [6740/10000], train loss: 0.15148 train acc: 0.98667 val_loss: 0.15776, val_acc: 0.96000\n",
      "Epoch [6750/10000], train loss: 0.15137 train acc: 0.98667 val_loss: 0.15766, val_acc: 0.96000\n",
      "Epoch [6760/10000], train loss: 0.15126 train acc: 0.98667 val_loss: 0.15756, val_acc: 0.96000\n",
      "Epoch [6770/10000], train loss: 0.15115 train acc: 0.98667 val_loss: 0.15747, val_acc: 0.96000\n",
      "Epoch [6780/10000], train loss: 0.15104 train acc: 0.98667 val_loss: 0.15737, val_acc: 0.96000\n",
      "Epoch [6790/10000], train loss: 0.15093 train acc: 0.98667 val_loss: 0.15727, val_acc: 0.96000\n",
      "Epoch [6800/10000], train loss: 0.15082 train acc: 0.98667 val_loss: 0.15718, val_acc: 0.96000\n",
      "Epoch [6810/10000], train loss: 0.15071 train acc: 0.98667 val_loss: 0.15708, val_acc: 0.96000\n",
      "Epoch [6820/10000], train loss: 0.15060 train acc: 0.98667 val_loss: 0.15699, val_acc: 0.96000\n",
      "Epoch [6830/10000], train loss: 0.15050 train acc: 0.98667 val_loss: 0.15689, val_acc: 0.96000\n",
      "Epoch [6840/10000], train loss: 0.15039 train acc: 0.98667 val_loss: 0.15680, val_acc: 0.96000\n",
      "Epoch [6850/10000], train loss: 0.15028 train acc: 0.98667 val_loss: 0.15670, val_acc: 0.96000\n",
      "Epoch [6860/10000], train loss: 0.15017 train acc: 0.98667 val_loss: 0.15661, val_acc: 0.96000\n",
      "Epoch [6870/10000], train loss: 0.15007 train acc: 0.98667 val_loss: 0.15651, val_acc: 0.96000\n",
      "Epoch [6880/10000], train loss: 0.14996 train acc: 0.98667 val_loss: 0.15642, val_acc: 0.96000\n",
      "Epoch [6890/10000], train loss: 0.14985 train acc: 0.98667 val_loss: 0.15633, val_acc: 0.96000\n",
      "Epoch [6900/10000], train loss: 0.14975 train acc: 0.98667 val_loss: 0.15623, val_acc: 0.96000\n",
      "Epoch [6910/10000], train loss: 0.14964 train acc: 0.98667 val_loss: 0.15614, val_acc: 0.96000\n",
      "Epoch [6920/10000], train loss: 0.14954 train acc: 0.98667 val_loss: 0.15605, val_acc: 0.96000\n",
      "Epoch [6930/10000], train loss: 0.14943 train acc: 0.98667 val_loss: 0.15595, val_acc: 0.96000\n",
      "Epoch [6940/10000], train loss: 0.14933 train acc: 0.98667 val_loss: 0.15586, val_acc: 0.96000\n",
      "Epoch [6950/10000], train loss: 0.14922 train acc: 0.98667 val_loss: 0.15577, val_acc: 0.96000\n",
      "Epoch [6960/10000], train loss: 0.14912 train acc: 0.98667 val_loss: 0.15568, val_acc: 0.96000\n",
      "Epoch [6970/10000], train loss: 0.14901 train acc: 0.98667 val_loss: 0.15558, val_acc: 0.96000\n",
      "Epoch [6980/10000], train loss: 0.14891 train acc: 0.98667 val_loss: 0.15549, val_acc: 0.96000\n",
      "Epoch [6990/10000], train loss: 0.14881 train acc: 0.98667 val_loss: 0.15540, val_acc: 0.96000\n",
      "Epoch [7000/10000], train loss: 0.14870 train acc: 0.98667 val_loss: 0.15531, val_acc: 0.96000\n",
      "Epoch [7010/10000], train loss: 0.14860 train acc: 0.98667 val_loss: 0.15522, val_acc: 0.96000\n",
      "Epoch [7020/10000], train loss: 0.14850 train acc: 0.98667 val_loss: 0.15513, val_acc: 0.96000\n",
      "Epoch [7030/10000], train loss: 0.14839 train acc: 0.98667 val_loss: 0.15504, val_acc: 0.96000\n",
      "Epoch [7040/10000], train loss: 0.14829 train acc: 0.98667 val_loss: 0.15495, val_acc: 0.96000\n",
      "Epoch [7050/10000], train loss: 0.14819 train acc: 0.98667 val_loss: 0.15486, val_acc: 0.96000\n",
      "Epoch [7060/10000], train loss: 0.14809 train acc: 0.98667 val_loss: 0.15477, val_acc: 0.96000\n",
      "Epoch [7070/10000], train loss: 0.14798 train acc: 0.98667 val_loss: 0.15468, val_acc: 0.96000\n",
      "Epoch [7080/10000], train loss: 0.14788 train acc: 0.98667 val_loss: 0.15459, val_acc: 0.96000\n",
      "Epoch [7090/10000], train loss: 0.14778 train acc: 0.98667 val_loss: 0.15450, val_acc: 0.96000\n",
      "Epoch [7100/10000], train loss: 0.14768 train acc: 0.98667 val_loss: 0.15441, val_acc: 0.96000\n",
      "Epoch [7110/10000], train loss: 0.14758 train acc: 0.98667 val_loss: 0.15432, val_acc: 0.96000\n",
      "Epoch [7120/10000], train loss: 0.14748 train acc: 0.98667 val_loss: 0.15423, val_acc: 0.96000\n",
      "Epoch [7130/10000], train loss: 0.14738 train acc: 0.98667 val_loss: 0.15414, val_acc: 0.96000\n",
      "Epoch [7140/10000], train loss: 0.14728 train acc: 0.98667 val_loss: 0.15406, val_acc: 0.96000\n",
      "Epoch [7150/10000], train loss: 0.14718 train acc: 0.98667 val_loss: 0.15397, val_acc: 0.96000\n",
      "Epoch [7160/10000], train loss: 0.14708 train acc: 0.98667 val_loss: 0.15388, val_acc: 0.96000\n",
      "Epoch [7170/10000], train loss: 0.14698 train acc: 0.98667 val_loss: 0.15379, val_acc: 0.96000\n",
      "Epoch [7180/10000], train loss: 0.14688 train acc: 0.98667 val_loss: 0.15371, val_acc: 0.96000\n",
      "Epoch [7190/10000], train loss: 0.14678 train acc: 0.98667 val_loss: 0.15362, val_acc: 0.96000\n",
      "Epoch [7200/10000], train loss: 0.14668 train acc: 0.98667 val_loss: 0.15353, val_acc: 0.96000\n",
      "Epoch [7210/10000], train loss: 0.14658 train acc: 0.98667 val_loss: 0.15345, val_acc: 0.96000\n",
      "Epoch [7220/10000], train loss: 0.14649 train acc: 0.98667 val_loss: 0.15336, val_acc: 0.96000\n",
      "Epoch [7230/10000], train loss: 0.14639 train acc: 0.98667 val_loss: 0.15327, val_acc: 0.96000\n",
      "Epoch [7240/10000], train loss: 0.14629 train acc: 0.98667 val_loss: 0.15319, val_acc: 0.96000\n",
      "Epoch [7250/10000], train loss: 0.14619 train acc: 0.98667 val_loss: 0.15310, val_acc: 0.96000\n",
      "Epoch [7260/10000], train loss: 0.14610 train acc: 0.98667 val_loss: 0.15301, val_acc: 0.96000\n",
      "Epoch [7270/10000], train loss: 0.14600 train acc: 0.98667 val_loss: 0.15293, val_acc: 0.96000\n",
      "Epoch [7280/10000], train loss: 0.14590 train acc: 0.98667 val_loss: 0.15284, val_acc: 0.96000\n",
      "Epoch [7290/10000], train loss: 0.14580 train acc: 0.98667 val_loss: 0.15276, val_acc: 0.96000\n",
      "Epoch [7300/10000], train loss: 0.14571 train acc: 0.98667 val_loss: 0.15267, val_acc: 0.96000\n",
      "Epoch [7310/10000], train loss: 0.14561 train acc: 0.98667 val_loss: 0.15259, val_acc: 0.96000\n",
      "Epoch [7320/10000], train loss: 0.14552 train acc: 0.98667 val_loss: 0.15251, val_acc: 0.96000\n",
      "Epoch [7330/10000], train loss: 0.14542 train acc: 0.98667 val_loss: 0.15242, val_acc: 0.96000\n",
      "Epoch [7340/10000], train loss: 0.14532 train acc: 0.98667 val_loss: 0.15234, val_acc: 0.96000\n",
      "Epoch [7350/10000], train loss: 0.14523 train acc: 0.98667 val_loss: 0.15225, val_acc: 0.96000\n",
      "Epoch [7360/10000], train loss: 0.14513 train acc: 0.98667 val_loss: 0.15217, val_acc: 0.96000\n",
      "Epoch [7370/10000], train loss: 0.14504 train acc: 0.98667 val_loss: 0.15209, val_acc: 0.96000\n",
      "Epoch [7380/10000], train loss: 0.14494 train acc: 0.98667 val_loss: 0.15200, val_acc: 0.96000\n",
      "Epoch [7390/10000], train loss: 0.14485 train acc: 0.98667 val_loss: 0.15192, val_acc: 0.96000\n",
      "Epoch [7400/10000], train loss: 0.14475 train acc: 0.98667 val_loss: 0.15184, val_acc: 0.96000\n",
      "Epoch [7410/10000], train loss: 0.14466 train acc: 0.98667 val_loss: 0.15176, val_acc: 0.96000\n",
      "Epoch [7420/10000], train loss: 0.14457 train acc: 0.98667 val_loss: 0.15167, val_acc: 0.96000\n",
      "Epoch [7430/10000], train loss: 0.14447 train acc: 0.98667 val_loss: 0.15159, val_acc: 0.96000\n",
      "Epoch [7440/10000], train loss: 0.14438 train acc: 0.98667 val_loss: 0.15151, val_acc: 0.96000\n",
      "Epoch [7450/10000], train loss: 0.14429 train acc: 0.98667 val_loss: 0.15143, val_acc: 0.96000\n",
      "Epoch [7460/10000], train loss: 0.14419 train acc: 0.98667 val_loss: 0.15135, val_acc: 0.96000\n",
      "Epoch [7470/10000], train loss: 0.14410 train acc: 0.98667 val_loss: 0.15126, val_acc: 0.96000\n",
      "Epoch [7480/10000], train loss: 0.14401 train acc: 0.98667 val_loss: 0.15118, val_acc: 0.96000\n",
      "Epoch [7490/10000], train loss: 0.14391 train acc: 0.98667 val_loss: 0.15110, val_acc: 0.96000\n",
      "Epoch [7500/10000], train loss: 0.14382 train acc: 0.98667 val_loss: 0.15102, val_acc: 0.96000\n",
      "Epoch [7510/10000], train loss: 0.14373 train acc: 0.98667 val_loss: 0.15094, val_acc: 0.96000\n",
      "Epoch [7520/10000], train loss: 0.14364 train acc: 0.98667 val_loss: 0.15086, val_acc: 0.96000\n",
      "Epoch [7530/10000], train loss: 0.14355 train acc: 0.98667 val_loss: 0.15078, val_acc: 0.96000\n",
      "Epoch [7540/10000], train loss: 0.14346 train acc: 0.98667 val_loss: 0.15070, val_acc: 0.96000\n",
      "Epoch [7550/10000], train loss: 0.14336 train acc: 0.98667 val_loss: 0.15062, val_acc: 0.96000\n",
      "Epoch [7560/10000], train loss: 0.14327 train acc: 0.98667 val_loss: 0.15054, val_acc: 0.96000\n",
      "Epoch [7570/10000], train loss: 0.14318 train acc: 0.98667 val_loss: 0.15046, val_acc: 0.96000\n",
      "Epoch [7580/10000], train loss: 0.14309 train acc: 0.98667 val_loss: 0.15038, val_acc: 0.96000\n",
      "Epoch [7590/10000], train loss: 0.14300 train acc: 0.98667 val_loss: 0.15030, val_acc: 0.96000\n",
      "Epoch [7600/10000], train loss: 0.14291 train acc: 0.98667 val_loss: 0.15022, val_acc: 0.96000\n",
      "Epoch [7610/10000], train loss: 0.14282 train acc: 0.98667 val_loss: 0.15014, val_acc: 0.96000\n",
      "Epoch [7620/10000], train loss: 0.14273 train acc: 0.98667 val_loss: 0.15007, val_acc: 0.96000\n",
      "Epoch [7630/10000], train loss: 0.14264 train acc: 0.98667 val_loss: 0.14999, val_acc: 0.96000\n",
      "Epoch [7640/10000], train loss: 0.14255 train acc: 0.98667 val_loss: 0.14991, val_acc: 0.96000\n",
      "Epoch [7650/10000], train loss: 0.14246 train acc: 0.98667 val_loss: 0.14983, val_acc: 0.96000\n",
      "Epoch [7660/10000], train loss: 0.14237 train acc: 0.98667 val_loss: 0.14975, val_acc: 0.96000\n",
      "Epoch [7670/10000], train loss: 0.14228 train acc: 0.98667 val_loss: 0.14968, val_acc: 0.96000\n",
      "Epoch [7680/10000], train loss: 0.14220 train acc: 0.98667 val_loss: 0.14960, val_acc: 0.96000\n",
      "Epoch [7690/10000], train loss: 0.14211 train acc: 0.98667 val_loss: 0.14952, val_acc: 0.96000\n",
      "Epoch [7700/10000], train loss: 0.14202 train acc: 0.98667 val_loss: 0.14944, val_acc: 0.96000\n",
      "Epoch [7710/10000], train loss: 0.14193 train acc: 0.98667 val_loss: 0.14937, val_acc: 0.96000\n",
      "Epoch [7720/10000], train loss: 0.14184 train acc: 0.98667 val_loss: 0.14929, val_acc: 0.96000\n",
      "Epoch [7730/10000], train loss: 0.14176 train acc: 0.98667 val_loss: 0.14921, val_acc: 0.96000\n",
      "Epoch [7740/10000], train loss: 0.14167 train acc: 0.98667 val_loss: 0.14914, val_acc: 0.96000\n",
      "Epoch [7750/10000], train loss: 0.14158 train acc: 0.98667 val_loss: 0.14906, val_acc: 0.96000\n",
      "Epoch [7760/10000], train loss: 0.14149 train acc: 0.98667 val_loss: 0.14898, val_acc: 0.96000\n",
      "Epoch [7770/10000], train loss: 0.14141 train acc: 0.98667 val_loss: 0.14891, val_acc: 0.96000\n",
      "Epoch [7780/10000], train loss: 0.14132 train acc: 0.98667 val_loss: 0.14883, val_acc: 0.96000\n",
      "Epoch [7790/10000], train loss: 0.14123 train acc: 0.98667 val_loss: 0.14876, val_acc: 0.96000\n",
      "Epoch [7800/10000], train loss: 0.14115 train acc: 0.98667 val_loss: 0.14868, val_acc: 0.96000\n",
      "Epoch [7810/10000], train loss: 0.14106 train acc: 0.98667 val_loss: 0.14861, val_acc: 0.96000\n",
      "Epoch [7820/10000], train loss: 0.14097 train acc: 0.98667 val_loss: 0.14853, val_acc: 0.96000\n",
      "Epoch [7830/10000], train loss: 0.14089 train acc: 0.98667 val_loss: 0.14846, val_acc: 0.96000\n",
      "Epoch [7840/10000], train loss: 0.14080 train acc: 0.98667 val_loss: 0.14838, val_acc: 0.96000\n",
      "Epoch [7850/10000], train loss: 0.14072 train acc: 0.98667 val_loss: 0.14831, val_acc: 0.96000\n",
      "Epoch [7860/10000], train loss: 0.14063 train acc: 0.98667 val_loss: 0.14823, val_acc: 0.96000\n",
      "Epoch [7870/10000], train loss: 0.14055 train acc: 0.98667 val_loss: 0.14816, val_acc: 0.96000\n",
      "Epoch [7880/10000], train loss: 0.14046 train acc: 0.98667 val_loss: 0.14808, val_acc: 0.96000\n",
      "Epoch [7890/10000], train loss: 0.14038 train acc: 0.98667 val_loss: 0.14801, val_acc: 0.96000\n",
      "Epoch [7900/10000], train loss: 0.14029 train acc: 0.98667 val_loss: 0.14794, val_acc: 0.96000\n",
      "Epoch [7910/10000], train loss: 0.14021 train acc: 0.98667 val_loss: 0.14786, val_acc: 0.96000\n",
      "Epoch [7920/10000], train loss: 0.14012 train acc: 0.98667 val_loss: 0.14779, val_acc: 0.96000\n",
      "Epoch [7930/10000], train loss: 0.14004 train acc: 0.98667 val_loss: 0.14772, val_acc: 0.96000\n",
      "Epoch [7940/10000], train loss: 0.13996 train acc: 0.98667 val_loss: 0.14764, val_acc: 0.96000\n",
      "Epoch [7950/10000], train loss: 0.13987 train acc: 0.98667 val_loss: 0.14757, val_acc: 0.96000\n",
      "Epoch [7960/10000], train loss: 0.13979 train acc: 0.98667 val_loss: 0.14750, val_acc: 0.96000\n",
      "Epoch [7970/10000], train loss: 0.13971 train acc: 0.98667 val_loss: 0.14742, val_acc: 0.96000\n",
      "Epoch [7980/10000], train loss: 0.13962 train acc: 0.98667 val_loss: 0.14735, val_acc: 0.96000\n",
      "Epoch [7990/10000], train loss: 0.13954 train acc: 0.98667 val_loss: 0.14728, val_acc: 0.96000\n",
      "Epoch [8000/10000], train loss: 0.13946 train acc: 0.98667 val_loss: 0.14721, val_acc: 0.96000\n",
      "Epoch [8010/10000], train loss: 0.13937 train acc: 0.98667 val_loss: 0.14713, val_acc: 0.96000\n",
      "Epoch [8020/10000], train loss: 0.13929 train acc: 0.98667 val_loss: 0.14706, val_acc: 0.96000\n",
      "Epoch [8030/10000], train loss: 0.13921 train acc: 0.98667 val_loss: 0.14699, val_acc: 0.96000\n",
      "Epoch [8040/10000], train loss: 0.13913 train acc: 0.98667 val_loss: 0.14692, val_acc: 0.96000\n",
      "Epoch [8050/10000], train loss: 0.13905 train acc: 0.98667 val_loss: 0.14685, val_acc: 0.96000\n",
      "Epoch [8060/10000], train loss: 0.13896 train acc: 0.98667 val_loss: 0.14678, val_acc: 0.96000\n",
      "Epoch [8070/10000], train loss: 0.13888 train acc: 0.98667 val_loss: 0.14671, val_acc: 0.96000\n",
      "Epoch [8080/10000], train loss: 0.13880 train acc: 0.98667 val_loss: 0.14664, val_acc: 0.94667\n",
      "Epoch [8090/10000], train loss: 0.13872 train acc: 0.98667 val_loss: 0.14656, val_acc: 0.94667\n",
      "Epoch [8100/10000], train loss: 0.13864 train acc: 0.98667 val_loss: 0.14649, val_acc: 0.94667\n",
      "Epoch [8110/10000], train loss: 0.13856 train acc: 0.98667 val_loss: 0.14642, val_acc: 0.94667\n",
      "Epoch [8120/10000], train loss: 0.13848 train acc: 0.98667 val_loss: 0.14635, val_acc: 0.94667\n",
      "Epoch [8130/10000], train loss: 0.13840 train acc: 0.98667 val_loss: 0.14628, val_acc: 0.94667\n",
      "Epoch [8140/10000], train loss: 0.13831 train acc: 0.98667 val_loss: 0.14621, val_acc: 0.94667\n",
      "Epoch [8150/10000], train loss: 0.13823 train acc: 0.98667 val_loss: 0.14614, val_acc: 0.94667\n",
      "Epoch [8160/10000], train loss: 0.13815 train acc: 0.98667 val_loss: 0.14607, val_acc: 0.94667\n",
      "Epoch [8170/10000], train loss: 0.13807 train acc: 0.98667 val_loss: 0.14600, val_acc: 0.94667\n",
      "Epoch [8180/10000], train loss: 0.13799 train acc: 0.98667 val_loss: 0.14594, val_acc: 0.94667\n",
      "Epoch [8190/10000], train loss: 0.13792 train acc: 0.98667 val_loss: 0.14587, val_acc: 0.94667\n",
      "Epoch [8200/10000], train loss: 0.13784 train acc: 0.98667 val_loss: 0.14580, val_acc: 0.94667\n",
      "Epoch [8210/10000], train loss: 0.13776 train acc: 0.98667 val_loss: 0.14573, val_acc: 0.94667\n",
      "Epoch [8220/10000], train loss: 0.13768 train acc: 0.98667 val_loss: 0.14566, val_acc: 0.94667\n",
      "Epoch [8230/10000], train loss: 0.13760 train acc: 0.98667 val_loss: 0.14559, val_acc: 0.94667\n",
      "Epoch [8240/10000], train loss: 0.13752 train acc: 0.98667 val_loss: 0.14552, val_acc: 0.94667\n",
      "Epoch [8250/10000], train loss: 0.13744 train acc: 0.98667 val_loss: 0.14545, val_acc: 0.94667\n",
      "Epoch [8260/10000], train loss: 0.13736 train acc: 0.98667 val_loss: 0.14539, val_acc: 0.94667\n",
      "Epoch [8270/10000], train loss: 0.13728 train acc: 0.98667 val_loss: 0.14532, val_acc: 0.94667\n",
      "Epoch [8280/10000], train loss: 0.13721 train acc: 0.98667 val_loss: 0.14525, val_acc: 0.94667\n",
      "Epoch [8290/10000], train loss: 0.13713 train acc: 0.98667 val_loss: 0.14518, val_acc: 0.94667\n",
      "Epoch [8300/10000], train loss: 0.13705 train acc: 0.98667 val_loss: 0.14511, val_acc: 0.94667\n",
      "Epoch [8310/10000], train loss: 0.13697 train acc: 0.98667 val_loss: 0.14505, val_acc: 0.94667\n",
      "Epoch [8320/10000], train loss: 0.13689 train acc: 0.98667 val_loss: 0.14498, val_acc: 0.94667\n",
      "Epoch [8330/10000], train loss: 0.13682 train acc: 0.98667 val_loss: 0.14491, val_acc: 0.94667\n",
      "Epoch [8340/10000], train loss: 0.13674 train acc: 0.98667 val_loss: 0.14485, val_acc: 0.94667\n",
      "Epoch [8350/10000], train loss: 0.13666 train acc: 0.98667 val_loss: 0.14478, val_acc: 0.94667\n",
      "Epoch [8360/10000], train loss: 0.13659 train acc: 0.98667 val_loss: 0.14471, val_acc: 0.94667\n",
      "Epoch [8370/10000], train loss: 0.13651 train acc: 0.98667 val_loss: 0.14465, val_acc: 0.94667\n",
      "Epoch [8380/10000], train loss: 0.13643 train acc: 0.98667 val_loss: 0.14458, val_acc: 0.94667\n",
      "Epoch [8390/10000], train loss: 0.13636 train acc: 0.98667 val_loss: 0.14451, val_acc: 0.94667\n",
      "Epoch [8400/10000], train loss: 0.13628 train acc: 0.98667 val_loss: 0.14445, val_acc: 0.94667\n",
      "Epoch [8410/10000], train loss: 0.13620 train acc: 0.98667 val_loss: 0.14438, val_acc: 0.94667\n",
      "Epoch [8420/10000], train loss: 0.13613 train acc: 0.98667 val_loss: 0.14431, val_acc: 0.94667\n",
      "Epoch [8430/10000], train loss: 0.13605 train acc: 0.98667 val_loss: 0.14425, val_acc: 0.94667\n",
      "Epoch [8440/10000], train loss: 0.13598 train acc: 0.98667 val_loss: 0.14418, val_acc: 0.94667\n",
      "Epoch [8450/10000], train loss: 0.13590 train acc: 0.98667 val_loss: 0.14412, val_acc: 0.94667\n",
      "Epoch [8460/10000], train loss: 0.13582 train acc: 0.98667 val_loss: 0.14405, val_acc: 0.94667\n",
      "Epoch [8470/10000], train loss: 0.13575 train acc: 0.98667 val_loss: 0.14399, val_acc: 0.94667\n",
      "Epoch [8480/10000], train loss: 0.13567 train acc: 0.98667 val_loss: 0.14392, val_acc: 0.94667\n",
      "Epoch [8490/10000], train loss: 0.13560 train acc: 0.98667 val_loss: 0.14386, val_acc: 0.94667\n",
      "Epoch [8500/10000], train loss: 0.13552 train acc: 0.98667 val_loss: 0.14379, val_acc: 0.94667\n",
      "Epoch [8510/10000], train loss: 0.13545 train acc: 0.98667 val_loss: 0.14373, val_acc: 0.94667\n",
      "Epoch [8520/10000], train loss: 0.13538 train acc: 0.98667 val_loss: 0.14366, val_acc: 0.94667\n",
      "Epoch [8530/10000], train loss: 0.13530 train acc: 0.98667 val_loss: 0.14360, val_acc: 0.94667\n",
      "Epoch [8540/10000], train loss: 0.13523 train acc: 0.98667 val_loss: 0.14354, val_acc: 0.94667\n",
      "Epoch [8550/10000], train loss: 0.13515 train acc: 0.98667 val_loss: 0.14347, val_acc: 0.94667\n",
      "Epoch [8560/10000], train loss: 0.13508 train acc: 0.98667 val_loss: 0.14341, val_acc: 0.94667\n",
      "Epoch [8570/10000], train loss: 0.13500 train acc: 0.98667 val_loss: 0.14334, val_acc: 0.94667\n",
      "Epoch [8580/10000], train loss: 0.13493 train acc: 0.98667 val_loss: 0.14328, val_acc: 0.94667\n",
      "Epoch [8590/10000], train loss: 0.13486 train acc: 0.98667 val_loss: 0.14322, val_acc: 0.94667\n",
      "Epoch [8600/10000], train loss: 0.13478 train acc: 0.98667 val_loss: 0.14315, val_acc: 0.94667\n",
      "Epoch [8610/10000], train loss: 0.13471 train acc: 0.98667 val_loss: 0.14309, val_acc: 0.94667\n",
      "Epoch [8620/10000], train loss: 0.13464 train acc: 0.98667 val_loss: 0.14303, val_acc: 0.94667\n",
      "Epoch [8630/10000], train loss: 0.13457 train acc: 0.98667 val_loss: 0.14296, val_acc: 0.94667\n",
      "Epoch [8640/10000], train loss: 0.13449 train acc: 0.98667 val_loss: 0.14290, val_acc: 0.94667\n",
      "Epoch [8650/10000], train loss: 0.13442 train acc: 0.98667 val_loss: 0.14284, val_acc: 0.94667\n",
      "Epoch [8660/10000], train loss: 0.13435 train acc: 0.98667 val_loss: 0.14278, val_acc: 0.94667\n",
      "Epoch [8670/10000], train loss: 0.13427 train acc: 0.98667 val_loss: 0.14271, val_acc: 0.94667\n",
      "Epoch [8680/10000], train loss: 0.13420 train acc: 0.98667 val_loss: 0.14265, val_acc: 0.94667\n",
      "Epoch [8690/10000], train loss: 0.13413 train acc: 0.98667 val_loss: 0.14259, val_acc: 0.94667\n",
      "Epoch [8700/10000], train loss: 0.13406 train acc: 0.98667 val_loss: 0.14253, val_acc: 0.94667\n",
      "Epoch [8710/10000], train loss: 0.13399 train acc: 0.98667 val_loss: 0.14246, val_acc: 0.94667\n",
      "Epoch [8720/10000], train loss: 0.13392 train acc: 0.98667 val_loss: 0.14240, val_acc: 0.94667\n",
      "Epoch [8730/10000], train loss: 0.13384 train acc: 0.98667 val_loss: 0.14234, val_acc: 0.94667\n",
      "Epoch [8740/10000], train loss: 0.13377 train acc: 0.98667 val_loss: 0.14228, val_acc: 0.94667\n",
      "Epoch [8750/10000], train loss: 0.13370 train acc: 0.98667 val_loss: 0.14222, val_acc: 0.94667\n",
      "Epoch [8760/10000], train loss: 0.13363 train acc: 0.98667 val_loss: 0.14216, val_acc: 0.94667\n",
      "Epoch [8770/10000], train loss: 0.13356 train acc: 0.98667 val_loss: 0.14209, val_acc: 0.94667\n",
      "Epoch [8780/10000], train loss: 0.13349 train acc: 0.98667 val_loss: 0.14203, val_acc: 0.94667\n",
      "Epoch [8790/10000], train loss: 0.13342 train acc: 0.98667 val_loss: 0.14197, val_acc: 0.94667\n",
      "Epoch [8800/10000], train loss: 0.13335 train acc: 0.98667 val_loss: 0.14191, val_acc: 0.94667\n",
      "Epoch [8810/10000], train loss: 0.13328 train acc: 0.98667 val_loss: 0.14185, val_acc: 0.94667\n",
      "Epoch [8820/10000], train loss: 0.13321 train acc: 0.98667 val_loss: 0.14179, val_acc: 0.94667\n",
      "Epoch [8830/10000], train loss: 0.13314 train acc: 0.98667 val_loss: 0.14173, val_acc: 0.94667\n",
      "Epoch [8840/10000], train loss: 0.13307 train acc: 0.98667 val_loss: 0.14167, val_acc: 0.94667\n",
      "Epoch [8850/10000], train loss: 0.13300 train acc: 0.98667 val_loss: 0.14161, val_acc: 0.94667\n",
      "Epoch [8860/10000], train loss: 0.13293 train acc: 0.98667 val_loss: 0.14155, val_acc: 0.94667\n",
      "Epoch [8870/10000], train loss: 0.13286 train acc: 0.98667 val_loss: 0.14149, val_acc: 0.94667\n",
      "Epoch [8880/10000], train loss: 0.13279 train acc: 0.98667 val_loss: 0.14143, val_acc: 0.94667\n",
      "Epoch [8890/10000], train loss: 0.13272 train acc: 0.98667 val_loss: 0.14137, val_acc: 0.94667\n",
      "Epoch [8900/10000], train loss: 0.13265 train acc: 0.98667 val_loss: 0.14131, val_acc: 0.94667\n",
      "Epoch [8910/10000], train loss: 0.13258 train acc: 0.98667 val_loss: 0.14125, val_acc: 0.94667\n",
      "Epoch [8920/10000], train loss: 0.13251 train acc: 0.98667 val_loss: 0.14119, val_acc: 0.94667\n",
      "Epoch [8930/10000], train loss: 0.13244 train acc: 0.98667 val_loss: 0.14113, val_acc: 0.94667\n",
      "Epoch [8940/10000], train loss: 0.13237 train acc: 0.98667 val_loss: 0.14107, val_acc: 0.94667\n",
      "Epoch [8950/10000], train loss: 0.13230 train acc: 0.98667 val_loss: 0.14101, val_acc: 0.94667\n",
      "Epoch [8960/10000], train loss: 0.13224 train acc: 0.98667 val_loss: 0.14095, val_acc: 0.94667\n",
      "Epoch [8970/10000], train loss: 0.13217 train acc: 0.98667 val_loss: 0.14090, val_acc: 0.94667\n",
      "Epoch [8980/10000], train loss: 0.13210 train acc: 0.98667 val_loss: 0.14084, val_acc: 0.94667\n",
      "Epoch [8990/10000], train loss: 0.13203 train acc: 0.98667 val_loss: 0.14078, val_acc: 0.94667\n",
      "Epoch [9000/10000], train loss: 0.13196 train acc: 0.98667 val_loss: 0.14072, val_acc: 0.94667\n",
      "Epoch [9010/10000], train loss: 0.13190 train acc: 0.98667 val_loss: 0.14066, val_acc: 0.94667\n",
      "Epoch [9020/10000], train loss: 0.13183 train acc: 0.98667 val_loss: 0.14060, val_acc: 0.94667\n",
      "Epoch [9030/10000], train loss: 0.13176 train acc: 0.98667 val_loss: 0.14054, val_acc: 0.94667\n",
      "Epoch [9040/10000], train loss: 0.13169 train acc: 0.98667 val_loss: 0.14049, val_acc: 0.94667\n",
      "Epoch [9050/10000], train loss: 0.13163 train acc: 0.98667 val_loss: 0.14043, val_acc: 0.94667\n",
      "Epoch [9060/10000], train loss: 0.13156 train acc: 0.98667 val_loss: 0.14037, val_acc: 0.94667\n",
      "Epoch [9070/10000], train loss: 0.13149 train acc: 0.98667 val_loss: 0.14031, val_acc: 0.94667\n",
      "Epoch [9080/10000], train loss: 0.13142 train acc: 0.98667 val_loss: 0.14026, val_acc: 0.94667\n",
      "Epoch [9090/10000], train loss: 0.13136 train acc: 0.98667 val_loss: 0.14020, val_acc: 0.94667\n",
      "Epoch [9100/10000], train loss: 0.13129 train acc: 0.98667 val_loss: 0.14014, val_acc: 0.94667\n",
      "Epoch [9110/10000], train loss: 0.13122 train acc: 0.98667 val_loss: 0.14008, val_acc: 0.94667\n",
      "Epoch [9120/10000], train loss: 0.13116 train acc: 0.98667 val_loss: 0.14003, val_acc: 0.94667\n",
      "Epoch [9130/10000], train loss: 0.13109 train acc: 0.98667 val_loss: 0.13997, val_acc: 0.94667\n",
      "Epoch [9140/10000], train loss: 0.13102 train acc: 0.98667 val_loss: 0.13991, val_acc: 0.94667\n",
      "Epoch [9150/10000], train loss: 0.13096 train acc: 0.98667 val_loss: 0.13986, val_acc: 0.94667\n",
      "Epoch [9160/10000], train loss: 0.13089 train acc: 0.98667 val_loss: 0.13980, val_acc: 0.94667\n",
      "Epoch [9170/10000], train loss: 0.13083 train acc: 0.98667 val_loss: 0.13974, val_acc: 0.94667\n",
      "Epoch [9180/10000], train loss: 0.13076 train acc: 0.98667 val_loss: 0.13969, val_acc: 0.94667\n",
      "Epoch [9190/10000], train loss: 0.13070 train acc: 0.98667 val_loss: 0.13963, val_acc: 0.94667\n",
      "Epoch [9200/10000], train loss: 0.13063 train acc: 0.98667 val_loss: 0.13957, val_acc: 0.94667\n",
      "Epoch [9210/10000], train loss: 0.13056 train acc: 0.98667 val_loss: 0.13952, val_acc: 0.94667\n",
      "Epoch [9220/10000], train loss: 0.13050 train acc: 0.98667 val_loss: 0.13946, val_acc: 0.94667\n",
      "Epoch [9230/10000], train loss: 0.13043 train acc: 0.98667 val_loss: 0.13941, val_acc: 0.94667\n",
      "Epoch [9240/10000], train loss: 0.13037 train acc: 0.98667 val_loss: 0.13935, val_acc: 0.94667\n",
      "Epoch [9250/10000], train loss: 0.13030 train acc: 0.98667 val_loss: 0.13929, val_acc: 0.94667\n",
      "Epoch [9260/10000], train loss: 0.13024 train acc: 0.98667 val_loss: 0.13924, val_acc: 0.94667\n",
      "Epoch [9270/10000], train loss: 0.13017 train acc: 0.98667 val_loss: 0.13918, val_acc: 0.94667\n",
      "Epoch [9280/10000], train loss: 0.13011 train acc: 0.98667 val_loss: 0.13913, val_acc: 0.94667\n",
      "Epoch [9290/10000], train loss: 0.13005 train acc: 0.98667 val_loss: 0.13907, val_acc: 0.94667\n",
      "Epoch [9300/10000], train loss: 0.12998 train acc: 0.98667 val_loss: 0.13902, val_acc: 0.94667\n",
      "Epoch [9310/10000], train loss: 0.12992 train acc: 0.98667 val_loss: 0.13896, val_acc: 0.94667\n",
      "Epoch [9320/10000], train loss: 0.12985 train acc: 0.98667 val_loss: 0.13891, val_acc: 0.94667\n",
      "Epoch [9330/10000], train loss: 0.12979 train acc: 0.98667 val_loss: 0.13885, val_acc: 0.94667\n",
      "Epoch [9340/10000], train loss: 0.12973 train acc: 0.98667 val_loss: 0.13880, val_acc: 0.94667\n",
      "Epoch [9350/10000], train loss: 0.12966 train acc: 0.98667 val_loss: 0.13874, val_acc: 0.94667\n",
      "Epoch [9360/10000], train loss: 0.12960 train acc: 0.98667 val_loss: 0.13869, val_acc: 0.94667\n",
      "Epoch [9370/10000], train loss: 0.12953 train acc: 0.98667 val_loss: 0.13864, val_acc: 0.94667\n",
      "Epoch [9380/10000], train loss: 0.12947 train acc: 0.98667 val_loss: 0.13858, val_acc: 0.94667\n",
      "Epoch [9390/10000], train loss: 0.12941 train acc: 0.98667 val_loss: 0.13853, val_acc: 0.94667\n",
      "Epoch [9400/10000], train loss: 0.12935 train acc: 0.98667 val_loss: 0.13847, val_acc: 0.94667\n",
      "Epoch [9410/10000], train loss: 0.12928 train acc: 0.98667 val_loss: 0.13842, val_acc: 0.94667\n",
      "Epoch [9420/10000], train loss: 0.12922 train acc: 0.98667 val_loss: 0.13836, val_acc: 0.94667\n",
      "Epoch [9430/10000], train loss: 0.12916 train acc: 0.98667 val_loss: 0.13831, val_acc: 0.94667\n",
      "Epoch [9440/10000], train loss: 0.12909 train acc: 0.98667 val_loss: 0.13826, val_acc: 0.94667\n",
      "Epoch [9450/10000], train loss: 0.12903 train acc: 0.98667 val_loss: 0.13820, val_acc: 0.94667\n",
      "Epoch [9460/10000], train loss: 0.12897 train acc: 0.98667 val_loss: 0.13815, val_acc: 0.94667\n",
      "Epoch [9470/10000], train loss: 0.12891 train acc: 0.98667 val_loss: 0.13810, val_acc: 0.94667\n",
      "Epoch [9480/10000], train loss: 0.12884 train acc: 0.98667 val_loss: 0.13804, val_acc: 0.94667\n",
      "Epoch [9490/10000], train loss: 0.12878 train acc: 0.98667 val_loss: 0.13799, val_acc: 0.94667\n",
      "Epoch [9500/10000], train loss: 0.12872 train acc: 0.98667 val_loss: 0.13794, val_acc: 0.94667\n",
      "Epoch [9510/10000], train loss: 0.12866 train acc: 0.98667 val_loss: 0.13788, val_acc: 0.94667\n",
      "Epoch [9520/10000], train loss: 0.12860 train acc: 0.98667 val_loss: 0.13783, val_acc: 0.94667\n",
      "Epoch [9530/10000], train loss: 0.12853 train acc: 0.98667 val_loss: 0.13778, val_acc: 0.94667\n",
      "Epoch [9540/10000], train loss: 0.12847 train acc: 0.98667 val_loss: 0.13773, val_acc: 0.94667\n",
      "Epoch [9550/10000], train loss: 0.12841 train acc: 0.98667 val_loss: 0.13767, val_acc: 0.94667\n",
      "Epoch [9560/10000], train loss: 0.12835 train acc: 0.98667 val_loss: 0.13762, val_acc: 0.94667\n",
      "Epoch [9570/10000], train loss: 0.12829 train acc: 0.98667 val_loss: 0.13757, val_acc: 0.94667\n",
      "Epoch [9580/10000], train loss: 0.12823 train acc: 0.98667 val_loss: 0.13752, val_acc: 0.94667\n",
      "Epoch [9590/10000], train loss: 0.12817 train acc: 0.98667 val_loss: 0.13747, val_acc: 0.94667\n",
      "Epoch [9600/10000], train loss: 0.12811 train acc: 0.98667 val_loss: 0.13741, val_acc: 0.94667\n",
      "Epoch [9610/10000], train loss: 0.12804 train acc: 0.98667 val_loss: 0.13736, val_acc: 0.94667\n",
      "Epoch [9620/10000], train loss: 0.12798 train acc: 0.98667 val_loss: 0.13731, val_acc: 0.94667\n",
      "Epoch [9630/10000], train loss: 0.12792 train acc: 0.98667 val_loss: 0.13726, val_acc: 0.94667\n",
      "Epoch [9640/10000], train loss: 0.12786 train acc: 0.98667 val_loss: 0.13721, val_acc: 0.94667\n",
      "Epoch [9650/10000], train loss: 0.12780 train acc: 0.98667 val_loss: 0.13715, val_acc: 0.94667\n",
      "Epoch [9660/10000], train loss: 0.12774 train acc: 0.98667 val_loss: 0.13710, val_acc: 0.94667\n",
      "Epoch [9670/10000], train loss: 0.12768 train acc: 0.98667 val_loss: 0.13705, val_acc: 0.94667\n",
      "Epoch [9680/10000], train loss: 0.12762 train acc: 0.98667 val_loss: 0.13700, val_acc: 0.94667\n",
      "Epoch [9690/10000], train loss: 0.12756 train acc: 0.98667 val_loss: 0.13695, val_acc: 0.94667\n",
      "Epoch [9700/10000], train loss: 0.12750 train acc: 0.98667 val_loss: 0.13690, val_acc: 0.94667\n",
      "Epoch [9710/10000], train loss: 0.12744 train acc: 0.98667 val_loss: 0.13685, val_acc: 0.94667\n",
      "Epoch [9720/10000], train loss: 0.12738 train acc: 0.98667 val_loss: 0.13680, val_acc: 0.94667\n",
      "Epoch [9730/10000], train loss: 0.12732 train acc: 0.98667 val_loss: 0.13675, val_acc: 0.94667\n",
      "Epoch [9740/10000], train loss: 0.12726 train acc: 0.98667 val_loss: 0.13669, val_acc: 0.94667\n",
      "Epoch [9750/10000], train loss: 0.12720 train acc: 0.98667 val_loss: 0.13664, val_acc: 0.94667\n",
      "Epoch [9760/10000], train loss: 0.12714 train acc: 0.98667 val_loss: 0.13659, val_acc: 0.94667\n",
      "Epoch [9770/10000], train loss: 0.12709 train acc: 0.98667 val_loss: 0.13654, val_acc: 0.94667\n",
      "Epoch [9780/10000], train loss: 0.12703 train acc: 0.98667 val_loss: 0.13649, val_acc: 0.94667\n",
      "Epoch [9790/10000], train loss: 0.12697 train acc: 0.98667 val_loss: 0.13644, val_acc: 0.94667\n",
      "Epoch [9800/10000], train loss: 0.12691 train acc: 0.98667 val_loss: 0.13639, val_acc: 0.94667\n",
      "Epoch [9810/10000], train loss: 0.12685 train acc: 0.98667 val_loss: 0.13634, val_acc: 0.94667\n",
      "Epoch [9820/10000], train loss: 0.12679 train acc: 0.98667 val_loss: 0.13629, val_acc: 0.94667\n",
      "Epoch [9830/10000], train loss: 0.12673 train acc: 0.98667 val_loss: 0.13624, val_acc: 0.94667\n",
      "Epoch [9840/10000], train loss: 0.12667 train acc: 0.98667 val_loss: 0.13619, val_acc: 0.94667\n",
      "Epoch [9850/10000], train loss: 0.12662 train acc: 0.98667 val_loss: 0.13614, val_acc: 0.94667\n",
      "Epoch [9860/10000], train loss: 0.12656 train acc: 0.98667 val_loss: 0.13609, val_acc: 0.94667\n",
      "Epoch [9870/10000], train loss: 0.12650 train acc: 0.98667 val_loss: 0.13604, val_acc: 0.94667\n",
      "Epoch [9880/10000], train loss: 0.12644 train acc: 0.98667 val_loss: 0.13599, val_acc: 0.94667\n",
      "Epoch [9890/10000], train loss: 0.12638 train acc: 0.98667 val_loss: 0.13595, val_acc: 0.94667\n",
      "Epoch [9900/10000], train loss: 0.12633 train acc: 0.98667 val_loss: 0.13590, val_acc: 0.94667\n",
      "Epoch [9910/10000], train loss: 0.12627 train acc: 0.98667 val_loss: 0.13585, val_acc: 0.94667\n",
      "Epoch [9920/10000], train loss: 0.12621 train acc: 0.98667 val_loss: 0.13580, val_acc: 0.94667\n",
      "Epoch [9930/10000], train loss: 0.12615 train acc: 0.98667 val_loss: 0.13575, val_acc: 0.94667\n",
      "Epoch [9940/10000], train loss: 0.12610 train acc: 0.98667 val_loss: 0.13570, val_acc: 0.94667\n",
      "Epoch [9950/10000], train loss: 0.12604 train acc: 0.98667 val_loss: 0.13565, val_acc: 0.94667\n",
      "Epoch [9960/10000], train loss: 0.12598 train acc: 0.98667 val_loss: 0.13560, val_acc: 0.94667\n",
      "Epoch [9970/10000], train loss: 0.12592 train acc: 0.98667 val_loss: 0.13555, val_acc: 0.94667\n",
      "Epoch [9980/10000], train loss: 0.12587 train acc: 0.98667 val_loss: 0.13551, val_acc: 0.94667\n",
      "Epoch [9990/10000], train loss: 0.12581 train acc: 0.98667 val_loss: 0.13546, val_acc: 0.94667\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # 훈련 페이즈\n",
    "\n",
    "    # 경사 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 예측 계산\n",
    "    outputs = net(inputs) #logsoftmax \n",
    "\n",
    "    # 손실 계산\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # 경사 계산\n",
    "    loss.backward()\n",
    "\n",
    "    # 파라미터 수정\n",
    "    optimizer.step()\n",
    "\n",
    "    # 예측 라벨 산출\n",
    "    predicted = torch.max(outputs, 1)[1]\n",
    "\n",
    "    # 손실과 정확도 계산\n",
    "    train_loss = loss.item()\n",
    "    train_acc = (predicted == labels).sum()  / len(labels)\n",
    "\n",
    "    # 예측 페이즈\n",
    "\n",
    "    # 예측 계산\n",
    "    outputs_test = net(inputs_test)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss_test = criterion(outputs_test, labels_test)\n",
    "\n",
    "    # 예측 라벨 산출\n",
    "    predicted_test = torch.max(outputs_test, 1)[1]\n",
    "\n",
    "    # 손실과 정확도 계산\n",
    "    val_loss =  loss_test.item()\n",
    "    val_acc =  (predicted_test == labels_test).sum() / len(labels_test)\n",
    "\n",
    "    if ( epoch % 10 == 0):\n",
    "        print (f'Epoch [{epoch}/{num_epochs}], train loss: {train_loss:.5f} train acc: {train_acc:.5f} val_loss: {val_loss:.5f}, val_acc: {val_acc:.5f}')\n",
    "        item = np.array([epoch , train_loss, train_acc, val_loss, val_acc])\n",
    "        history = np.vstack((history, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1739157005354,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "JNK3ajHR0UG_",
    "outputId": "5d432f21-4950-4890-db4e-9bb0f082e737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기상태 : 손실 : 2.18588  정확도 : 0.36000\n",
      "최종상태 : 손실 : 0.13546  정확도 : 0.94667\n"
     ]
    }
   ],
   "source": [
    "# 손실과 정확도 확인\n",
    "\n",
    "print(f'초기상태 : 손실 : {history[0,3]:.5f}  정확도 : {history[0,4]:.5f}' )\n",
    "print(f'최종상태 : 손실 : {history[-1,3]:.5f}  정확도 : {history[-1,4]:.5f}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739157005357,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "l86PUr7m0UG_",
    "outputId": "2a775b30-76eb-4e3d-84a3-34425512b36a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.8479  -0.1029  -2.4082]\n",
      " [ -5.02    -0.0226  -4.1506]\n",
      " [ -0.0661  -2.7497 -16.0479]\n",
      " [-11.5789  -3.1836  -0.0423]\n",
      " [ -9.323   -1.7407  -0.193 ]]\n",
      "[[0.0078 0.9022 0.09  ]\n",
      " [0.0066 0.9776 0.0158]\n",
      " [0.9361 0.0639 0.    ]\n",
      " [0.     0.0414 0.9586]\n",
      " [0.0001 0.1754 0.8245]]\n"
     ]
    }
   ],
   "source": [
    "# 패턴 2 모델의 출력 결과\n",
    "w = outputs[:5,:].data\n",
    "print(w.numpy())\n",
    "\n",
    "# 확률값을 얻고 싶은 경우\n",
    "print(torch.exp(w).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6vcAxVv0UHA"
   },
   "source": [
    "### 모델 클래스측에 소프트맥스 함수 만 포함된 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1739157005359,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "onUZD0RO0UHA"
   },
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "# 2입력 3출력 로지스틱 회귀 모델\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_input, n_output)\n",
    "        # 소프트맥스 함수 정의\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # 초깃값을 모두 1로 함\n",
    "        # \"딥러닝을 위한 수학\"과 조건을 맞추기 위한 목적\n",
    "        self.l1.weight.data.fill_(1.0)\n",
    "        self.l1.bias.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.l1(x)\n",
    "        x2 = self.softmax(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739157005372,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "7eyYcJgw0UHA"
   },
   "outputs": [],
   "source": [
    "# 학습률\n",
    "lr = 0.01\n",
    "\n",
    "# 초기화\n",
    "net = Net(n_input, n_output)\n",
    "\n",
    "# 손실 함수： NLLLoss 함수\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# 최적화 함수: 경사 하강법\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# 반복 횟수\n",
    "num_epochs = 10000\n",
    "\n",
    "# 평가 결과 기록\n",
    "history = np.zeros((0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9227,
     "status": "ok",
     "timestamp": 1739157014627,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "tny2lU5h0UHA",
    "outputId": "6c9e1454-bb99-46bf-e4fb-a60aea7c4e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10000], loss: 1.09861 acc: 0.30667 val_loss: 1.09158, val_acc: 0.26667\n",
      "Epoch [10/10000], loss: 1.01848 acc: 0.40000 val_loss: 1.04171, val_acc: 0.26667\n",
      "Epoch [20/10000], loss: 0.96854 acc: 0.40000 val_loss: 0.98850, val_acc: 0.26667\n",
      "Epoch [30/10000], loss: 0.92459 acc: 0.65333 val_loss: 0.93996, val_acc: 0.57333\n",
      "Epoch [40/10000], loss: 0.88568 acc: 0.70667 val_loss: 0.89704, val_acc: 0.62667\n",
      "Epoch [50/10000], loss: 0.85120 acc: 0.70667 val_loss: 0.85918, val_acc: 0.62667\n",
      "Epoch [60/10000], loss: 0.82059 acc: 0.70667 val_loss: 0.82572, val_acc: 0.62667\n",
      "Epoch [70/10000], loss: 0.79335 acc: 0.72000 val_loss: 0.79607, val_acc: 0.62667\n",
      "Epoch [80/10000], loss: 0.76900 acc: 0.72000 val_loss: 0.76968, val_acc: 0.65333\n",
      "Epoch [90/10000], loss: 0.74717 acc: 0.72000 val_loss: 0.74610, val_acc: 0.65333\n",
      "Epoch [100/10000], loss: 0.72750 acc: 0.76000 val_loss: 0.72494, val_acc: 0.69333\n",
      "Epoch [110/10000], loss: 0.70970 acc: 0.77333 val_loss: 0.70585, val_acc: 0.74667\n",
      "Epoch [120/10000], loss: 0.69354 acc: 0.81333 val_loss: 0.68856, val_acc: 0.76000\n",
      "Epoch [130/10000], loss: 0.67878 acc: 0.84000 val_loss: 0.67283, val_acc: 0.76000\n",
      "Epoch [140/10000], loss: 0.66526 acc: 0.84000 val_loss: 0.65846, val_acc: 0.78667\n",
      "Epoch [150/10000], loss: 0.65283 acc: 0.86667 val_loss: 0.64528, val_acc: 0.78667\n",
      "Epoch [160/10000], loss: 0.64135 acc: 0.88000 val_loss: 0.63313, val_acc: 0.78667\n",
      "Epoch [170/10000], loss: 0.63070 acc: 0.89333 val_loss: 0.62190, val_acc: 0.81333\n",
      "Epoch [180/10000], loss: 0.62080 acc: 0.90667 val_loss: 0.61149, val_acc: 0.81333\n",
      "Epoch [190/10000], loss: 0.61157 acc: 0.90667 val_loss: 0.60179, val_acc: 0.84000\n",
      "Epoch [200/10000], loss: 0.60292 acc: 0.90667 val_loss: 0.59273, val_acc: 0.84000\n",
      "Epoch [210/10000], loss: 0.59481 acc: 0.90667 val_loss: 0.58425, val_acc: 0.88000\n",
      "Epoch [220/10000], loss: 0.58717 acc: 0.93333 val_loss: 0.57628, val_acc: 0.88000\n",
      "Epoch [230/10000], loss: 0.57996 acc: 0.93333 val_loss: 0.56877, val_acc: 0.89333\n",
      "Epoch [240/10000], loss: 0.57313 acc: 0.93333 val_loss: 0.56169, val_acc: 0.90667\n",
      "Epoch [250/10000], loss: 0.56666 acc: 0.93333 val_loss: 0.55498, val_acc: 0.90667\n",
      "Epoch [260/10000], loss: 0.56051 acc: 0.92000 val_loss: 0.54862, val_acc: 0.90667\n",
      "Epoch [270/10000], loss: 0.55465 acc: 0.92000 val_loss: 0.54257, val_acc: 0.90667\n",
      "Epoch [280/10000], loss: 0.54906 acc: 0.92000 val_loss: 0.53681, val_acc: 0.90667\n",
      "Epoch [290/10000], loss: 0.54371 acc: 0.92000 val_loss: 0.53131, val_acc: 0.90667\n",
      "Epoch [300/10000], loss: 0.53859 acc: 0.93333 val_loss: 0.52605, val_acc: 0.90667\n",
      "Epoch [310/10000], loss: 0.53368 acc: 0.93333 val_loss: 0.52102, val_acc: 0.90667\n",
      "Epoch [320/10000], loss: 0.52896 acc: 0.93333 val_loss: 0.51619, val_acc: 0.90667\n",
      "Epoch [330/10000], loss: 0.52442 acc: 0.93333 val_loss: 0.51155, val_acc: 0.90667\n",
      "Epoch [340/10000], loss: 0.52004 acc: 0.93333 val_loss: 0.50709, val_acc: 0.90667\n",
      "Epoch [350/10000], loss: 0.51582 acc: 0.93333 val_loss: 0.50280, val_acc: 0.90667\n",
      "Epoch [360/10000], loss: 0.51173 acc: 0.93333 val_loss: 0.49865, val_acc: 0.90667\n",
      "Epoch [370/10000], loss: 0.50779 acc: 0.93333 val_loss: 0.49465, val_acc: 0.90667\n",
      "Epoch [380/10000], loss: 0.50397 acc: 0.93333 val_loss: 0.49078, val_acc: 0.90667\n",
      "Epoch [390/10000], loss: 0.50026 acc: 0.93333 val_loss: 0.48703, val_acc: 0.90667\n",
      "Epoch [400/10000], loss: 0.49666 acc: 0.94667 val_loss: 0.48340, val_acc: 0.90667\n",
      "Epoch [410/10000], loss: 0.49317 acc: 0.94667 val_loss: 0.47988, val_acc: 0.90667\n",
      "Epoch [420/10000], loss: 0.48978 acc: 0.94667 val_loss: 0.47647, val_acc: 0.90667\n",
      "Epoch [430/10000], loss: 0.48647 acc: 0.96000 val_loss: 0.47315, val_acc: 0.90667\n",
      "Epoch [440/10000], loss: 0.48326 acc: 0.96000 val_loss: 0.46992, val_acc: 0.90667\n",
      "Epoch [450/10000], loss: 0.48012 acc: 0.96000 val_loss: 0.46678, val_acc: 0.90667\n",
      "Epoch [460/10000], loss: 0.47706 acc: 0.96000 val_loss: 0.46372, val_acc: 0.90667\n",
      "Epoch [470/10000], loss: 0.47408 acc: 0.96000 val_loss: 0.46073, val_acc: 0.90667\n",
      "Epoch [480/10000], loss: 0.47116 acc: 0.96000 val_loss: 0.45783, val_acc: 0.90667\n",
      "Epoch [490/10000], loss: 0.46831 acc: 0.96000 val_loss: 0.45499, val_acc: 0.90667\n",
      "Epoch [500/10000], loss: 0.46553 acc: 0.96000 val_loss: 0.45221, val_acc: 0.90667\n",
      "Epoch [510/10000], loss: 0.46280 acc: 0.96000 val_loss: 0.44951, val_acc: 0.90667\n",
      "Epoch [520/10000], loss: 0.46013 acc: 0.96000 val_loss: 0.44686, val_acc: 0.90667\n",
      "Epoch [530/10000], loss: 0.45752 acc: 0.96000 val_loss: 0.44426, val_acc: 0.90667\n",
      "Epoch [540/10000], loss: 0.45496 acc: 0.96000 val_loss: 0.44173, val_acc: 0.90667\n",
      "Epoch [550/10000], loss: 0.45245 acc: 0.96000 val_loss: 0.43924, val_acc: 0.90667\n",
      "Epoch [560/10000], loss: 0.44998 acc: 0.96000 val_loss: 0.43681, val_acc: 0.90667\n",
      "Epoch [570/10000], loss: 0.44757 acc: 0.96000 val_loss: 0.43442, val_acc: 0.90667\n",
      "Epoch [580/10000], loss: 0.44519 acc: 0.96000 val_loss: 0.43208, val_acc: 0.90667\n",
      "Epoch [590/10000], loss: 0.44286 acc: 0.96000 val_loss: 0.42979, val_acc: 0.92000\n",
      "Epoch [600/10000], loss: 0.44057 acc: 0.96000 val_loss: 0.42753, val_acc: 0.92000\n",
      "Epoch [610/10000], loss: 0.43832 acc: 0.96000 val_loss: 0.42532, val_acc: 0.92000\n",
      "Epoch [620/10000], loss: 0.43611 acc: 0.96000 val_loss: 0.42315, val_acc: 0.92000\n",
      "Epoch [630/10000], loss: 0.43393 acc: 0.96000 val_loss: 0.42101, val_acc: 0.92000\n",
      "Epoch [640/10000], loss: 0.43179 acc: 0.96000 val_loss: 0.41891, val_acc: 0.92000\n",
      "Epoch [650/10000], loss: 0.42968 acc: 0.96000 val_loss: 0.41685, val_acc: 0.92000\n",
      "Epoch [660/10000], loss: 0.42761 acc: 0.96000 val_loss: 0.41482, val_acc: 0.92000\n",
      "Epoch [670/10000], loss: 0.42556 acc: 0.96000 val_loss: 0.41282, val_acc: 0.92000\n",
      "Epoch [680/10000], loss: 0.42355 acc: 0.96000 val_loss: 0.41085, val_acc: 0.92000\n",
      "Epoch [690/10000], loss: 0.42157 acc: 0.96000 val_loss: 0.40892, val_acc: 0.92000\n",
      "Epoch [700/10000], loss: 0.41961 acc: 0.96000 val_loss: 0.40701, val_acc: 0.92000\n",
      "Epoch [710/10000], loss: 0.41768 acc: 0.96000 val_loss: 0.40513, val_acc: 0.92000\n",
      "Epoch [720/10000], loss: 0.41578 acc: 0.96000 val_loss: 0.40329, val_acc: 0.92000\n",
      "Epoch [730/10000], loss: 0.41391 acc: 0.96000 val_loss: 0.40146, val_acc: 0.92000\n",
      "Epoch [740/10000], loss: 0.41206 acc: 0.96000 val_loss: 0.39967, val_acc: 0.92000\n",
      "Epoch [750/10000], loss: 0.41024 acc: 0.96000 val_loss: 0.39789, val_acc: 0.92000\n",
      "Epoch [760/10000], loss: 0.40844 acc: 0.96000 val_loss: 0.39615, val_acc: 0.92000\n",
      "Epoch [770/10000], loss: 0.40666 acc: 0.96000 val_loss: 0.39443, val_acc: 0.93333\n",
      "Epoch [780/10000], loss: 0.40491 acc: 0.96000 val_loss: 0.39273, val_acc: 0.93333\n",
      "Epoch [790/10000], loss: 0.40317 acc: 0.96000 val_loss: 0.39105, val_acc: 0.93333\n",
      "Epoch [800/10000], loss: 0.40146 acc: 0.96000 val_loss: 0.38939, val_acc: 0.93333\n",
      "Epoch [810/10000], loss: 0.39977 acc: 0.96000 val_loss: 0.38776, val_acc: 0.93333\n",
      "Epoch [820/10000], loss: 0.39810 acc: 0.96000 val_loss: 0.38615, val_acc: 0.93333\n",
      "Epoch [830/10000], loss: 0.39646 acc: 0.96000 val_loss: 0.38456, val_acc: 0.93333\n",
      "Epoch [840/10000], loss: 0.39483 acc: 0.96000 val_loss: 0.38298, val_acc: 0.93333\n",
      "Epoch [850/10000], loss: 0.39321 acc: 0.97333 val_loss: 0.38143, val_acc: 0.94667\n",
      "Epoch [860/10000], loss: 0.39162 acc: 0.97333 val_loss: 0.37990, val_acc: 0.94667\n",
      "Epoch [870/10000], loss: 0.39005 acc: 0.97333 val_loss: 0.37838, val_acc: 0.94667\n",
      "Epoch [880/10000], loss: 0.38849 acc: 0.97333 val_loss: 0.37688, val_acc: 0.94667\n",
      "Epoch [890/10000], loss: 0.38695 acc: 0.97333 val_loss: 0.37540, val_acc: 0.94667\n",
      "Epoch [900/10000], loss: 0.38543 acc: 0.97333 val_loss: 0.37394, val_acc: 0.94667\n",
      "Epoch [910/10000], loss: 0.38392 acc: 0.97333 val_loss: 0.37249, val_acc: 0.94667\n",
      "Epoch [920/10000], loss: 0.38243 acc: 0.97333 val_loss: 0.37106, val_acc: 0.94667\n",
      "Epoch [930/10000], loss: 0.38096 acc: 0.97333 val_loss: 0.36965, val_acc: 0.94667\n",
      "Epoch [940/10000], loss: 0.37950 acc: 0.97333 val_loss: 0.36825, val_acc: 0.94667\n",
      "Epoch [950/10000], loss: 0.37806 acc: 0.97333 val_loss: 0.36686, val_acc: 0.94667\n",
      "Epoch [960/10000], loss: 0.37663 acc: 0.97333 val_loss: 0.36550, val_acc: 0.96000\n",
      "Epoch [970/10000], loss: 0.37522 acc: 0.97333 val_loss: 0.36414, val_acc: 0.96000\n",
      "Epoch [980/10000], loss: 0.37382 acc: 0.97333 val_loss: 0.36280, val_acc: 0.96000\n",
      "Epoch [990/10000], loss: 0.37243 acc: 0.97333 val_loss: 0.36148, val_acc: 0.96000\n",
      "Epoch [1000/10000], loss: 0.37106 acc: 0.97333 val_loss: 0.36017, val_acc: 0.96000\n",
      "Epoch [1010/10000], loss: 0.36970 acc: 0.97333 val_loss: 0.35887, val_acc: 0.96000\n",
      "Epoch [1020/10000], loss: 0.36836 acc: 0.97333 val_loss: 0.35758, val_acc: 0.96000\n",
      "Epoch [1030/10000], loss: 0.36703 acc: 0.97333 val_loss: 0.35631, val_acc: 0.96000\n",
      "Epoch [1040/10000], loss: 0.36571 acc: 0.97333 val_loss: 0.35505, val_acc: 0.96000\n",
      "Epoch [1050/10000], loss: 0.36440 acc: 0.97333 val_loss: 0.35381, val_acc: 0.96000\n",
      "Epoch [1060/10000], loss: 0.36311 acc: 0.97333 val_loss: 0.35258, val_acc: 0.96000\n",
      "Epoch [1070/10000], loss: 0.36183 acc: 0.97333 val_loss: 0.35135, val_acc: 0.96000\n",
      "Epoch [1080/10000], loss: 0.36056 acc: 0.97333 val_loss: 0.35014, val_acc: 0.96000\n",
      "Epoch [1090/10000], loss: 0.35930 acc: 0.97333 val_loss: 0.34895, val_acc: 0.96000\n",
      "Epoch [1100/10000], loss: 0.35805 acc: 0.97333 val_loss: 0.34776, val_acc: 0.96000\n",
      "Epoch [1110/10000], loss: 0.35682 acc: 0.97333 val_loss: 0.34659, val_acc: 0.96000\n",
      "Epoch [1120/10000], loss: 0.35559 acc: 0.97333 val_loss: 0.34542, val_acc: 0.96000\n",
      "Epoch [1130/10000], loss: 0.35438 acc: 0.97333 val_loss: 0.34427, val_acc: 0.96000\n",
      "Epoch [1140/10000], loss: 0.35318 acc: 0.97333 val_loss: 0.34313, val_acc: 0.96000\n",
      "Epoch [1150/10000], loss: 0.35199 acc: 0.97333 val_loss: 0.34199, val_acc: 0.96000\n",
      "Epoch [1160/10000], loss: 0.35081 acc: 0.97333 val_loss: 0.34087, val_acc: 0.96000\n",
      "Epoch [1170/10000], loss: 0.34964 acc: 0.97333 val_loss: 0.33976, val_acc: 0.96000\n",
      "Epoch [1180/10000], loss: 0.34848 acc: 0.97333 val_loss: 0.33866, val_acc: 0.96000\n",
      "Epoch [1190/10000], loss: 0.34732 acc: 0.97333 val_loss: 0.33757, val_acc: 0.96000\n",
      "Epoch [1200/10000], loss: 0.34618 acc: 0.97333 val_loss: 0.33649, val_acc: 0.96000\n",
      "Epoch [1210/10000], loss: 0.34505 acc: 0.97333 val_loss: 0.33542, val_acc: 0.96000\n",
      "Epoch [1220/10000], loss: 0.34393 acc: 0.97333 val_loss: 0.33435, val_acc: 0.96000\n",
      "Epoch [1230/10000], loss: 0.34282 acc: 0.97333 val_loss: 0.33330, val_acc: 0.96000\n",
      "Epoch [1240/10000], loss: 0.34172 acc: 0.97333 val_loss: 0.33226, val_acc: 0.96000\n",
      "Epoch [1250/10000], loss: 0.34062 acc: 0.97333 val_loss: 0.33122, val_acc: 0.96000\n",
      "Epoch [1260/10000], loss: 0.33954 acc: 0.97333 val_loss: 0.33020, val_acc: 0.96000\n",
      "Epoch [1270/10000], loss: 0.33846 acc: 0.97333 val_loss: 0.32918, val_acc: 0.96000\n",
      "Epoch [1280/10000], loss: 0.33740 acc: 0.97333 val_loss: 0.32817, val_acc: 0.96000\n",
      "Epoch [1290/10000], loss: 0.33634 acc: 0.97333 val_loss: 0.32717, val_acc: 0.96000\n",
      "Epoch [1300/10000], loss: 0.33529 acc: 0.97333 val_loss: 0.32618, val_acc: 0.96000\n",
      "Epoch [1310/10000], loss: 0.33425 acc: 0.97333 val_loss: 0.32520, val_acc: 0.96000\n",
      "Epoch [1320/10000], loss: 0.33321 acc: 0.97333 val_loss: 0.32422, val_acc: 0.96000\n",
      "Epoch [1330/10000], loss: 0.33219 acc: 0.97333 val_loss: 0.32325, val_acc: 0.96000\n",
      "Epoch [1340/10000], loss: 0.33117 acc: 0.97333 val_loss: 0.32229, val_acc: 0.96000\n",
      "Epoch [1350/10000], loss: 0.33016 acc: 0.97333 val_loss: 0.32134, val_acc: 0.96000\n",
      "Epoch [1360/10000], loss: 0.32916 acc: 0.97333 val_loss: 0.32040, val_acc: 0.96000\n",
      "Epoch [1370/10000], loss: 0.32817 acc: 0.97333 val_loss: 0.31946, val_acc: 0.96000\n",
      "Epoch [1380/10000], loss: 0.32719 acc: 0.97333 val_loss: 0.31853, val_acc: 0.96000\n",
      "Epoch [1390/10000], loss: 0.32621 acc: 0.97333 val_loss: 0.31761, val_acc: 0.96000\n",
      "Epoch [1400/10000], loss: 0.32524 acc: 0.97333 val_loss: 0.31670, val_acc: 0.96000\n",
      "Epoch [1410/10000], loss: 0.32428 acc: 0.97333 val_loss: 0.31579, val_acc: 0.96000\n",
      "Epoch [1420/10000], loss: 0.32332 acc: 0.97333 val_loss: 0.31489, val_acc: 0.96000\n",
      "Epoch [1430/10000], loss: 0.32237 acc: 0.97333 val_loss: 0.31400, val_acc: 0.96000\n",
      "Epoch [1440/10000], loss: 0.32143 acc: 0.97333 val_loss: 0.31312, val_acc: 0.96000\n",
      "Epoch [1450/10000], loss: 0.32050 acc: 0.97333 val_loss: 0.31224, val_acc: 0.96000\n",
      "Epoch [1460/10000], loss: 0.31957 acc: 0.97333 val_loss: 0.31137, val_acc: 0.96000\n",
      "Epoch [1470/10000], loss: 0.31865 acc: 0.97333 val_loss: 0.31050, val_acc: 0.96000\n",
      "Epoch [1480/10000], loss: 0.31774 acc: 0.97333 val_loss: 0.30964, val_acc: 0.96000\n",
      "Epoch [1490/10000], loss: 0.31683 acc: 0.97333 val_loss: 0.30879, val_acc: 0.96000\n",
      "Epoch [1500/10000], loss: 0.31593 acc: 0.97333 val_loss: 0.30795, val_acc: 0.96000\n",
      "Epoch [1510/10000], loss: 0.31504 acc: 0.97333 val_loss: 0.30711, val_acc: 0.96000\n",
      "Epoch [1520/10000], loss: 0.31415 acc: 0.97333 val_loss: 0.30628, val_acc: 0.96000\n",
      "Epoch [1530/10000], loss: 0.31327 acc: 0.97333 val_loss: 0.30545, val_acc: 0.96000\n",
      "Epoch [1540/10000], loss: 0.31240 acc: 0.97333 val_loss: 0.30463, val_acc: 0.96000\n",
      "Epoch [1550/10000], loss: 0.31153 acc: 0.97333 val_loss: 0.30382, val_acc: 0.96000\n",
      "Epoch [1560/10000], loss: 0.31067 acc: 0.97333 val_loss: 0.30301, val_acc: 0.96000\n",
      "Epoch [1570/10000], loss: 0.30981 acc: 0.97333 val_loss: 0.30221, val_acc: 0.96000\n",
      "Epoch [1580/10000], loss: 0.30896 acc: 0.97333 val_loss: 0.30141, val_acc: 0.96000\n",
      "Epoch [1590/10000], loss: 0.30812 acc: 0.97333 val_loss: 0.30062, val_acc: 0.96000\n",
      "Epoch [1600/10000], loss: 0.30728 acc: 0.97333 val_loss: 0.29984, val_acc: 0.96000\n",
      "Epoch [1610/10000], loss: 0.30645 acc: 0.97333 val_loss: 0.29906, val_acc: 0.96000\n",
      "Epoch [1620/10000], loss: 0.30562 acc: 0.97333 val_loss: 0.29828, val_acc: 0.96000\n",
      "Epoch [1630/10000], loss: 0.30480 acc: 0.97333 val_loss: 0.29752, val_acc: 0.96000\n",
      "Epoch [1640/10000], loss: 0.30399 acc: 0.97333 val_loss: 0.29675, val_acc: 0.96000\n",
      "Epoch [1650/10000], loss: 0.30318 acc: 0.97333 val_loss: 0.29600, val_acc: 0.96000\n",
      "Epoch [1660/10000], loss: 0.30237 acc: 0.97333 val_loss: 0.29525, val_acc: 0.96000\n",
      "Epoch [1670/10000], loss: 0.30158 acc: 0.97333 val_loss: 0.29450, val_acc: 0.96000\n",
      "Epoch [1680/10000], loss: 0.30078 acc: 0.97333 val_loss: 0.29376, val_acc: 0.96000\n",
      "Epoch [1690/10000], loss: 0.30000 acc: 0.97333 val_loss: 0.29302, val_acc: 0.96000\n",
      "Epoch [1700/10000], loss: 0.29922 acc: 0.97333 val_loss: 0.29229, val_acc: 0.96000\n",
      "Epoch [1710/10000], loss: 0.29844 acc: 0.97333 val_loss: 0.29157, val_acc: 0.96000\n",
      "Epoch [1720/10000], loss: 0.29767 acc: 0.97333 val_loss: 0.29085, val_acc: 0.96000\n",
      "Epoch [1730/10000], loss: 0.29690 acc: 0.97333 val_loss: 0.29013, val_acc: 0.96000\n",
      "Epoch [1740/10000], loss: 0.29614 acc: 0.97333 val_loss: 0.28942, val_acc: 0.96000\n",
      "Epoch [1750/10000], loss: 0.29538 acc: 0.97333 val_loss: 0.28872, val_acc: 0.96000\n",
      "Epoch [1760/10000], loss: 0.29463 acc: 0.97333 val_loss: 0.28801, val_acc: 0.96000\n",
      "Epoch [1770/10000], loss: 0.29389 acc: 0.97333 val_loss: 0.28732, val_acc: 0.96000\n",
      "Epoch [1780/10000], loss: 0.29315 acc: 0.97333 val_loss: 0.28663, val_acc: 0.96000\n",
      "Epoch [1790/10000], loss: 0.29241 acc: 0.97333 val_loss: 0.28594, val_acc: 0.96000\n",
      "Epoch [1800/10000], loss: 0.29168 acc: 0.97333 val_loss: 0.28526, val_acc: 0.96000\n",
      "Epoch [1810/10000], loss: 0.29095 acc: 0.97333 val_loss: 0.28458, val_acc: 0.96000\n",
      "Epoch [1820/10000], loss: 0.29023 acc: 0.97333 val_loss: 0.28391, val_acc: 0.96000\n",
      "Epoch [1830/10000], loss: 0.28951 acc: 0.97333 val_loss: 0.28324, val_acc: 0.96000\n",
      "Epoch [1840/10000], loss: 0.28880 acc: 0.97333 val_loss: 0.28258, val_acc: 0.96000\n",
      "Epoch [1850/10000], loss: 0.28809 acc: 0.97333 val_loss: 0.28192, val_acc: 0.96000\n",
      "Epoch [1860/10000], loss: 0.28739 acc: 0.97333 val_loss: 0.28126, val_acc: 0.96000\n",
      "Epoch [1870/10000], loss: 0.28669 acc: 0.97333 val_loss: 0.28061, val_acc: 0.96000\n",
      "Epoch [1880/10000], loss: 0.28599 acc: 0.97333 val_loss: 0.27996, val_acc: 0.96000\n",
      "Epoch [1890/10000], loss: 0.28530 acc: 0.97333 val_loss: 0.27932, val_acc: 0.96000\n",
      "Epoch [1900/10000], loss: 0.28462 acc: 0.97333 val_loss: 0.27868, val_acc: 0.96000\n",
      "Epoch [1910/10000], loss: 0.28394 acc: 0.97333 val_loss: 0.27805, val_acc: 0.96000\n",
      "Epoch [1920/10000], loss: 0.28326 acc: 0.97333 val_loss: 0.27742, val_acc: 0.96000\n",
      "Epoch [1930/10000], loss: 0.28258 acc: 0.97333 val_loss: 0.27679, val_acc: 0.96000\n",
      "Epoch [1940/10000], loss: 0.28192 acc: 0.97333 val_loss: 0.27617, val_acc: 0.96000\n",
      "Epoch [1950/10000], loss: 0.28125 acc: 0.97333 val_loss: 0.27555, val_acc: 0.96000\n",
      "Epoch [1960/10000], loss: 0.28059 acc: 0.97333 val_loss: 0.27494, val_acc: 0.96000\n",
      "Epoch [1970/10000], loss: 0.27993 acc: 0.97333 val_loss: 0.27433, val_acc: 0.96000\n",
      "Epoch [1980/10000], loss: 0.27928 acc: 0.97333 val_loss: 0.27372, val_acc: 0.96000\n",
      "Epoch [1990/10000], loss: 0.27863 acc: 0.97333 val_loss: 0.27312, val_acc: 0.96000\n",
      "Epoch [2000/10000], loss: 0.27799 acc: 0.97333 val_loss: 0.27252, val_acc: 0.96000\n",
      "Epoch [2010/10000], loss: 0.27735 acc: 0.97333 val_loss: 0.27193, val_acc: 0.96000\n",
      "Epoch [2020/10000], loss: 0.27671 acc: 0.97333 val_loss: 0.27134, val_acc: 0.96000\n",
      "Epoch [2030/10000], loss: 0.27608 acc: 0.97333 val_loss: 0.27075, val_acc: 0.96000\n",
      "Epoch [2040/10000], loss: 0.27545 acc: 0.97333 val_loss: 0.27016, val_acc: 0.96000\n",
      "Epoch [2050/10000], loss: 0.27482 acc: 0.97333 val_loss: 0.26958, val_acc: 0.96000\n",
      "Epoch [2060/10000], loss: 0.27420 acc: 0.97333 val_loss: 0.26901, val_acc: 0.96000\n",
      "Epoch [2070/10000], loss: 0.27358 acc: 0.97333 val_loss: 0.26843, val_acc: 0.96000\n",
      "Epoch [2080/10000], loss: 0.27297 acc: 0.97333 val_loss: 0.26786, val_acc: 0.96000\n",
      "Epoch [2090/10000], loss: 0.27236 acc: 0.97333 val_loss: 0.26730, val_acc: 0.96000\n",
      "Epoch [2100/10000], loss: 0.27175 acc: 0.97333 val_loss: 0.26674, val_acc: 0.96000\n",
      "Epoch [2110/10000], loss: 0.27115 acc: 0.97333 val_loss: 0.26618, val_acc: 0.96000\n",
      "Epoch [2120/10000], loss: 0.27055 acc: 0.97333 val_loss: 0.26562, val_acc: 0.96000\n",
      "Epoch [2130/10000], loss: 0.26995 acc: 0.97333 val_loss: 0.26507, val_acc: 0.96000\n",
      "Epoch [2140/10000], loss: 0.26936 acc: 0.97333 val_loss: 0.26452, val_acc: 0.96000\n",
      "Epoch [2150/10000], loss: 0.26877 acc: 0.97333 val_loss: 0.26397, val_acc: 0.96000\n",
      "Epoch [2160/10000], loss: 0.26818 acc: 0.97333 val_loss: 0.26343, val_acc: 0.96000\n",
      "Epoch [2170/10000], loss: 0.26760 acc: 0.97333 val_loss: 0.26289, val_acc: 0.96000\n",
      "Epoch [2180/10000], loss: 0.26702 acc: 0.97333 val_loss: 0.26236, val_acc: 0.96000\n",
      "Epoch [2190/10000], loss: 0.26644 acc: 0.97333 val_loss: 0.26182, val_acc: 0.96000\n",
      "Epoch [2200/10000], loss: 0.26587 acc: 0.97333 val_loss: 0.26129, val_acc: 0.96000\n",
      "Epoch [2210/10000], loss: 0.26530 acc: 0.97333 val_loss: 0.26077, val_acc: 0.96000\n",
      "Epoch [2220/10000], loss: 0.26473 acc: 0.97333 val_loss: 0.26024, val_acc: 0.96000\n",
      "Epoch [2230/10000], loss: 0.26417 acc: 0.97333 val_loss: 0.25972, val_acc: 0.96000\n",
      "Epoch [2240/10000], loss: 0.26361 acc: 0.97333 val_loss: 0.25921, val_acc: 0.96000\n",
      "Epoch [2250/10000], loss: 0.26305 acc: 0.97333 val_loss: 0.25869, val_acc: 0.96000\n",
      "Epoch [2260/10000], loss: 0.26250 acc: 0.97333 val_loss: 0.25818, val_acc: 0.96000\n",
      "Epoch [2270/10000], loss: 0.26195 acc: 0.97333 val_loss: 0.25767, val_acc: 0.96000\n",
      "Epoch [2280/10000], loss: 0.26140 acc: 0.97333 val_loss: 0.25717, val_acc: 0.96000\n",
      "Epoch [2290/10000], loss: 0.26086 acc: 0.97333 val_loss: 0.25666, val_acc: 0.96000\n",
      "Epoch [2300/10000], loss: 0.26032 acc: 0.97333 val_loss: 0.25616, val_acc: 0.96000\n",
      "Epoch [2310/10000], loss: 0.25978 acc: 0.97333 val_loss: 0.25567, val_acc: 0.96000\n",
      "Epoch [2320/10000], loss: 0.25924 acc: 0.97333 val_loss: 0.25517, val_acc: 0.96000\n",
      "Epoch [2330/10000], loss: 0.25871 acc: 0.97333 val_loss: 0.25468, val_acc: 0.96000\n",
      "Epoch [2340/10000], loss: 0.25818 acc: 0.97333 val_loss: 0.25419, val_acc: 0.96000\n",
      "Epoch [2350/10000], loss: 0.25766 acc: 0.97333 val_loss: 0.25371, val_acc: 0.96000\n",
      "Epoch [2360/10000], loss: 0.25713 acc: 0.97333 val_loss: 0.25322, val_acc: 0.96000\n",
      "Epoch [2370/10000], loss: 0.25661 acc: 0.97333 val_loss: 0.25274, val_acc: 0.96000\n",
      "Epoch [2380/10000], loss: 0.25609 acc: 0.97333 val_loss: 0.25227, val_acc: 0.96000\n",
      "Epoch [2390/10000], loss: 0.25558 acc: 0.97333 val_loss: 0.25179, val_acc: 0.96000\n",
      "Epoch [2400/10000], loss: 0.25507 acc: 0.97333 val_loss: 0.25132, val_acc: 0.96000\n",
      "Epoch [2410/10000], loss: 0.25456 acc: 0.97333 val_loss: 0.25085, val_acc: 0.96000\n",
      "Epoch [2420/10000], loss: 0.25405 acc: 0.97333 val_loss: 0.25038, val_acc: 0.96000\n",
      "Epoch [2430/10000], loss: 0.25355 acc: 0.97333 val_loss: 0.24992, val_acc: 0.96000\n",
      "Epoch [2440/10000], loss: 0.25304 acc: 0.97333 val_loss: 0.24946, val_acc: 0.96000\n",
      "Epoch [2450/10000], loss: 0.25255 acc: 0.97333 val_loss: 0.24900, val_acc: 0.96000\n",
      "Epoch [2460/10000], loss: 0.25205 acc: 0.97333 val_loss: 0.24854, val_acc: 0.96000\n",
      "Epoch [2470/10000], loss: 0.25156 acc: 0.97333 val_loss: 0.24809, val_acc: 0.96000\n",
      "Epoch [2480/10000], loss: 0.25107 acc: 0.97333 val_loss: 0.24764, val_acc: 0.96000\n",
      "Epoch [2490/10000], loss: 0.25058 acc: 0.97333 val_loss: 0.24719, val_acc: 0.96000\n",
      "Epoch [2500/10000], loss: 0.25009 acc: 0.97333 val_loss: 0.24674, val_acc: 0.96000\n",
      "Epoch [2510/10000], loss: 0.24961 acc: 0.97333 val_loss: 0.24630, val_acc: 0.96000\n",
      "Epoch [2520/10000], loss: 0.24913 acc: 0.97333 val_loss: 0.24585, val_acc: 0.96000\n",
      "Epoch [2530/10000], loss: 0.24865 acc: 0.97333 val_loss: 0.24541, val_acc: 0.96000\n",
      "Epoch [2540/10000], loss: 0.24818 acc: 0.97333 val_loss: 0.24498, val_acc: 0.96000\n",
      "Epoch [2550/10000], loss: 0.24770 acc: 0.97333 val_loss: 0.24454, val_acc: 0.96000\n",
      "Epoch [2560/10000], loss: 0.24723 acc: 0.97333 val_loss: 0.24411, val_acc: 0.96000\n",
      "Epoch [2570/10000], loss: 0.24676 acc: 0.97333 val_loss: 0.24368, val_acc: 0.96000\n",
      "Epoch [2580/10000], loss: 0.24630 acc: 0.98667 val_loss: 0.24325, val_acc: 0.96000\n",
      "Epoch [2590/10000], loss: 0.24584 acc: 0.98667 val_loss: 0.24283, val_acc: 0.96000\n",
      "Epoch [2600/10000], loss: 0.24537 acc: 0.98667 val_loss: 0.24240, val_acc: 0.96000\n",
      "Epoch [2610/10000], loss: 0.24492 acc: 0.98667 val_loss: 0.24198, val_acc: 0.96000\n",
      "Epoch [2620/10000], loss: 0.24446 acc: 0.98667 val_loss: 0.24156, val_acc: 0.96000\n",
      "Epoch [2630/10000], loss: 0.24401 acc: 0.98667 val_loss: 0.24115, val_acc: 0.96000\n",
      "Epoch [2640/10000], loss: 0.24355 acc: 0.98667 val_loss: 0.24073, val_acc: 0.96000\n",
      "Epoch [2650/10000], loss: 0.24311 acc: 0.98667 val_loss: 0.24032, val_acc: 0.96000\n",
      "Epoch [2660/10000], loss: 0.24266 acc: 0.98667 val_loss: 0.23991, val_acc: 0.96000\n",
      "Epoch [2670/10000], loss: 0.24221 acc: 0.98667 val_loss: 0.23950, val_acc: 0.96000\n",
      "Epoch [2680/10000], loss: 0.24177 acc: 0.98667 val_loss: 0.23909, val_acc: 0.96000\n",
      "Epoch [2690/10000], loss: 0.24133 acc: 0.98667 val_loss: 0.23869, val_acc: 0.96000\n",
      "Epoch [2700/10000], loss: 0.24089 acc: 0.98667 val_loss: 0.23829, val_acc: 0.96000\n",
      "Epoch [2710/10000], loss: 0.24046 acc: 0.98667 val_loss: 0.23789, val_acc: 0.96000\n",
      "Epoch [2720/10000], loss: 0.24002 acc: 0.98667 val_loss: 0.23749, val_acc: 0.96000\n",
      "Epoch [2730/10000], loss: 0.23959 acc: 0.98667 val_loss: 0.23710, val_acc: 0.96000\n",
      "Epoch [2740/10000], loss: 0.23916 acc: 0.98667 val_loss: 0.23670, val_acc: 0.96000\n",
      "Epoch [2750/10000], loss: 0.23874 acc: 0.98667 val_loss: 0.23631, val_acc: 0.96000\n",
      "Epoch [2760/10000], loss: 0.23831 acc: 0.98667 val_loss: 0.23592, val_acc: 0.96000\n",
      "Epoch [2770/10000], loss: 0.23789 acc: 0.98667 val_loss: 0.23553, val_acc: 0.96000\n",
      "Epoch [2780/10000], loss: 0.23747 acc: 0.98667 val_loss: 0.23515, val_acc: 0.96000\n",
      "Epoch [2790/10000], loss: 0.23705 acc: 0.98667 val_loss: 0.23476, val_acc: 0.96000\n",
      "Epoch [2800/10000], loss: 0.23663 acc: 0.98667 val_loss: 0.23438, val_acc: 0.96000\n",
      "Epoch [2810/10000], loss: 0.23622 acc: 0.98667 val_loss: 0.23400, val_acc: 0.96000\n",
      "Epoch [2820/10000], loss: 0.23580 acc: 0.98667 val_loss: 0.23363, val_acc: 0.96000\n",
      "Epoch [2830/10000], loss: 0.23539 acc: 0.98667 val_loss: 0.23325, val_acc: 0.96000\n",
      "Epoch [2840/10000], loss: 0.23498 acc: 0.98667 val_loss: 0.23287, val_acc: 0.96000\n",
      "Epoch [2850/10000], loss: 0.23458 acc: 0.98667 val_loss: 0.23250, val_acc: 0.96000\n",
      "Epoch [2860/10000], loss: 0.23417 acc: 0.98667 val_loss: 0.23213, val_acc: 0.96000\n",
      "Epoch [2870/10000], loss: 0.23377 acc: 0.98667 val_loss: 0.23176, val_acc: 0.96000\n",
      "Epoch [2880/10000], loss: 0.23337 acc: 0.98667 val_loss: 0.23140, val_acc: 0.96000\n",
      "Epoch [2890/10000], loss: 0.23297 acc: 0.98667 val_loss: 0.23103, val_acc: 0.96000\n",
      "Epoch [2900/10000], loss: 0.23257 acc: 0.98667 val_loss: 0.23067, val_acc: 0.96000\n",
      "Epoch [2910/10000], loss: 0.23218 acc: 0.98667 val_loss: 0.23031, val_acc: 0.96000\n",
      "Epoch [2920/10000], loss: 0.23178 acc: 0.98667 val_loss: 0.22995, val_acc: 0.96000\n",
      "Epoch [2930/10000], loss: 0.23139 acc: 0.98667 val_loss: 0.22959, val_acc: 0.96000\n",
      "Epoch [2940/10000], loss: 0.23100 acc: 0.98667 val_loss: 0.22923, val_acc: 0.96000\n",
      "Epoch [2950/10000], loss: 0.23061 acc: 0.98667 val_loss: 0.22888, val_acc: 0.96000\n",
      "Epoch [2960/10000], loss: 0.23023 acc: 0.98667 val_loss: 0.22853, val_acc: 0.96000\n",
      "Epoch [2970/10000], loss: 0.22984 acc: 0.98667 val_loss: 0.22818, val_acc: 0.96000\n",
      "Epoch [2980/10000], loss: 0.22946 acc: 0.98667 val_loss: 0.22783, val_acc: 0.96000\n",
      "Epoch [2990/10000], loss: 0.22908 acc: 0.98667 val_loss: 0.22748, val_acc: 0.96000\n",
      "Epoch [3000/10000], loss: 0.22870 acc: 0.98667 val_loss: 0.22713, val_acc: 0.96000\n",
      "Epoch [3010/10000], loss: 0.22832 acc: 0.98667 val_loss: 0.22679, val_acc: 0.96000\n",
      "Epoch [3020/10000], loss: 0.22795 acc: 0.98667 val_loss: 0.22645, val_acc: 0.96000\n",
      "Epoch [3030/10000], loss: 0.22757 acc: 0.98667 val_loss: 0.22610, val_acc: 0.96000\n",
      "Epoch [3040/10000], loss: 0.22720 acc: 0.98667 val_loss: 0.22577, val_acc: 0.96000\n",
      "Epoch [3050/10000], loss: 0.22683 acc: 0.98667 val_loss: 0.22543, val_acc: 0.96000\n",
      "Epoch [3060/10000], loss: 0.22646 acc: 0.98667 val_loss: 0.22509, val_acc: 0.96000\n",
      "Epoch [3070/10000], loss: 0.22610 acc: 0.98667 val_loss: 0.22476, val_acc: 0.96000\n",
      "Epoch [3080/10000], loss: 0.22573 acc: 0.98667 val_loss: 0.22442, val_acc: 0.96000\n",
      "Epoch [3090/10000], loss: 0.22537 acc: 0.98667 val_loss: 0.22409, val_acc: 0.96000\n",
      "Epoch [3100/10000], loss: 0.22501 acc: 0.98667 val_loss: 0.22376, val_acc: 0.96000\n",
      "Epoch [3110/10000], loss: 0.22465 acc: 0.98667 val_loss: 0.22343, val_acc: 0.96000\n",
      "Epoch [3120/10000], loss: 0.22429 acc: 0.98667 val_loss: 0.22311, val_acc: 0.96000\n",
      "Epoch [3130/10000], loss: 0.22393 acc: 0.98667 val_loss: 0.22278, val_acc: 0.96000\n",
      "Epoch [3140/10000], loss: 0.22357 acc: 0.98667 val_loss: 0.22246, val_acc: 0.96000\n",
      "Epoch [3150/10000], loss: 0.22322 acc: 0.98667 val_loss: 0.22214, val_acc: 0.96000\n",
      "Epoch [3160/10000], loss: 0.22287 acc: 0.98667 val_loss: 0.22181, val_acc: 0.96000\n",
      "Epoch [3170/10000], loss: 0.22252 acc: 0.98667 val_loss: 0.22150, val_acc: 0.96000\n",
      "Epoch [3180/10000], loss: 0.22217 acc: 0.98667 val_loss: 0.22118, val_acc: 0.96000\n",
      "Epoch [3190/10000], loss: 0.22182 acc: 0.98667 val_loss: 0.22086, val_acc: 0.96000\n",
      "Epoch [3200/10000], loss: 0.22148 acc: 0.98667 val_loss: 0.22055, val_acc: 0.96000\n",
      "Epoch [3210/10000], loss: 0.22113 acc: 0.98667 val_loss: 0.22023, val_acc: 0.96000\n",
      "Epoch [3220/10000], loss: 0.22079 acc: 0.98667 val_loss: 0.21992, val_acc: 0.96000\n",
      "Epoch [3230/10000], loss: 0.22045 acc: 0.98667 val_loss: 0.21961, val_acc: 0.96000\n",
      "Epoch [3240/10000], loss: 0.22011 acc: 0.98667 val_loss: 0.21930, val_acc: 0.96000\n",
      "Epoch [3250/10000], loss: 0.21977 acc: 0.98667 val_loss: 0.21899, val_acc: 0.96000\n",
      "Epoch [3260/10000], loss: 0.21943 acc: 0.98667 val_loss: 0.21869, val_acc: 0.96000\n",
      "Epoch [3270/10000], loss: 0.21910 acc: 0.98667 val_loss: 0.21838, val_acc: 0.96000\n",
      "Epoch [3280/10000], loss: 0.21876 acc: 0.98667 val_loss: 0.21808, val_acc: 0.96000\n",
      "Epoch [3290/10000], loss: 0.21843 acc: 0.98667 val_loss: 0.21778, val_acc: 0.96000\n",
      "Epoch [3300/10000], loss: 0.21810 acc: 0.98667 val_loss: 0.21747, val_acc: 0.96000\n",
      "Epoch [3310/10000], loss: 0.21777 acc: 0.98667 val_loss: 0.21717, val_acc: 0.96000\n",
      "Epoch [3320/10000], loss: 0.21744 acc: 0.98667 val_loss: 0.21688, val_acc: 0.96000\n",
      "Epoch [3330/10000], loss: 0.21711 acc: 0.98667 val_loss: 0.21658, val_acc: 0.96000\n",
      "Epoch [3340/10000], loss: 0.21679 acc: 0.98667 val_loss: 0.21628, val_acc: 0.96000\n",
      "Epoch [3350/10000], loss: 0.21646 acc: 0.98667 val_loss: 0.21599, val_acc: 0.96000\n",
      "Epoch [3360/10000], loss: 0.21614 acc: 0.98667 val_loss: 0.21570, val_acc: 0.96000\n",
      "Epoch [3370/10000], loss: 0.21582 acc: 0.98667 val_loss: 0.21540, val_acc: 0.96000\n",
      "Epoch [3380/10000], loss: 0.21550 acc: 0.98667 val_loss: 0.21511, val_acc: 0.96000\n",
      "Epoch [3390/10000], loss: 0.21518 acc: 0.98667 val_loss: 0.21483, val_acc: 0.96000\n",
      "Epoch [3400/10000], loss: 0.21487 acc: 0.98667 val_loss: 0.21454, val_acc: 0.96000\n",
      "Epoch [3410/10000], loss: 0.21455 acc: 0.98667 val_loss: 0.21425, val_acc: 0.96000\n",
      "Epoch [3420/10000], loss: 0.21424 acc: 0.98667 val_loss: 0.21396, val_acc: 0.96000\n",
      "Epoch [3430/10000], loss: 0.21392 acc: 0.98667 val_loss: 0.21368, val_acc: 0.96000\n",
      "Epoch [3440/10000], loss: 0.21361 acc: 0.98667 val_loss: 0.21340, val_acc: 0.96000\n",
      "Epoch [3450/10000], loss: 0.21330 acc: 0.98667 val_loss: 0.21312, val_acc: 0.96000\n",
      "Epoch [3460/10000], loss: 0.21299 acc: 0.98667 val_loss: 0.21284, val_acc: 0.96000\n",
      "Epoch [3470/10000], loss: 0.21268 acc: 0.98667 val_loss: 0.21256, val_acc: 0.96000\n",
      "Epoch [3480/10000], loss: 0.21238 acc: 0.98667 val_loss: 0.21228, val_acc: 0.96000\n",
      "Epoch [3490/10000], loss: 0.21207 acc: 0.98667 val_loss: 0.21200, val_acc: 0.96000\n",
      "Epoch [3500/10000], loss: 0.21177 acc: 0.98667 val_loss: 0.21173, val_acc: 0.96000\n",
      "Epoch [3510/10000], loss: 0.21146 acc: 0.98667 val_loss: 0.21145, val_acc: 0.96000\n",
      "Epoch [3520/10000], loss: 0.21116 acc: 0.98667 val_loss: 0.21118, val_acc: 0.96000\n",
      "Epoch [3530/10000], loss: 0.21086 acc: 0.98667 val_loss: 0.21091, val_acc: 0.96000\n",
      "Epoch [3540/10000], loss: 0.21056 acc: 0.98667 val_loss: 0.21064, val_acc: 0.96000\n",
      "Epoch [3550/10000], loss: 0.21026 acc: 0.98667 val_loss: 0.21037, val_acc: 0.96000\n",
      "Epoch [3560/10000], loss: 0.20997 acc: 0.98667 val_loss: 0.21010, val_acc: 0.96000\n",
      "Epoch [3570/10000], loss: 0.20967 acc: 0.98667 val_loss: 0.20983, val_acc: 0.96000\n",
      "Epoch [3580/10000], loss: 0.20938 acc: 0.98667 val_loss: 0.20956, val_acc: 0.96000\n",
      "Epoch [3590/10000], loss: 0.20909 acc: 0.98667 val_loss: 0.20930, val_acc: 0.96000\n",
      "Epoch [3600/10000], loss: 0.20879 acc: 0.98667 val_loss: 0.20903, val_acc: 0.96000\n",
      "Epoch [3610/10000], loss: 0.20850 acc: 0.98667 val_loss: 0.20877, val_acc: 0.96000\n",
      "Epoch [3620/10000], loss: 0.20821 acc: 0.98667 val_loss: 0.20851, val_acc: 0.96000\n",
      "Epoch [3630/10000], loss: 0.20793 acc: 0.98667 val_loss: 0.20825, val_acc: 0.96000\n",
      "Epoch [3640/10000], loss: 0.20764 acc: 0.98667 val_loss: 0.20799, val_acc: 0.96000\n",
      "Epoch [3650/10000], loss: 0.20735 acc: 0.98667 val_loss: 0.20773, val_acc: 0.96000\n",
      "Epoch [3660/10000], loss: 0.20707 acc: 0.98667 val_loss: 0.20747, val_acc: 0.96000\n",
      "Epoch [3670/10000], loss: 0.20678 acc: 0.98667 val_loss: 0.20721, val_acc: 0.96000\n",
      "Epoch [3680/10000], loss: 0.20650 acc: 0.98667 val_loss: 0.20696, val_acc: 0.96000\n",
      "Epoch [3690/10000], loss: 0.20622 acc: 0.98667 val_loss: 0.20670, val_acc: 0.96000\n",
      "Epoch [3700/10000], loss: 0.20594 acc: 0.98667 val_loss: 0.20645, val_acc: 0.96000\n",
      "Epoch [3710/10000], loss: 0.20566 acc: 0.98667 val_loss: 0.20620, val_acc: 0.96000\n",
      "Epoch [3720/10000], loss: 0.20538 acc: 0.98667 val_loss: 0.20595, val_acc: 0.96000\n",
      "Epoch [3730/10000], loss: 0.20511 acc: 0.98667 val_loss: 0.20570, val_acc: 0.96000\n",
      "Epoch [3740/10000], loss: 0.20483 acc: 0.98667 val_loss: 0.20545, val_acc: 0.96000\n",
      "Epoch [3750/10000], loss: 0.20455 acc: 0.98667 val_loss: 0.20520, val_acc: 0.96000\n",
      "Epoch [3760/10000], loss: 0.20428 acc: 0.98667 val_loss: 0.20495, val_acc: 0.96000\n",
      "Epoch [3770/10000], loss: 0.20401 acc: 0.98667 val_loss: 0.20471, val_acc: 0.96000\n",
      "Epoch [3780/10000], loss: 0.20374 acc: 0.98667 val_loss: 0.20446, val_acc: 0.96000\n",
      "Epoch [3790/10000], loss: 0.20347 acc: 0.98667 val_loss: 0.20422, val_acc: 0.96000\n",
      "Epoch [3800/10000], loss: 0.20320 acc: 0.98667 val_loss: 0.20397, val_acc: 0.96000\n",
      "Epoch [3810/10000], loss: 0.20293 acc: 0.98667 val_loss: 0.20373, val_acc: 0.96000\n",
      "Epoch [3820/10000], loss: 0.20266 acc: 0.98667 val_loss: 0.20349, val_acc: 0.96000\n",
      "Epoch [3830/10000], loss: 0.20239 acc: 0.98667 val_loss: 0.20325, val_acc: 0.96000\n",
      "Epoch [3840/10000], loss: 0.20213 acc: 0.98667 val_loss: 0.20301, val_acc: 0.96000\n",
      "Epoch [3850/10000], loss: 0.20186 acc: 0.98667 val_loss: 0.20277, val_acc: 0.96000\n",
      "Epoch [3860/10000], loss: 0.20160 acc: 0.98667 val_loss: 0.20253, val_acc: 0.96000\n",
      "Epoch [3870/10000], loss: 0.20134 acc: 0.98667 val_loss: 0.20230, val_acc: 0.96000\n",
      "Epoch [3880/10000], loss: 0.20108 acc: 0.98667 val_loss: 0.20206, val_acc: 0.96000\n",
      "Epoch [3890/10000], loss: 0.20082 acc: 0.98667 val_loss: 0.20183, val_acc: 0.96000\n",
      "Epoch [3900/10000], loss: 0.20056 acc: 0.98667 val_loss: 0.20159, val_acc: 0.96000\n",
      "Epoch [3910/10000], loss: 0.20030 acc: 0.98667 val_loss: 0.20136, val_acc: 0.96000\n",
      "Epoch [3920/10000], loss: 0.20004 acc: 0.98667 val_loss: 0.20113, val_acc: 0.96000\n",
      "Epoch [3930/10000], loss: 0.19979 acc: 0.98667 val_loss: 0.20090, val_acc: 0.96000\n",
      "Epoch [3940/10000], loss: 0.19953 acc: 0.98667 val_loss: 0.20067, val_acc: 0.96000\n",
      "Epoch [3950/10000], loss: 0.19928 acc: 0.98667 val_loss: 0.20044, val_acc: 0.96000\n",
      "Epoch [3960/10000], loss: 0.19902 acc: 0.98667 val_loss: 0.20021, val_acc: 0.96000\n",
      "Epoch [3970/10000], loss: 0.19877 acc: 0.98667 val_loss: 0.19998, val_acc: 0.96000\n",
      "Epoch [3980/10000], loss: 0.19852 acc: 0.98667 val_loss: 0.19976, val_acc: 0.96000\n",
      "Epoch [3990/10000], loss: 0.19827 acc: 0.98667 val_loss: 0.19953, val_acc: 0.96000\n",
      "Epoch [4000/10000], loss: 0.19802 acc: 0.98667 val_loss: 0.19931, val_acc: 0.96000\n",
      "Epoch [4010/10000], loss: 0.19777 acc: 0.98667 val_loss: 0.19908, val_acc: 0.96000\n",
      "Epoch [4020/10000], loss: 0.19752 acc: 0.98667 val_loss: 0.19886, val_acc: 0.96000\n",
      "Epoch [4030/10000], loss: 0.19728 acc: 0.98667 val_loss: 0.19864, val_acc: 0.96000\n",
      "Epoch [4040/10000], loss: 0.19703 acc: 0.98667 val_loss: 0.19842, val_acc: 0.96000\n",
      "Epoch [4050/10000], loss: 0.19679 acc: 0.98667 val_loss: 0.19820, val_acc: 0.96000\n",
      "Epoch [4060/10000], loss: 0.19654 acc: 0.98667 val_loss: 0.19798, val_acc: 0.96000\n",
      "Epoch [4070/10000], loss: 0.19630 acc: 0.98667 val_loss: 0.19776, val_acc: 0.96000\n",
      "Epoch [4080/10000], loss: 0.19606 acc: 0.98667 val_loss: 0.19754, val_acc: 0.96000\n",
      "Epoch [4090/10000], loss: 0.19582 acc: 0.98667 val_loss: 0.19732, val_acc: 0.96000\n",
      "Epoch [4100/10000], loss: 0.19557 acc: 0.98667 val_loss: 0.19711, val_acc: 0.96000\n",
      "Epoch [4110/10000], loss: 0.19534 acc: 0.98667 val_loss: 0.19689, val_acc: 0.96000\n",
      "Epoch [4120/10000], loss: 0.19510 acc: 0.98667 val_loss: 0.19668, val_acc: 0.96000\n",
      "Epoch [4130/10000], loss: 0.19486 acc: 0.98667 val_loss: 0.19646, val_acc: 0.96000\n",
      "Epoch [4140/10000], loss: 0.19462 acc: 0.98667 val_loss: 0.19625, val_acc: 0.96000\n",
      "Epoch [4150/10000], loss: 0.19439 acc: 0.98667 val_loss: 0.19604, val_acc: 0.96000\n",
      "Epoch [4160/10000], loss: 0.19415 acc: 0.98667 val_loss: 0.19583, val_acc: 0.96000\n",
      "Epoch [4170/10000], loss: 0.19392 acc: 0.98667 val_loss: 0.19562, val_acc: 0.96000\n",
      "Epoch [4180/10000], loss: 0.19368 acc: 0.98667 val_loss: 0.19541, val_acc: 0.96000\n",
      "Epoch [4190/10000], loss: 0.19345 acc: 0.98667 val_loss: 0.19520, val_acc: 0.96000\n",
      "Epoch [4200/10000], loss: 0.19322 acc: 0.98667 val_loss: 0.19499, val_acc: 0.96000\n",
      "Epoch [4210/10000], loss: 0.19299 acc: 0.98667 val_loss: 0.19478, val_acc: 0.96000\n",
      "Epoch [4220/10000], loss: 0.19276 acc: 0.98667 val_loss: 0.19457, val_acc: 0.96000\n",
      "Epoch [4230/10000], loss: 0.19253 acc: 0.98667 val_loss: 0.19437, val_acc: 0.96000\n",
      "Epoch [4240/10000], loss: 0.19230 acc: 0.98667 val_loss: 0.19416, val_acc: 0.96000\n",
      "Epoch [4250/10000], loss: 0.19207 acc: 0.98667 val_loss: 0.19396, val_acc: 0.96000\n",
      "Epoch [4260/10000], loss: 0.19184 acc: 0.98667 val_loss: 0.19376, val_acc: 0.96000\n",
      "Epoch [4270/10000], loss: 0.19162 acc: 0.98667 val_loss: 0.19355, val_acc: 0.96000\n",
      "Epoch [4280/10000], loss: 0.19139 acc: 0.98667 val_loss: 0.19335, val_acc: 0.96000\n",
      "Epoch [4290/10000], loss: 0.19117 acc: 0.98667 val_loss: 0.19315, val_acc: 0.96000\n",
      "Epoch [4300/10000], loss: 0.19094 acc: 0.98667 val_loss: 0.19295, val_acc: 0.96000\n",
      "Epoch [4310/10000], loss: 0.19072 acc: 0.98667 val_loss: 0.19275, val_acc: 0.96000\n",
      "Epoch [4320/10000], loss: 0.19050 acc: 0.98667 val_loss: 0.19255, val_acc: 0.96000\n",
      "Epoch [4330/10000], loss: 0.19028 acc: 0.98667 val_loss: 0.19235, val_acc: 0.96000\n",
      "Epoch [4340/10000], loss: 0.19006 acc: 0.98667 val_loss: 0.19215, val_acc: 0.96000\n",
      "Epoch [4350/10000], loss: 0.18984 acc: 0.98667 val_loss: 0.19196, val_acc: 0.96000\n",
      "Epoch [4360/10000], loss: 0.18962 acc: 0.98667 val_loss: 0.19176, val_acc: 0.96000\n",
      "Epoch [4370/10000], loss: 0.18940 acc: 0.98667 val_loss: 0.19156, val_acc: 0.96000\n",
      "Epoch [4380/10000], loss: 0.18918 acc: 0.98667 val_loss: 0.19137, val_acc: 0.96000\n",
      "Epoch [4390/10000], loss: 0.18897 acc: 0.98667 val_loss: 0.19118, val_acc: 0.96000\n",
      "Epoch [4400/10000], loss: 0.18875 acc: 0.98667 val_loss: 0.19098, val_acc: 0.96000\n",
      "Epoch [4410/10000], loss: 0.18853 acc: 0.98667 val_loss: 0.19079, val_acc: 0.96000\n",
      "Epoch [4420/10000], loss: 0.18832 acc: 0.98667 val_loss: 0.19060, val_acc: 0.96000\n",
      "Epoch [4430/10000], loss: 0.18811 acc: 0.98667 val_loss: 0.19041, val_acc: 0.96000\n",
      "Epoch [4440/10000], loss: 0.18789 acc: 0.98667 val_loss: 0.19021, val_acc: 0.96000\n",
      "Epoch [4450/10000], loss: 0.18768 acc: 0.98667 val_loss: 0.19002, val_acc: 0.96000\n",
      "Epoch [4460/10000], loss: 0.18747 acc: 0.98667 val_loss: 0.18984, val_acc: 0.96000\n",
      "Epoch [4470/10000], loss: 0.18726 acc: 0.98667 val_loss: 0.18965, val_acc: 0.96000\n",
      "Epoch [4480/10000], loss: 0.18705 acc: 0.98667 val_loss: 0.18946, val_acc: 0.96000\n",
      "Epoch [4490/10000], loss: 0.18684 acc: 0.98667 val_loss: 0.18927, val_acc: 0.96000\n",
      "Epoch [4500/10000], loss: 0.18663 acc: 0.98667 val_loss: 0.18908, val_acc: 0.96000\n",
      "Epoch [4510/10000], loss: 0.18642 acc: 0.98667 val_loss: 0.18890, val_acc: 0.96000\n",
      "Epoch [4520/10000], loss: 0.18622 acc: 0.98667 val_loss: 0.18871, val_acc: 0.96000\n",
      "Epoch [4530/10000], loss: 0.18601 acc: 0.98667 val_loss: 0.18853, val_acc: 0.96000\n",
      "Epoch [4540/10000], loss: 0.18580 acc: 0.98667 val_loss: 0.18834, val_acc: 0.96000\n",
      "Epoch [4550/10000], loss: 0.18560 acc: 0.98667 val_loss: 0.18816, val_acc: 0.96000\n",
      "Epoch [4560/10000], loss: 0.18539 acc: 0.98667 val_loss: 0.18798, val_acc: 0.96000\n",
      "Epoch [4570/10000], loss: 0.18519 acc: 0.98667 val_loss: 0.18780, val_acc: 0.96000\n",
      "Epoch [4580/10000], loss: 0.18499 acc: 0.98667 val_loss: 0.18762, val_acc: 0.96000\n",
      "Epoch [4590/10000], loss: 0.18478 acc: 0.98667 val_loss: 0.18743, val_acc: 0.96000\n",
      "Epoch [4600/10000], loss: 0.18458 acc: 0.98667 val_loss: 0.18725, val_acc: 0.96000\n",
      "Epoch [4610/10000], loss: 0.18438 acc: 0.98667 val_loss: 0.18707, val_acc: 0.96000\n",
      "Epoch [4620/10000], loss: 0.18418 acc: 0.98667 val_loss: 0.18690, val_acc: 0.96000\n",
      "Epoch [4630/10000], loss: 0.18398 acc: 0.98667 val_loss: 0.18672, val_acc: 0.96000\n",
      "Epoch [4640/10000], loss: 0.18378 acc: 0.98667 val_loss: 0.18654, val_acc: 0.96000\n",
      "Epoch [4650/10000], loss: 0.18358 acc: 0.98667 val_loss: 0.18636, val_acc: 0.96000\n",
      "Epoch [4660/10000], loss: 0.18339 acc: 0.98667 val_loss: 0.18619, val_acc: 0.96000\n",
      "Epoch [4670/10000], loss: 0.18319 acc: 0.98667 val_loss: 0.18601, val_acc: 0.96000\n",
      "Epoch [4680/10000], loss: 0.18299 acc: 0.98667 val_loss: 0.18583, val_acc: 0.96000\n",
      "Epoch [4690/10000], loss: 0.18280 acc: 0.98667 val_loss: 0.18566, val_acc: 0.96000\n",
      "Epoch [4700/10000], loss: 0.18260 acc: 0.98667 val_loss: 0.18549, val_acc: 0.96000\n",
      "Epoch [4710/10000], loss: 0.18241 acc: 0.98667 val_loss: 0.18531, val_acc: 0.96000\n",
      "Epoch [4720/10000], loss: 0.18221 acc: 0.98667 val_loss: 0.18514, val_acc: 0.96000\n",
      "Epoch [4730/10000], loss: 0.18202 acc: 0.98667 val_loss: 0.18497, val_acc: 0.96000\n",
      "Epoch [4740/10000], loss: 0.18183 acc: 0.98667 val_loss: 0.18479, val_acc: 0.96000\n",
      "Epoch [4750/10000], loss: 0.18164 acc: 0.98667 val_loss: 0.18462, val_acc: 0.96000\n",
      "Epoch [4760/10000], loss: 0.18144 acc: 0.98667 val_loss: 0.18445, val_acc: 0.96000\n",
      "Epoch [4770/10000], loss: 0.18125 acc: 0.98667 val_loss: 0.18428, val_acc: 0.96000\n",
      "Epoch [4780/10000], loss: 0.18106 acc: 0.98667 val_loss: 0.18411, val_acc: 0.96000\n",
      "Epoch [4790/10000], loss: 0.18087 acc: 0.98667 val_loss: 0.18395, val_acc: 0.96000\n",
      "Epoch [4800/10000], loss: 0.18068 acc: 0.98667 val_loss: 0.18378, val_acc: 0.96000\n",
      "Epoch [4810/10000], loss: 0.18050 acc: 0.98667 val_loss: 0.18361, val_acc: 0.96000\n",
      "Epoch [4820/10000], loss: 0.18031 acc: 0.98667 val_loss: 0.18344, val_acc: 0.96000\n",
      "Epoch [4830/10000], loss: 0.18012 acc: 0.98667 val_loss: 0.18328, val_acc: 0.96000\n",
      "Epoch [4840/10000], loss: 0.17994 acc: 0.98667 val_loss: 0.18311, val_acc: 0.96000\n",
      "Epoch [4850/10000], loss: 0.17975 acc: 0.98667 val_loss: 0.18294, val_acc: 0.96000\n",
      "Epoch [4860/10000], loss: 0.17956 acc: 0.98667 val_loss: 0.18278, val_acc: 0.96000\n",
      "Epoch [4870/10000], loss: 0.17938 acc: 0.98667 val_loss: 0.18261, val_acc: 0.96000\n",
      "Epoch [4880/10000], loss: 0.17920 acc: 0.98667 val_loss: 0.18245, val_acc: 0.96000\n",
      "Epoch [4890/10000], loss: 0.17901 acc: 0.98667 val_loss: 0.18229, val_acc: 0.96000\n",
      "Epoch [4900/10000], loss: 0.17883 acc: 0.98667 val_loss: 0.18212, val_acc: 0.96000\n",
      "Epoch [4910/10000], loss: 0.17865 acc: 0.98667 val_loss: 0.18196, val_acc: 0.96000\n",
      "Epoch [4920/10000], loss: 0.17846 acc: 0.98667 val_loss: 0.18180, val_acc: 0.96000\n",
      "Epoch [4930/10000], loss: 0.17828 acc: 0.98667 val_loss: 0.18164, val_acc: 0.96000\n",
      "Epoch [4940/10000], loss: 0.17810 acc: 0.98667 val_loss: 0.18148, val_acc: 0.96000\n",
      "Epoch [4950/10000], loss: 0.17792 acc: 0.98667 val_loss: 0.18132, val_acc: 0.96000\n",
      "Epoch [4960/10000], loss: 0.17774 acc: 0.98667 val_loss: 0.18116, val_acc: 0.96000\n",
      "Epoch [4970/10000], loss: 0.17756 acc: 0.98667 val_loss: 0.18100, val_acc: 0.96000\n",
      "Epoch [4980/10000], loss: 0.17739 acc: 0.98667 val_loss: 0.18084, val_acc: 0.96000\n",
      "Epoch [4990/10000], loss: 0.17721 acc: 0.98667 val_loss: 0.18068, val_acc: 0.96000\n",
      "Epoch [5000/10000], loss: 0.17703 acc: 0.98667 val_loss: 0.18053, val_acc: 0.96000\n",
      "Epoch [5010/10000], loss: 0.17685 acc: 0.98667 val_loss: 0.18037, val_acc: 0.96000\n",
      "Epoch [5020/10000], loss: 0.17668 acc: 0.98667 val_loss: 0.18021, val_acc: 0.96000\n",
      "Epoch [5030/10000], loss: 0.17650 acc: 0.98667 val_loss: 0.18006, val_acc: 0.96000\n",
      "Epoch [5040/10000], loss: 0.17633 acc: 0.98667 val_loss: 0.17990, val_acc: 0.96000\n",
      "Epoch [5050/10000], loss: 0.17615 acc: 0.98667 val_loss: 0.17975, val_acc: 0.96000\n",
      "Epoch [5060/10000], loss: 0.17598 acc: 0.98667 val_loss: 0.17959, val_acc: 0.96000\n",
      "Epoch [5070/10000], loss: 0.17581 acc: 0.98667 val_loss: 0.17944, val_acc: 0.96000\n",
      "Epoch [5080/10000], loss: 0.17563 acc: 0.98667 val_loss: 0.17928, val_acc: 0.96000\n",
      "Epoch [5090/10000], loss: 0.17546 acc: 0.98667 val_loss: 0.17913, val_acc: 0.96000\n",
      "Epoch [5100/10000], loss: 0.17529 acc: 0.98667 val_loss: 0.17898, val_acc: 0.96000\n",
      "Epoch [5110/10000], loss: 0.17512 acc: 0.98667 val_loss: 0.17883, val_acc: 0.96000\n",
      "Epoch [5120/10000], loss: 0.17495 acc: 0.98667 val_loss: 0.17867, val_acc: 0.96000\n",
      "Epoch [5130/10000], loss: 0.17478 acc: 0.98667 val_loss: 0.17852, val_acc: 0.96000\n",
      "Epoch [5140/10000], loss: 0.17461 acc: 0.98667 val_loss: 0.17837, val_acc: 0.96000\n",
      "Epoch [5150/10000], loss: 0.17444 acc: 0.98667 val_loss: 0.17822, val_acc: 0.96000\n",
      "Epoch [5160/10000], loss: 0.17427 acc: 0.98667 val_loss: 0.17807, val_acc: 0.96000\n",
      "Epoch [5170/10000], loss: 0.17410 acc: 0.98667 val_loss: 0.17792, val_acc: 0.96000\n",
      "Epoch [5180/10000], loss: 0.17393 acc: 0.98667 val_loss: 0.17778, val_acc: 0.96000\n",
      "Epoch [5190/10000], loss: 0.17377 acc: 0.98667 val_loss: 0.17763, val_acc: 0.96000\n",
      "Epoch [5200/10000], loss: 0.17360 acc: 0.98667 val_loss: 0.17748, val_acc: 0.96000\n",
      "Epoch [5210/10000], loss: 0.17343 acc: 0.98667 val_loss: 0.17733, val_acc: 0.96000\n",
      "Epoch [5220/10000], loss: 0.17327 acc: 0.98667 val_loss: 0.17719, val_acc: 0.96000\n",
      "Epoch [5230/10000], loss: 0.17310 acc: 0.98667 val_loss: 0.17704, val_acc: 0.96000\n",
      "Epoch [5240/10000], loss: 0.17294 acc: 0.98667 val_loss: 0.17689, val_acc: 0.96000\n",
      "Epoch [5250/10000], loss: 0.17277 acc: 0.98667 val_loss: 0.17675, val_acc: 0.96000\n",
      "Epoch [5260/10000], loss: 0.17261 acc: 0.98667 val_loss: 0.17660, val_acc: 0.96000\n",
      "Epoch [5270/10000], loss: 0.17245 acc: 0.98667 val_loss: 0.17646, val_acc: 0.96000\n",
      "Epoch [5280/10000], loss: 0.17229 acc: 0.98667 val_loss: 0.17631, val_acc: 0.96000\n",
      "Epoch [5290/10000], loss: 0.17212 acc: 0.98667 val_loss: 0.17617, val_acc: 0.96000\n",
      "Epoch [5300/10000], loss: 0.17196 acc: 0.98667 val_loss: 0.17603, val_acc: 0.96000\n",
      "Epoch [5310/10000], loss: 0.17180 acc: 0.98667 val_loss: 0.17589, val_acc: 0.96000\n",
      "Epoch [5320/10000], loss: 0.17164 acc: 0.98667 val_loss: 0.17574, val_acc: 0.96000\n",
      "Epoch [5330/10000], loss: 0.17148 acc: 0.98667 val_loss: 0.17560, val_acc: 0.96000\n",
      "Epoch [5340/10000], loss: 0.17132 acc: 0.98667 val_loss: 0.17546, val_acc: 0.96000\n",
      "Epoch [5350/10000], loss: 0.17116 acc: 0.98667 val_loss: 0.17532, val_acc: 0.96000\n",
      "Epoch [5360/10000], loss: 0.17100 acc: 0.98667 val_loss: 0.17518, val_acc: 0.96000\n",
      "Epoch [5370/10000], loss: 0.17084 acc: 0.98667 val_loss: 0.17504, val_acc: 0.96000\n",
      "Epoch [5380/10000], loss: 0.17068 acc: 0.98667 val_loss: 0.17490, val_acc: 0.96000\n",
      "Epoch [5390/10000], loss: 0.17053 acc: 0.98667 val_loss: 0.17476, val_acc: 0.96000\n",
      "Epoch [5400/10000], loss: 0.17037 acc: 0.98667 val_loss: 0.17462, val_acc: 0.96000\n",
      "Epoch [5410/10000], loss: 0.17021 acc: 0.98667 val_loss: 0.17448, val_acc: 0.96000\n",
      "Epoch [5420/10000], loss: 0.17006 acc: 0.98667 val_loss: 0.17434, val_acc: 0.96000\n",
      "Epoch [5430/10000], loss: 0.16990 acc: 0.98667 val_loss: 0.17421, val_acc: 0.96000\n",
      "Epoch [5440/10000], loss: 0.16975 acc: 0.98667 val_loss: 0.17407, val_acc: 0.96000\n",
      "Epoch [5450/10000], loss: 0.16959 acc: 0.98667 val_loss: 0.17393, val_acc: 0.96000\n",
      "Epoch [5460/10000], loss: 0.16944 acc: 0.98667 val_loss: 0.17380, val_acc: 0.96000\n",
      "Epoch [5470/10000], loss: 0.16928 acc: 0.98667 val_loss: 0.17366, val_acc: 0.96000\n",
      "Epoch [5480/10000], loss: 0.16913 acc: 0.98667 val_loss: 0.17352, val_acc: 0.96000\n",
      "Epoch [5490/10000], loss: 0.16898 acc: 0.98667 val_loss: 0.17339, val_acc: 0.96000\n",
      "Epoch [5500/10000], loss: 0.16883 acc: 0.98667 val_loss: 0.17325, val_acc: 0.96000\n",
      "Epoch [5510/10000], loss: 0.16867 acc: 0.98667 val_loss: 0.17312, val_acc: 0.96000\n",
      "Epoch [5520/10000], loss: 0.16852 acc: 0.98667 val_loss: 0.17299, val_acc: 0.96000\n",
      "Epoch [5530/10000], loss: 0.16837 acc: 0.98667 val_loss: 0.17285, val_acc: 0.96000\n",
      "Epoch [5540/10000], loss: 0.16822 acc: 0.98667 val_loss: 0.17272, val_acc: 0.96000\n",
      "Epoch [5550/10000], loss: 0.16807 acc: 0.98667 val_loss: 0.17259, val_acc: 0.96000\n",
      "Epoch [5560/10000], loss: 0.16792 acc: 0.98667 val_loss: 0.17246, val_acc: 0.96000\n",
      "Epoch [5570/10000], loss: 0.16777 acc: 0.98667 val_loss: 0.17232, val_acc: 0.96000\n",
      "Epoch [5580/10000], loss: 0.16762 acc: 0.98667 val_loss: 0.17219, val_acc: 0.96000\n",
      "Epoch [5590/10000], loss: 0.16747 acc: 0.98667 val_loss: 0.17206, val_acc: 0.96000\n",
      "Epoch [5600/10000], loss: 0.16732 acc: 0.98667 val_loss: 0.17193, val_acc: 0.96000\n",
      "Epoch [5610/10000], loss: 0.16718 acc: 0.98667 val_loss: 0.17180, val_acc: 0.96000\n",
      "Epoch [5620/10000], loss: 0.16703 acc: 0.98667 val_loss: 0.17167, val_acc: 0.96000\n",
      "Epoch [5630/10000], loss: 0.16688 acc: 0.98667 val_loss: 0.17154, val_acc: 0.96000\n",
      "Epoch [5640/10000], loss: 0.16674 acc: 0.98667 val_loss: 0.17141, val_acc: 0.96000\n",
      "Epoch [5650/10000], loss: 0.16659 acc: 0.98667 val_loss: 0.17128, val_acc: 0.96000\n",
      "Epoch [5660/10000], loss: 0.16644 acc: 0.98667 val_loss: 0.17115, val_acc: 0.96000\n",
      "Epoch [5670/10000], loss: 0.16630 acc: 0.98667 val_loss: 0.17103, val_acc: 0.96000\n",
      "Epoch [5680/10000], loss: 0.16615 acc: 0.98667 val_loss: 0.17090, val_acc: 0.96000\n",
      "Epoch [5690/10000], loss: 0.16601 acc: 0.98667 val_loss: 0.17077, val_acc: 0.96000\n",
      "Epoch [5700/10000], loss: 0.16587 acc: 0.98667 val_loss: 0.17064, val_acc: 0.96000\n",
      "Epoch [5710/10000], loss: 0.16572 acc: 0.98667 val_loss: 0.17052, val_acc: 0.96000\n",
      "Epoch [5720/10000], loss: 0.16558 acc: 0.98667 val_loss: 0.17039, val_acc: 0.96000\n",
      "Epoch [5730/10000], loss: 0.16544 acc: 0.98667 val_loss: 0.17027, val_acc: 0.96000\n",
      "Epoch [5740/10000], loss: 0.16529 acc: 0.98667 val_loss: 0.17014, val_acc: 0.96000\n",
      "Epoch [5750/10000], loss: 0.16515 acc: 0.98667 val_loss: 0.17001, val_acc: 0.96000\n",
      "Epoch [5760/10000], loss: 0.16501 acc: 0.98667 val_loss: 0.16989, val_acc: 0.96000\n",
      "Epoch [5770/10000], loss: 0.16487 acc: 0.98667 val_loss: 0.16977, val_acc: 0.96000\n",
      "Epoch [5780/10000], loss: 0.16473 acc: 0.98667 val_loss: 0.16964, val_acc: 0.96000\n",
      "Epoch [5790/10000], loss: 0.16459 acc: 0.98667 val_loss: 0.16952, val_acc: 0.96000\n",
      "Epoch [5800/10000], loss: 0.16445 acc: 0.98667 val_loss: 0.16939, val_acc: 0.96000\n",
      "Epoch [5810/10000], loss: 0.16431 acc: 0.98667 val_loss: 0.16927, val_acc: 0.96000\n",
      "Epoch [5820/10000], loss: 0.16417 acc: 0.98667 val_loss: 0.16915, val_acc: 0.96000\n",
      "Epoch [5830/10000], loss: 0.16403 acc: 0.98667 val_loss: 0.16903, val_acc: 0.96000\n",
      "Epoch [5840/10000], loss: 0.16389 acc: 0.98667 val_loss: 0.16891, val_acc: 0.96000\n",
      "Epoch [5850/10000], loss: 0.16375 acc: 0.98667 val_loss: 0.16878, val_acc: 0.96000\n",
      "Epoch [5860/10000], loss: 0.16361 acc: 0.98667 val_loss: 0.16866, val_acc: 0.96000\n",
      "Epoch [5870/10000], loss: 0.16348 acc: 0.98667 val_loss: 0.16854, val_acc: 0.96000\n",
      "Epoch [5880/10000], loss: 0.16334 acc: 0.98667 val_loss: 0.16842, val_acc: 0.96000\n",
      "Epoch [5890/10000], loss: 0.16320 acc: 0.98667 val_loss: 0.16830, val_acc: 0.96000\n",
      "Epoch [5900/10000], loss: 0.16307 acc: 0.98667 val_loss: 0.16818, val_acc: 0.96000\n",
      "Epoch [5910/10000], loss: 0.16293 acc: 0.98667 val_loss: 0.16806, val_acc: 0.96000\n",
      "Epoch [5920/10000], loss: 0.16280 acc: 0.98667 val_loss: 0.16794, val_acc: 0.96000\n",
      "Epoch [5930/10000], loss: 0.16266 acc: 0.98667 val_loss: 0.16782, val_acc: 0.96000\n",
      "Epoch [5940/10000], loss: 0.16253 acc: 0.98667 val_loss: 0.16771, val_acc: 0.96000\n",
      "Epoch [5950/10000], loss: 0.16239 acc: 0.98667 val_loss: 0.16759, val_acc: 0.96000\n",
      "Epoch [5960/10000], loss: 0.16226 acc: 0.98667 val_loss: 0.16747, val_acc: 0.96000\n",
      "Epoch [5970/10000], loss: 0.16212 acc: 0.98667 val_loss: 0.16735, val_acc: 0.96000\n",
      "Epoch [5980/10000], loss: 0.16199 acc: 0.98667 val_loss: 0.16724, val_acc: 0.96000\n",
      "Epoch [5990/10000], loss: 0.16186 acc: 0.98667 val_loss: 0.16712, val_acc: 0.96000\n",
      "Epoch [6000/10000], loss: 0.16172 acc: 0.98667 val_loss: 0.16700, val_acc: 0.96000\n",
      "Epoch [6010/10000], loss: 0.16159 acc: 0.98667 val_loss: 0.16689, val_acc: 0.96000\n",
      "Epoch [6020/10000], loss: 0.16146 acc: 0.98667 val_loss: 0.16677, val_acc: 0.96000\n",
      "Epoch [6030/10000], loss: 0.16133 acc: 0.98667 val_loss: 0.16665, val_acc: 0.96000\n",
      "Epoch [6040/10000], loss: 0.16120 acc: 0.98667 val_loss: 0.16654, val_acc: 0.96000\n",
      "Epoch [6050/10000], loss: 0.16107 acc: 0.98667 val_loss: 0.16642, val_acc: 0.96000\n",
      "Epoch [6060/10000], loss: 0.16093 acc: 0.98667 val_loss: 0.16631, val_acc: 0.96000\n",
      "Epoch [6070/10000], loss: 0.16080 acc: 0.98667 val_loss: 0.16620, val_acc: 0.96000\n",
      "Epoch [6080/10000], loss: 0.16067 acc: 0.98667 val_loss: 0.16608, val_acc: 0.96000\n",
      "Epoch [6090/10000], loss: 0.16055 acc: 0.98667 val_loss: 0.16597, val_acc: 0.96000\n",
      "Epoch [6100/10000], loss: 0.16042 acc: 0.98667 val_loss: 0.16585, val_acc: 0.96000\n",
      "Epoch [6110/10000], loss: 0.16029 acc: 0.98667 val_loss: 0.16574, val_acc: 0.96000\n",
      "Epoch [6120/10000], loss: 0.16016 acc: 0.98667 val_loss: 0.16563, val_acc: 0.96000\n",
      "Epoch [6130/10000], loss: 0.16003 acc: 0.98667 val_loss: 0.16552, val_acc: 0.96000\n",
      "Epoch [6140/10000], loss: 0.15990 acc: 0.98667 val_loss: 0.16540, val_acc: 0.96000\n",
      "Epoch [6150/10000], loss: 0.15978 acc: 0.98667 val_loss: 0.16529, val_acc: 0.96000\n",
      "Epoch [6160/10000], loss: 0.15965 acc: 0.98667 val_loss: 0.16518, val_acc: 0.96000\n",
      "Epoch [6170/10000], loss: 0.15952 acc: 0.98667 val_loss: 0.16507, val_acc: 0.96000\n",
      "Epoch [6180/10000], loss: 0.15939 acc: 0.98667 val_loss: 0.16496, val_acc: 0.96000\n",
      "Epoch [6190/10000], loss: 0.15927 acc: 0.98667 val_loss: 0.16485, val_acc: 0.96000\n",
      "Epoch [6200/10000], loss: 0.15914 acc: 0.98667 val_loss: 0.16474, val_acc: 0.96000\n",
      "Epoch [6210/10000], loss: 0.15902 acc: 0.98667 val_loss: 0.16463, val_acc: 0.96000\n",
      "Epoch [6220/10000], loss: 0.15889 acc: 0.98667 val_loss: 0.16452, val_acc: 0.96000\n",
      "Epoch [6230/10000], loss: 0.15877 acc: 0.98667 val_loss: 0.16441, val_acc: 0.96000\n",
      "Epoch [6240/10000], loss: 0.15864 acc: 0.98667 val_loss: 0.16430, val_acc: 0.96000\n",
      "Epoch [6250/10000], loss: 0.15852 acc: 0.98667 val_loss: 0.16419, val_acc: 0.96000\n",
      "Epoch [6260/10000], loss: 0.15839 acc: 0.98667 val_loss: 0.16408, val_acc: 0.96000\n",
      "Epoch [6270/10000], loss: 0.15827 acc: 0.98667 val_loss: 0.16398, val_acc: 0.96000\n",
      "Epoch [6280/10000], loss: 0.15815 acc: 0.98667 val_loss: 0.16387, val_acc: 0.96000\n",
      "Epoch [6290/10000], loss: 0.15802 acc: 0.98667 val_loss: 0.16376, val_acc: 0.96000\n",
      "Epoch [6300/10000], loss: 0.15790 acc: 0.98667 val_loss: 0.16365, val_acc: 0.96000\n",
      "Epoch [6310/10000], loss: 0.15778 acc: 0.98667 val_loss: 0.16355, val_acc: 0.96000\n",
      "Epoch [6320/10000], loss: 0.15766 acc: 0.98667 val_loss: 0.16344, val_acc: 0.96000\n",
      "Epoch [6330/10000], loss: 0.15754 acc: 0.98667 val_loss: 0.16333, val_acc: 0.96000\n",
      "Epoch [6340/10000], loss: 0.15741 acc: 0.98667 val_loss: 0.16323, val_acc: 0.96000\n",
      "Epoch [6350/10000], loss: 0.15729 acc: 0.98667 val_loss: 0.16312, val_acc: 0.96000\n",
      "Epoch [6360/10000], loss: 0.15717 acc: 0.98667 val_loss: 0.16302, val_acc: 0.96000\n",
      "Epoch [6370/10000], loss: 0.15705 acc: 0.98667 val_loss: 0.16291, val_acc: 0.96000\n",
      "Epoch [6380/10000], loss: 0.15693 acc: 0.98667 val_loss: 0.16281, val_acc: 0.96000\n",
      "Epoch [6390/10000], loss: 0.15681 acc: 0.98667 val_loss: 0.16270, val_acc: 0.96000\n",
      "Epoch [6400/10000], loss: 0.15669 acc: 0.98667 val_loss: 0.16260, val_acc: 0.96000\n",
      "Epoch [6410/10000], loss: 0.15657 acc: 0.98667 val_loss: 0.16249, val_acc: 0.96000\n",
      "Epoch [6420/10000], loss: 0.15645 acc: 0.98667 val_loss: 0.16239, val_acc: 0.96000\n",
      "Epoch [6430/10000], loss: 0.15634 acc: 0.98667 val_loss: 0.16228, val_acc: 0.96000\n",
      "Epoch [6440/10000], loss: 0.15622 acc: 0.98667 val_loss: 0.16218, val_acc: 0.96000\n",
      "Epoch [6450/10000], loss: 0.15610 acc: 0.98667 val_loss: 0.16208, val_acc: 0.96000\n",
      "Epoch [6460/10000], loss: 0.15598 acc: 0.98667 val_loss: 0.16198, val_acc: 0.96000\n",
      "Epoch [6470/10000], loss: 0.15586 acc: 0.98667 val_loss: 0.16187, val_acc: 0.96000\n",
      "Epoch [6480/10000], loss: 0.15575 acc: 0.98667 val_loss: 0.16177, val_acc: 0.96000\n",
      "Epoch [6490/10000], loss: 0.15563 acc: 0.98667 val_loss: 0.16167, val_acc: 0.96000\n",
      "Epoch [6500/10000], loss: 0.15551 acc: 0.98667 val_loss: 0.16157, val_acc: 0.96000\n",
      "Epoch [6510/10000], loss: 0.15540 acc: 0.98667 val_loss: 0.16147, val_acc: 0.96000\n",
      "Epoch [6520/10000], loss: 0.15528 acc: 0.98667 val_loss: 0.16136, val_acc: 0.96000\n",
      "Epoch [6530/10000], loss: 0.15516 acc: 0.98667 val_loss: 0.16126, val_acc: 0.96000\n",
      "Epoch [6540/10000], loss: 0.15505 acc: 0.98667 val_loss: 0.16116, val_acc: 0.96000\n",
      "Epoch [6550/10000], loss: 0.15493 acc: 0.98667 val_loss: 0.16106, val_acc: 0.96000\n",
      "Epoch [6560/10000], loss: 0.15482 acc: 0.98667 val_loss: 0.16096, val_acc: 0.96000\n",
      "Epoch [6570/10000], loss: 0.15470 acc: 0.98667 val_loss: 0.16086, val_acc: 0.96000\n",
      "Epoch [6580/10000], loss: 0.15459 acc: 0.98667 val_loss: 0.16076, val_acc: 0.96000\n",
      "Epoch [6590/10000], loss: 0.15448 acc: 0.98667 val_loss: 0.16066, val_acc: 0.96000\n",
      "Epoch [6600/10000], loss: 0.15436 acc: 0.98667 val_loss: 0.16056, val_acc: 0.96000\n",
      "Epoch [6610/10000], loss: 0.15425 acc: 0.98667 val_loss: 0.16047, val_acc: 0.96000\n",
      "Epoch [6620/10000], loss: 0.15414 acc: 0.98667 val_loss: 0.16037, val_acc: 0.96000\n",
      "Epoch [6630/10000], loss: 0.15402 acc: 0.98667 val_loss: 0.16027, val_acc: 0.96000\n",
      "Epoch [6640/10000], loss: 0.15391 acc: 0.98667 val_loss: 0.16017, val_acc: 0.96000\n",
      "Epoch [6650/10000], loss: 0.15380 acc: 0.98667 val_loss: 0.16007, val_acc: 0.96000\n",
      "Epoch [6660/10000], loss: 0.15369 acc: 0.98667 val_loss: 0.15997, val_acc: 0.96000\n",
      "Epoch [6670/10000], loss: 0.15357 acc: 0.98667 val_loss: 0.15988, val_acc: 0.96000\n",
      "Epoch [6680/10000], loss: 0.15346 acc: 0.98667 val_loss: 0.15978, val_acc: 0.96000\n",
      "Epoch [6690/10000], loss: 0.15335 acc: 0.98667 val_loss: 0.15968, val_acc: 0.96000\n",
      "Epoch [6700/10000], loss: 0.15324 acc: 0.98667 val_loss: 0.15959, val_acc: 0.96000\n",
      "Epoch [6710/10000], loss: 0.15313 acc: 0.98667 val_loss: 0.15949, val_acc: 0.96000\n",
      "Epoch [6720/10000], loss: 0.15302 acc: 0.98667 val_loss: 0.15939, val_acc: 0.96000\n",
      "Epoch [6730/10000], loss: 0.15291 acc: 0.98667 val_loss: 0.15930, val_acc: 0.96000\n",
      "Epoch [6740/10000], loss: 0.15280 acc: 0.98667 val_loss: 0.15920, val_acc: 0.96000\n",
      "Epoch [6750/10000], loss: 0.15269 acc: 0.98667 val_loss: 0.15911, val_acc: 0.96000\n",
      "Epoch [6760/10000], loss: 0.15258 acc: 0.98667 val_loss: 0.15901, val_acc: 0.96000\n",
      "Epoch [6770/10000], loss: 0.15247 acc: 0.98667 val_loss: 0.15892, val_acc: 0.96000\n",
      "Epoch [6780/10000], loss: 0.15236 acc: 0.98667 val_loss: 0.15882, val_acc: 0.96000\n",
      "Epoch [6790/10000], loss: 0.15225 acc: 0.98667 val_loss: 0.15873, val_acc: 0.96000\n",
      "Epoch [6800/10000], loss: 0.15214 acc: 0.98667 val_loss: 0.15863, val_acc: 0.96000\n",
      "Epoch [6810/10000], loss: 0.15204 acc: 0.98667 val_loss: 0.15854, val_acc: 0.96000\n",
      "Epoch [6820/10000], loss: 0.15193 acc: 0.98667 val_loss: 0.15845, val_acc: 0.96000\n",
      "Epoch [6830/10000], loss: 0.15182 acc: 0.98667 val_loss: 0.15835, val_acc: 0.96000\n",
      "Epoch [6840/10000], loss: 0.15171 acc: 0.98667 val_loss: 0.15826, val_acc: 0.96000\n",
      "Epoch [6850/10000], loss: 0.15161 acc: 0.98667 val_loss: 0.15817, val_acc: 0.96000\n",
      "Epoch [6860/10000], loss: 0.15150 acc: 0.98667 val_loss: 0.15807, val_acc: 0.96000\n",
      "Epoch [6870/10000], loss: 0.15139 acc: 0.98667 val_loss: 0.15798, val_acc: 0.96000\n",
      "Epoch [6880/10000], loss: 0.15129 acc: 0.98667 val_loss: 0.15789, val_acc: 0.96000\n",
      "Epoch [6890/10000], loss: 0.15118 acc: 0.98667 val_loss: 0.15780, val_acc: 0.96000\n",
      "Epoch [6900/10000], loss: 0.15108 acc: 0.98667 val_loss: 0.15771, val_acc: 0.96000\n",
      "Epoch [6910/10000], loss: 0.15097 acc: 0.98667 val_loss: 0.15761, val_acc: 0.96000\n",
      "Epoch [6920/10000], loss: 0.15086 acc: 0.98667 val_loss: 0.15752, val_acc: 0.96000\n",
      "Epoch [6930/10000], loss: 0.15076 acc: 0.98667 val_loss: 0.15743, val_acc: 0.96000\n",
      "Epoch [6940/10000], loss: 0.15065 acc: 0.98667 val_loss: 0.15734, val_acc: 0.96000\n",
      "Epoch [6950/10000], loss: 0.15055 acc: 0.98667 val_loss: 0.15725, val_acc: 0.96000\n",
      "Epoch [6960/10000], loss: 0.15045 acc: 0.98667 val_loss: 0.15716, val_acc: 0.96000\n",
      "Epoch [6970/10000], loss: 0.15034 acc: 0.98667 val_loss: 0.15707, val_acc: 0.96000\n",
      "Epoch [6980/10000], loss: 0.15024 acc: 0.98667 val_loss: 0.15698, val_acc: 0.96000\n",
      "Epoch [6990/10000], loss: 0.15013 acc: 0.98667 val_loss: 0.15689, val_acc: 0.96000\n",
      "Epoch [7000/10000], loss: 0.15003 acc: 0.98667 val_loss: 0.15680, val_acc: 0.96000\n",
      "Epoch [7010/10000], loss: 0.14993 acc: 0.98667 val_loss: 0.15671, val_acc: 0.96000\n",
      "Epoch [7020/10000], loss: 0.14983 acc: 0.98667 val_loss: 0.15662, val_acc: 0.96000\n",
      "Epoch [7030/10000], loss: 0.14972 acc: 0.98667 val_loss: 0.15653, val_acc: 0.96000\n",
      "Epoch [7040/10000], loss: 0.14962 acc: 0.98667 val_loss: 0.15644, val_acc: 0.96000\n",
      "Epoch [7050/10000], loss: 0.14952 acc: 0.98667 val_loss: 0.15636, val_acc: 0.96000\n",
      "Epoch [7060/10000], loss: 0.14942 acc: 0.98667 val_loss: 0.15627, val_acc: 0.96000\n",
      "Epoch [7070/10000], loss: 0.14931 acc: 0.98667 val_loss: 0.15618, val_acc: 0.96000\n",
      "Epoch [7080/10000], loss: 0.14921 acc: 0.98667 val_loss: 0.15609, val_acc: 0.96000\n",
      "Epoch [7090/10000], loss: 0.14911 acc: 0.98667 val_loss: 0.15600, val_acc: 0.96000\n",
      "Epoch [7100/10000], loss: 0.14901 acc: 0.98667 val_loss: 0.15592, val_acc: 0.96000\n",
      "Epoch [7110/10000], loss: 0.14891 acc: 0.98667 val_loss: 0.15583, val_acc: 0.96000\n",
      "Epoch [7120/10000], loss: 0.14881 acc: 0.98667 val_loss: 0.15574, val_acc: 0.96000\n",
      "Epoch [7130/10000], loss: 0.14871 acc: 0.98667 val_loss: 0.15565, val_acc: 0.96000\n",
      "Epoch [7140/10000], loss: 0.14861 acc: 0.98667 val_loss: 0.15557, val_acc: 0.96000\n",
      "Epoch [7150/10000], loss: 0.14851 acc: 0.98667 val_loss: 0.15548, val_acc: 0.96000\n",
      "Epoch [7160/10000], loss: 0.14841 acc: 0.98667 val_loss: 0.15540, val_acc: 0.96000\n",
      "Epoch [7170/10000], loss: 0.14831 acc: 0.98667 val_loss: 0.15531, val_acc: 0.96000\n",
      "Epoch [7180/10000], loss: 0.14821 acc: 0.98667 val_loss: 0.15522, val_acc: 0.96000\n",
      "Epoch [7190/10000], loss: 0.14811 acc: 0.98667 val_loss: 0.15514, val_acc: 0.96000\n",
      "Epoch [7200/10000], loss: 0.14801 acc: 0.98667 val_loss: 0.15505, val_acc: 0.96000\n",
      "Epoch [7210/10000], loss: 0.14792 acc: 0.98667 val_loss: 0.15497, val_acc: 0.96000\n",
      "Epoch [7220/10000], loss: 0.14782 acc: 0.98667 val_loss: 0.15488, val_acc: 0.96000\n",
      "Epoch [7230/10000], loss: 0.14772 acc: 0.98667 val_loss: 0.15480, val_acc: 0.96000\n",
      "Epoch [7240/10000], loss: 0.14762 acc: 0.98667 val_loss: 0.15471, val_acc: 0.96000\n",
      "Epoch [7250/10000], loss: 0.14752 acc: 0.98667 val_loss: 0.15463, val_acc: 0.96000\n",
      "Epoch [7260/10000], loss: 0.14743 acc: 0.98667 val_loss: 0.15455, val_acc: 0.96000\n",
      "Epoch [7270/10000], loss: 0.14733 acc: 0.98667 val_loss: 0.15446, val_acc: 0.96000\n",
      "Epoch [7280/10000], loss: 0.14723 acc: 0.98667 val_loss: 0.15438, val_acc: 0.96000\n",
      "Epoch [7290/10000], loss: 0.14714 acc: 0.98667 val_loss: 0.15429, val_acc: 0.96000\n",
      "Epoch [7300/10000], loss: 0.14704 acc: 0.98667 val_loss: 0.15421, val_acc: 0.96000\n",
      "Epoch [7310/10000], loss: 0.14694 acc: 0.98667 val_loss: 0.15413, val_acc: 0.96000\n",
      "Epoch [7320/10000], loss: 0.14685 acc: 0.98667 val_loss: 0.15404, val_acc: 0.96000\n",
      "Epoch [7330/10000], loss: 0.14675 acc: 0.98667 val_loss: 0.15396, val_acc: 0.96000\n",
      "Epoch [7340/10000], loss: 0.14666 acc: 0.98667 val_loss: 0.15388, val_acc: 0.96000\n",
      "Epoch [7350/10000], loss: 0.14656 acc: 0.98667 val_loss: 0.15380, val_acc: 0.96000\n",
      "Epoch [7360/10000], loss: 0.14646 acc: 0.98667 val_loss: 0.15371, val_acc: 0.96000\n",
      "Epoch [7370/10000], loss: 0.14637 acc: 0.98667 val_loss: 0.15363, val_acc: 0.96000\n",
      "Epoch [7380/10000], loss: 0.14627 acc: 0.98667 val_loss: 0.15355, val_acc: 0.96000\n",
      "Epoch [7390/10000], loss: 0.14618 acc: 0.98667 val_loss: 0.15347, val_acc: 0.96000\n",
      "Epoch [7400/10000], loss: 0.14609 acc: 0.98667 val_loss: 0.15339, val_acc: 0.96000\n",
      "Epoch [7410/10000], loss: 0.14599 acc: 0.98667 val_loss: 0.15331, val_acc: 0.96000\n",
      "Epoch [7420/10000], loss: 0.14590 acc: 0.98667 val_loss: 0.15323, val_acc: 0.96000\n",
      "Epoch [7430/10000], loss: 0.14580 acc: 0.98667 val_loss: 0.15314, val_acc: 0.96000\n",
      "Epoch [7440/10000], loss: 0.14571 acc: 0.98667 val_loss: 0.15306, val_acc: 0.96000\n",
      "Epoch [7450/10000], loss: 0.14562 acc: 0.98667 val_loss: 0.15298, val_acc: 0.96000\n",
      "Epoch [7460/10000], loss: 0.14552 acc: 0.98667 val_loss: 0.15290, val_acc: 0.96000\n",
      "Epoch [7470/10000], loss: 0.14543 acc: 0.98667 val_loss: 0.15282, val_acc: 0.96000\n",
      "Epoch [7480/10000], loss: 0.14534 acc: 0.98667 val_loss: 0.15274, val_acc: 0.96000\n",
      "Epoch [7490/10000], loss: 0.14525 acc: 0.98667 val_loss: 0.15266, val_acc: 0.96000\n",
      "Epoch [7500/10000], loss: 0.14515 acc: 0.98667 val_loss: 0.15258, val_acc: 0.96000\n",
      "Epoch [7510/10000], loss: 0.14506 acc: 0.98667 val_loss: 0.15250, val_acc: 0.96000\n",
      "Epoch [7520/10000], loss: 0.14497 acc: 0.98667 val_loss: 0.15243, val_acc: 0.96000\n",
      "Epoch [7530/10000], loss: 0.14488 acc: 0.98667 val_loss: 0.15235, val_acc: 0.96000\n",
      "Epoch [7540/10000], loss: 0.14479 acc: 0.98667 val_loss: 0.15227, val_acc: 0.96000\n",
      "Epoch [7550/10000], loss: 0.14470 acc: 0.98667 val_loss: 0.15219, val_acc: 0.96000\n",
      "Epoch [7560/10000], loss: 0.14460 acc: 0.98667 val_loss: 0.15211, val_acc: 0.96000\n",
      "Epoch [7570/10000], loss: 0.14451 acc: 0.98667 val_loss: 0.15203, val_acc: 0.96000\n",
      "Epoch [7580/10000], loss: 0.14442 acc: 0.98667 val_loss: 0.15195, val_acc: 0.96000\n",
      "Epoch [7590/10000], loss: 0.14433 acc: 0.98667 val_loss: 0.15188, val_acc: 0.96000\n",
      "Epoch [7600/10000], loss: 0.14424 acc: 0.98667 val_loss: 0.15180, val_acc: 0.96000\n",
      "Epoch [7610/10000], loss: 0.14415 acc: 0.98667 val_loss: 0.15172, val_acc: 0.96000\n",
      "Epoch [7620/10000], loss: 0.14406 acc: 0.98667 val_loss: 0.15164, val_acc: 0.96000\n",
      "Epoch [7630/10000], loss: 0.14397 acc: 0.98667 val_loss: 0.15157, val_acc: 0.96000\n",
      "Epoch [7640/10000], loss: 0.14388 acc: 0.98667 val_loss: 0.15149, val_acc: 0.96000\n",
      "Epoch [7650/10000], loss: 0.14379 acc: 0.98667 val_loss: 0.15141, val_acc: 0.96000\n",
      "Epoch [7660/10000], loss: 0.14370 acc: 0.98667 val_loss: 0.15134, val_acc: 0.96000\n",
      "Epoch [7670/10000], loss: 0.14362 acc: 0.98667 val_loss: 0.15126, val_acc: 0.96000\n",
      "Epoch [7680/10000], loss: 0.14353 acc: 0.98667 val_loss: 0.15118, val_acc: 0.96000\n",
      "Epoch [7690/10000], loss: 0.14344 acc: 0.98667 val_loss: 0.15111, val_acc: 0.96000\n",
      "Epoch [7700/10000], loss: 0.14335 acc: 0.98667 val_loss: 0.15103, val_acc: 0.96000\n",
      "Epoch [7710/10000], loss: 0.14326 acc: 0.98667 val_loss: 0.15096, val_acc: 0.96000\n",
      "Epoch [7720/10000], loss: 0.14317 acc: 0.98667 val_loss: 0.15088, val_acc: 0.96000\n",
      "Epoch [7730/10000], loss: 0.14309 acc: 0.98667 val_loss: 0.15080, val_acc: 0.96000\n",
      "Epoch [7740/10000], loss: 0.14300 acc: 0.98667 val_loss: 0.15073, val_acc: 0.96000\n",
      "Epoch [7750/10000], loss: 0.14291 acc: 0.98667 val_loss: 0.15065, val_acc: 0.96000\n",
      "Epoch [7760/10000], loss: 0.14282 acc: 0.98667 val_loss: 0.15058, val_acc: 0.96000\n",
      "Epoch [7770/10000], loss: 0.14274 acc: 0.98667 val_loss: 0.15050, val_acc: 0.96000\n",
      "Epoch [7780/10000], loss: 0.14265 acc: 0.98667 val_loss: 0.15043, val_acc: 0.96000\n",
      "Epoch [7790/10000], loss: 0.14256 acc: 0.98667 val_loss: 0.15036, val_acc: 0.96000\n",
      "Epoch [7800/10000], loss: 0.14248 acc: 0.98667 val_loss: 0.15028, val_acc: 0.96000\n",
      "Epoch [7810/10000], loss: 0.14239 acc: 0.98667 val_loss: 0.15021, val_acc: 0.96000\n",
      "Epoch [7820/10000], loss: 0.14230 acc: 0.98667 val_loss: 0.15013, val_acc: 0.96000\n",
      "Epoch [7830/10000], loss: 0.14222 acc: 0.98667 val_loss: 0.15006, val_acc: 0.96000\n",
      "Epoch [7840/10000], loss: 0.14213 acc: 0.98667 val_loss: 0.14999, val_acc: 0.96000\n",
      "Epoch [7850/10000], loss: 0.14205 acc: 0.98667 val_loss: 0.14991, val_acc: 0.96000\n",
      "Epoch [7860/10000], loss: 0.14196 acc: 0.98667 val_loss: 0.14984, val_acc: 0.96000\n",
      "Epoch [7870/10000], loss: 0.14188 acc: 0.98667 val_loss: 0.14977, val_acc: 0.96000\n",
      "Epoch [7880/10000], loss: 0.14179 acc: 0.98667 val_loss: 0.14969, val_acc: 0.96000\n",
      "Epoch [7890/10000], loss: 0.14171 acc: 0.98667 val_loss: 0.14962, val_acc: 0.96000\n",
      "Epoch [7900/10000], loss: 0.14162 acc: 0.98667 val_loss: 0.14955, val_acc: 0.96000\n",
      "Epoch [7910/10000], loss: 0.14154 acc: 0.98667 val_loss: 0.14948, val_acc: 0.96000\n",
      "Epoch [7920/10000], loss: 0.14145 acc: 0.98667 val_loss: 0.14940, val_acc: 0.96000\n",
      "Epoch [7930/10000], loss: 0.14137 acc: 0.98667 val_loss: 0.14933, val_acc: 0.96000\n",
      "Epoch [7940/10000], loss: 0.14128 acc: 0.98667 val_loss: 0.14926, val_acc: 0.96000\n",
      "Epoch [7950/10000], loss: 0.14120 acc: 0.98667 val_loss: 0.14919, val_acc: 0.96000\n",
      "Epoch [7960/10000], loss: 0.14112 acc: 0.98667 val_loss: 0.14912, val_acc: 0.96000\n",
      "Epoch [7970/10000], loss: 0.14103 acc: 0.98667 val_loss: 0.14904, val_acc: 0.96000\n",
      "Epoch [7980/10000], loss: 0.14095 acc: 0.98667 val_loss: 0.14897, val_acc: 0.96000\n",
      "Epoch [7990/10000], loss: 0.14087 acc: 0.98667 val_loss: 0.14890, val_acc: 0.96000\n",
      "Epoch [8000/10000], loss: 0.14078 acc: 0.98667 val_loss: 0.14883, val_acc: 0.96000\n",
      "Epoch [8010/10000], loss: 0.14070 acc: 0.98667 val_loss: 0.14876, val_acc: 0.96000\n",
      "Epoch [8020/10000], loss: 0.14062 acc: 0.98667 val_loss: 0.14869, val_acc: 0.96000\n",
      "Epoch [8030/10000], loss: 0.14054 acc: 0.98667 val_loss: 0.14862, val_acc: 0.96000\n",
      "Epoch [8040/10000], loss: 0.14045 acc: 0.98667 val_loss: 0.14855, val_acc: 0.96000\n",
      "Epoch [8050/10000], loss: 0.14037 acc: 0.98667 val_loss: 0.14848, val_acc: 0.96000\n",
      "Epoch [8060/10000], loss: 0.14029 acc: 0.98667 val_loss: 0.14841, val_acc: 0.96000\n",
      "Epoch [8070/10000], loss: 0.14021 acc: 0.98667 val_loss: 0.14834, val_acc: 0.96000\n",
      "Epoch [8080/10000], loss: 0.14013 acc: 0.98667 val_loss: 0.14827, val_acc: 0.96000\n",
      "Epoch [8090/10000], loss: 0.14004 acc: 0.98667 val_loss: 0.14820, val_acc: 0.96000\n",
      "Epoch [8100/10000], loss: 0.13996 acc: 0.98667 val_loss: 0.14813, val_acc: 0.96000\n",
      "Epoch [8110/10000], loss: 0.13988 acc: 0.98667 val_loss: 0.14806, val_acc: 0.96000\n",
      "Epoch [8120/10000], loss: 0.13980 acc: 0.98667 val_loss: 0.14799, val_acc: 0.96000\n",
      "Epoch [8130/10000], loss: 0.13972 acc: 0.98667 val_loss: 0.14792, val_acc: 0.96000\n",
      "Epoch [8140/10000], loss: 0.13964 acc: 0.98667 val_loss: 0.14785, val_acc: 0.96000\n",
      "Epoch [8150/10000], loss: 0.13956 acc: 0.98667 val_loss: 0.14778, val_acc: 0.96000\n",
      "Epoch [8160/10000], loss: 0.13948 acc: 0.98667 val_loss: 0.14771, val_acc: 0.96000\n",
      "Epoch [8170/10000], loss: 0.13940 acc: 0.98667 val_loss: 0.14765, val_acc: 0.96000\n",
      "Epoch [8180/10000], loss: 0.13932 acc: 0.98667 val_loss: 0.14758, val_acc: 0.96000\n",
      "Epoch [8190/10000], loss: 0.13924 acc: 0.98667 val_loss: 0.14751, val_acc: 0.96000\n",
      "Epoch [8200/10000], loss: 0.13916 acc: 0.98667 val_loss: 0.14744, val_acc: 0.96000\n",
      "Epoch [8210/10000], loss: 0.13908 acc: 0.98667 val_loss: 0.14737, val_acc: 0.96000\n",
      "Epoch [8220/10000], loss: 0.13900 acc: 0.98667 val_loss: 0.14731, val_acc: 0.96000\n",
      "Epoch [8230/10000], loss: 0.13892 acc: 0.98667 val_loss: 0.14724, val_acc: 0.96000\n",
      "Epoch [8240/10000], loss: 0.13884 acc: 0.98667 val_loss: 0.14717, val_acc: 0.96000\n",
      "Epoch [8250/10000], loss: 0.13876 acc: 0.98667 val_loss: 0.14710, val_acc: 0.96000\n",
      "Epoch [8260/10000], loss: 0.13869 acc: 0.98667 val_loss: 0.14704, val_acc: 0.96000\n",
      "Epoch [8270/10000], loss: 0.13861 acc: 0.98667 val_loss: 0.14697, val_acc: 0.96000\n",
      "Epoch [8280/10000], loss: 0.13853 acc: 0.98667 val_loss: 0.14690, val_acc: 0.96000\n",
      "Epoch [8290/10000], loss: 0.13845 acc: 0.98667 val_loss: 0.14684, val_acc: 0.96000\n",
      "Epoch [8300/10000], loss: 0.13837 acc: 0.98667 val_loss: 0.14677, val_acc: 0.96000\n",
      "Epoch [8310/10000], loss: 0.13829 acc: 0.98667 val_loss: 0.14670, val_acc: 0.96000\n",
      "Epoch [8320/10000], loss: 0.13822 acc: 0.98667 val_loss: 0.14664, val_acc: 0.96000\n",
      "Epoch [8330/10000], loss: 0.13814 acc: 0.98667 val_loss: 0.14657, val_acc: 0.96000\n",
      "Epoch [8340/10000], loss: 0.13806 acc: 0.98667 val_loss: 0.14650, val_acc: 0.96000\n",
      "Epoch [8350/10000], loss: 0.13798 acc: 0.98667 val_loss: 0.14644, val_acc: 0.96000\n",
      "Epoch [8360/10000], loss: 0.13791 acc: 0.98667 val_loss: 0.14637, val_acc: 0.96000\n",
      "Epoch [8370/10000], loss: 0.13783 acc: 0.98667 val_loss: 0.14631, val_acc: 0.96000\n",
      "Epoch [8380/10000], loss: 0.13775 acc: 0.98667 val_loss: 0.14624, val_acc: 0.96000\n",
      "Epoch [8390/10000], loss: 0.13768 acc: 0.98667 val_loss: 0.14618, val_acc: 0.96000\n",
      "Epoch [8400/10000], loss: 0.13760 acc: 0.98667 val_loss: 0.14611, val_acc: 0.96000\n",
      "Epoch [8410/10000], loss: 0.13752 acc: 0.98667 val_loss: 0.14605, val_acc: 0.96000\n",
      "Epoch [8420/10000], loss: 0.13745 acc: 0.98667 val_loss: 0.14598, val_acc: 0.96000\n",
      "Epoch [8430/10000], loss: 0.13737 acc: 0.98667 val_loss: 0.14592, val_acc: 0.96000\n",
      "Epoch [8440/10000], loss: 0.13730 acc: 0.98667 val_loss: 0.14585, val_acc: 0.96000\n",
      "Epoch [8450/10000], loss: 0.13722 acc: 0.98667 val_loss: 0.14579, val_acc: 0.96000\n",
      "Epoch [8460/10000], loss: 0.13714 acc: 0.98667 val_loss: 0.14572, val_acc: 0.96000\n",
      "Epoch [8470/10000], loss: 0.13707 acc: 0.98667 val_loss: 0.14566, val_acc: 0.96000\n",
      "Epoch [8480/10000], loss: 0.13699 acc: 0.98667 val_loss: 0.14559, val_acc: 0.96000\n",
      "Epoch [8490/10000], loss: 0.13692 acc: 0.98667 val_loss: 0.14553, val_acc: 0.96000\n",
      "Epoch [8500/10000], loss: 0.13684 acc: 0.98667 val_loss: 0.14547, val_acc: 0.96000\n",
      "Epoch [8510/10000], loss: 0.13677 acc: 0.98667 val_loss: 0.14540, val_acc: 0.96000\n",
      "Epoch [8520/10000], loss: 0.13669 acc: 0.98667 val_loss: 0.14534, val_acc: 0.96000\n",
      "Epoch [8530/10000], loss: 0.13662 acc: 0.98667 val_loss: 0.14528, val_acc: 0.96000\n",
      "Epoch [8540/10000], loss: 0.13654 acc: 0.98667 val_loss: 0.14521, val_acc: 0.96000\n",
      "Epoch [8550/10000], loss: 0.13647 acc: 0.98667 val_loss: 0.14515, val_acc: 0.96000\n",
      "Epoch [8560/10000], loss: 0.13640 acc: 0.98667 val_loss: 0.14509, val_acc: 0.96000\n",
      "Epoch [8570/10000], loss: 0.13632 acc: 0.98667 val_loss: 0.14502, val_acc: 0.96000\n",
      "Epoch [8580/10000], loss: 0.13625 acc: 0.98667 val_loss: 0.14496, val_acc: 0.96000\n",
      "Epoch [8590/10000], loss: 0.13617 acc: 0.98667 val_loss: 0.14490, val_acc: 0.96000\n",
      "Epoch [8600/10000], loss: 0.13610 acc: 0.98667 val_loss: 0.14483, val_acc: 0.96000\n",
      "Epoch [8610/10000], loss: 0.13603 acc: 0.98667 val_loss: 0.14477, val_acc: 0.96000\n",
      "Epoch [8620/10000], loss: 0.13595 acc: 0.98667 val_loss: 0.14471, val_acc: 0.96000\n",
      "Epoch [8630/10000], loss: 0.13588 acc: 0.98667 val_loss: 0.14465, val_acc: 0.96000\n",
      "Epoch [8640/10000], loss: 0.13581 acc: 0.98667 val_loss: 0.14459, val_acc: 0.96000\n",
      "Epoch [8650/10000], loss: 0.13574 acc: 0.98667 val_loss: 0.14452, val_acc: 0.96000\n",
      "Epoch [8660/10000], loss: 0.13566 acc: 0.98667 val_loss: 0.14446, val_acc: 0.96000\n",
      "Epoch [8670/10000], loss: 0.13559 acc: 0.98667 val_loss: 0.14440, val_acc: 0.96000\n",
      "Epoch [8680/10000], loss: 0.13552 acc: 0.98667 val_loss: 0.14434, val_acc: 0.96000\n",
      "Epoch [8690/10000], loss: 0.13545 acc: 0.98667 val_loss: 0.14428, val_acc: 0.96000\n",
      "Epoch [8700/10000], loss: 0.13537 acc: 0.98667 val_loss: 0.14422, val_acc: 0.96000\n",
      "Epoch [8710/10000], loss: 0.13530 acc: 0.98667 val_loss: 0.14415, val_acc: 0.96000\n",
      "Epoch [8720/10000], loss: 0.13523 acc: 0.98667 val_loss: 0.14409, val_acc: 0.96000\n",
      "Epoch [8730/10000], loss: 0.13516 acc: 0.98667 val_loss: 0.14403, val_acc: 0.96000\n",
      "Epoch [8740/10000], loss: 0.13509 acc: 0.98667 val_loss: 0.14397, val_acc: 0.96000\n",
      "Epoch [8750/10000], loss: 0.13501 acc: 0.98667 val_loss: 0.14391, val_acc: 0.96000\n",
      "Epoch [8760/10000], loss: 0.13494 acc: 0.98667 val_loss: 0.14385, val_acc: 0.96000\n",
      "Epoch [8770/10000], loss: 0.13487 acc: 0.98667 val_loss: 0.14379, val_acc: 0.96000\n",
      "Epoch [8780/10000], loss: 0.13480 acc: 0.98667 val_loss: 0.14373, val_acc: 0.96000\n",
      "Epoch [8790/10000], loss: 0.13473 acc: 0.98667 val_loss: 0.14367, val_acc: 0.96000\n",
      "Epoch [8800/10000], loss: 0.13466 acc: 0.98667 val_loss: 0.14361, val_acc: 0.96000\n",
      "Epoch [8810/10000], loss: 0.13459 acc: 0.98667 val_loss: 0.14355, val_acc: 0.96000\n",
      "Epoch [8820/10000], loss: 0.13452 acc: 0.98667 val_loss: 0.14349, val_acc: 0.96000\n",
      "Epoch [8830/10000], loss: 0.13445 acc: 0.98667 val_loss: 0.14343, val_acc: 0.96000\n",
      "Epoch [8840/10000], loss: 0.13438 acc: 0.98667 val_loss: 0.14337, val_acc: 0.96000\n",
      "Epoch [8850/10000], loss: 0.13431 acc: 0.98667 val_loss: 0.14331, val_acc: 0.96000\n",
      "Epoch [8860/10000], loss: 0.13424 acc: 0.98667 val_loss: 0.14325, val_acc: 0.96000\n",
      "Epoch [8870/10000], loss: 0.13417 acc: 0.98667 val_loss: 0.14319, val_acc: 0.96000\n",
      "Epoch [8880/10000], loss: 0.13410 acc: 0.98667 val_loss: 0.14313, val_acc: 0.96000\n",
      "Epoch [8890/10000], loss: 0.13403 acc: 0.98667 val_loss: 0.14308, val_acc: 0.96000\n",
      "Epoch [8900/10000], loss: 0.13396 acc: 0.98667 val_loss: 0.14302, val_acc: 0.96000\n",
      "Epoch [8910/10000], loss: 0.13389 acc: 0.98667 val_loss: 0.14296, val_acc: 0.96000\n",
      "Epoch [8920/10000], loss: 0.13382 acc: 0.98667 val_loss: 0.14290, val_acc: 0.96000\n",
      "Epoch [8930/10000], loss: 0.13375 acc: 0.98667 val_loss: 0.14284, val_acc: 0.96000\n",
      "Epoch [8940/10000], loss: 0.13368 acc: 0.98667 val_loss: 0.14278, val_acc: 0.96000\n",
      "Epoch [8950/10000], loss: 0.13361 acc: 0.98667 val_loss: 0.14272, val_acc: 0.96000\n",
      "Epoch [8960/10000], loss: 0.13354 acc: 0.98667 val_loss: 0.14266, val_acc: 0.96000\n",
      "Epoch [8970/10000], loss: 0.13347 acc: 0.98667 val_loss: 0.14261, val_acc: 0.96000\n",
      "Epoch [8980/10000], loss: 0.13341 acc: 0.98667 val_loss: 0.14255, val_acc: 0.96000\n",
      "Epoch [8990/10000], loss: 0.13334 acc: 0.98667 val_loss: 0.14249, val_acc: 0.96000\n",
      "Epoch [9000/10000], loss: 0.13327 acc: 0.98667 val_loss: 0.14243, val_acc: 0.96000\n",
      "Epoch [9010/10000], loss: 0.13320 acc: 0.98667 val_loss: 0.14238, val_acc: 0.96000\n",
      "Epoch [9020/10000], loss: 0.13313 acc: 0.98667 val_loss: 0.14232, val_acc: 0.96000\n",
      "Epoch [9030/10000], loss: 0.13307 acc: 0.98667 val_loss: 0.14226, val_acc: 0.96000\n",
      "Epoch [9040/10000], loss: 0.13300 acc: 0.98667 val_loss: 0.14220, val_acc: 0.96000\n",
      "Epoch [9050/10000], loss: 0.13293 acc: 0.98667 val_loss: 0.14215, val_acc: 0.96000\n",
      "Epoch [9060/10000], loss: 0.13286 acc: 0.98667 val_loss: 0.14209, val_acc: 0.96000\n",
      "Epoch [9070/10000], loss: 0.13280 acc: 0.98667 val_loss: 0.14203, val_acc: 0.96000\n",
      "Epoch [9080/10000], loss: 0.13273 acc: 0.98667 val_loss: 0.14198, val_acc: 0.96000\n",
      "Epoch [9090/10000], loss: 0.13266 acc: 0.98667 val_loss: 0.14192, val_acc: 0.96000\n",
      "Epoch [9100/10000], loss: 0.13259 acc: 0.98667 val_loss: 0.14186, val_acc: 0.96000\n",
      "Epoch [9110/10000], loss: 0.13253 acc: 0.98667 val_loss: 0.14181, val_acc: 0.96000\n",
      "Epoch [9120/10000], loss: 0.13246 acc: 0.98667 val_loss: 0.14175, val_acc: 0.96000\n",
      "Epoch [9130/10000], loss: 0.13239 acc: 0.98667 val_loss: 0.14169, val_acc: 0.96000\n",
      "Epoch [9140/10000], loss: 0.13233 acc: 0.98667 val_loss: 0.14164, val_acc: 0.96000\n",
      "Epoch [9150/10000], loss: 0.13226 acc: 0.98667 val_loss: 0.14158, val_acc: 0.96000\n",
      "Epoch [9160/10000], loss: 0.13220 acc: 0.98667 val_loss: 0.14153, val_acc: 0.96000\n",
      "Epoch [9170/10000], loss: 0.13213 acc: 0.98667 val_loss: 0.14147, val_acc: 0.96000\n",
      "Epoch [9180/10000], loss: 0.13206 acc: 0.98667 val_loss: 0.14141, val_acc: 0.96000\n",
      "Epoch [9190/10000], loss: 0.13200 acc: 0.98667 val_loss: 0.14136, val_acc: 0.96000\n",
      "Epoch [9200/10000], loss: 0.13193 acc: 0.98667 val_loss: 0.14130, val_acc: 0.96000\n",
      "Epoch [9210/10000], loss: 0.13187 acc: 0.98667 val_loss: 0.14125, val_acc: 0.96000\n",
      "Epoch [9220/10000], loss: 0.13180 acc: 0.98667 val_loss: 0.14119, val_acc: 0.96000\n",
      "Epoch [9230/10000], loss: 0.13174 acc: 0.98667 val_loss: 0.14114, val_acc: 0.96000\n",
      "Epoch [9240/10000], loss: 0.13167 acc: 0.98667 val_loss: 0.14108, val_acc: 0.96000\n",
      "Epoch [9250/10000], loss: 0.13160 acc: 0.98667 val_loss: 0.14103, val_acc: 0.96000\n",
      "Epoch [9260/10000], loss: 0.13154 acc: 0.98667 val_loss: 0.14097, val_acc: 0.96000\n",
      "Epoch [9270/10000], loss: 0.13147 acc: 0.98667 val_loss: 0.14092, val_acc: 0.96000\n",
      "Epoch [9280/10000], loss: 0.13141 acc: 0.98667 val_loss: 0.14086, val_acc: 0.96000\n",
      "Epoch [9290/10000], loss: 0.13135 acc: 0.98667 val_loss: 0.14081, val_acc: 0.96000\n",
      "Epoch [9300/10000], loss: 0.13128 acc: 0.98667 val_loss: 0.14075, val_acc: 0.96000\n",
      "Epoch [9310/10000], loss: 0.13122 acc: 0.98667 val_loss: 0.14070, val_acc: 0.96000\n",
      "Epoch [9320/10000], loss: 0.13115 acc: 0.98667 val_loss: 0.14065, val_acc: 0.96000\n",
      "Epoch [9330/10000], loss: 0.13109 acc: 0.98667 val_loss: 0.14059, val_acc: 0.96000\n",
      "Epoch [9340/10000], loss: 0.13102 acc: 0.98667 val_loss: 0.14054, val_acc: 0.96000\n",
      "Epoch [9350/10000], loss: 0.13096 acc: 0.98667 val_loss: 0.14048, val_acc: 0.96000\n",
      "Epoch [9360/10000], loss: 0.13090 acc: 0.98667 val_loss: 0.14043, val_acc: 0.96000\n",
      "Epoch [9370/10000], loss: 0.13083 acc: 0.98667 val_loss: 0.14038, val_acc: 0.96000\n",
      "Epoch [9380/10000], loss: 0.13077 acc: 0.98667 val_loss: 0.14032, val_acc: 0.96000\n",
      "Epoch [9390/10000], loss: 0.13070 acc: 0.98667 val_loss: 0.14027, val_acc: 0.96000\n",
      "Epoch [9400/10000], loss: 0.13064 acc: 0.98667 val_loss: 0.14022, val_acc: 0.96000\n",
      "Epoch [9410/10000], loss: 0.13058 acc: 0.98667 val_loss: 0.14016, val_acc: 0.96000\n",
      "Epoch [9420/10000], loss: 0.13051 acc: 0.98667 val_loss: 0.14011, val_acc: 0.96000\n",
      "Epoch [9430/10000], loss: 0.13045 acc: 0.98667 val_loss: 0.14006, val_acc: 0.96000\n",
      "Epoch [9440/10000], loss: 0.13039 acc: 0.98667 val_loss: 0.14000, val_acc: 0.96000\n",
      "Epoch [9450/10000], loss: 0.13033 acc: 0.98667 val_loss: 0.13995, val_acc: 0.96000\n",
      "Epoch [9460/10000], loss: 0.13026 acc: 0.98667 val_loss: 0.13990, val_acc: 0.96000\n",
      "Epoch [9470/10000], loss: 0.13020 acc: 0.98667 val_loss: 0.13984, val_acc: 0.96000\n",
      "Epoch [9480/10000], loss: 0.13014 acc: 0.98667 val_loss: 0.13979, val_acc: 0.96000\n",
      "Epoch [9490/10000], loss: 0.13008 acc: 0.98667 val_loss: 0.13974, val_acc: 0.96000\n",
      "Epoch [9500/10000], loss: 0.13001 acc: 0.98667 val_loss: 0.13969, val_acc: 0.96000\n",
      "Epoch [9510/10000], loss: 0.12995 acc: 0.98667 val_loss: 0.13963, val_acc: 0.96000\n",
      "Epoch [9520/10000], loss: 0.12989 acc: 0.98667 val_loss: 0.13958, val_acc: 0.96000\n",
      "Epoch [9530/10000], loss: 0.12983 acc: 0.98667 val_loss: 0.13953, val_acc: 0.96000\n",
      "Epoch [9540/10000], loss: 0.12976 acc: 0.98667 val_loss: 0.13948, val_acc: 0.96000\n",
      "Epoch [9550/10000], loss: 0.12970 acc: 0.98667 val_loss: 0.13943, val_acc: 0.96000\n",
      "Epoch [9560/10000], loss: 0.12964 acc: 0.98667 val_loss: 0.13937, val_acc: 0.96000\n",
      "Epoch [9570/10000], loss: 0.12958 acc: 0.98667 val_loss: 0.13932, val_acc: 0.96000\n",
      "Epoch [9580/10000], loss: 0.12952 acc: 0.98667 val_loss: 0.13927, val_acc: 0.96000\n",
      "Epoch [9590/10000], loss: 0.12946 acc: 0.98667 val_loss: 0.13922, val_acc: 0.96000\n",
      "Epoch [9600/10000], loss: 0.12940 acc: 0.98667 val_loss: 0.13917, val_acc: 0.96000\n",
      "Epoch [9610/10000], loss: 0.12933 acc: 0.98667 val_loss: 0.13912, val_acc: 0.96000\n",
      "Epoch [9620/10000], loss: 0.12927 acc: 0.98667 val_loss: 0.13907, val_acc: 0.96000\n",
      "Epoch [9630/10000], loss: 0.12921 acc: 0.98667 val_loss: 0.13901, val_acc: 0.96000\n",
      "Epoch [9640/10000], loss: 0.12915 acc: 0.98667 val_loss: 0.13896, val_acc: 0.96000\n",
      "Epoch [9650/10000], loss: 0.12909 acc: 0.98667 val_loss: 0.13891, val_acc: 0.96000\n",
      "Epoch [9660/10000], loss: 0.12903 acc: 0.98667 val_loss: 0.13886, val_acc: 0.96000\n",
      "Epoch [9670/10000], loss: 0.12897 acc: 0.98667 val_loss: 0.13881, val_acc: 0.96000\n",
      "Epoch [9680/10000], loss: 0.12891 acc: 0.98667 val_loss: 0.13876, val_acc: 0.96000\n",
      "Epoch [9690/10000], loss: 0.12885 acc: 0.98667 val_loss: 0.13871, val_acc: 0.96000\n",
      "Epoch [9700/10000], loss: 0.12879 acc: 0.98667 val_loss: 0.13866, val_acc: 0.96000\n",
      "Epoch [9710/10000], loss: 0.12873 acc: 0.98667 val_loss: 0.13861, val_acc: 0.96000\n",
      "Epoch [9720/10000], loss: 0.12867 acc: 0.98667 val_loss: 0.13856, val_acc: 0.96000\n",
      "Epoch [9730/10000], loss: 0.12861 acc: 0.98667 val_loss: 0.13851, val_acc: 0.96000\n",
      "Epoch [9740/10000], loss: 0.12855 acc: 0.98667 val_loss: 0.13846, val_acc: 0.96000\n",
      "Epoch [9750/10000], loss: 0.12849 acc: 0.98667 val_loss: 0.13841, val_acc: 0.96000\n",
      "Epoch [9760/10000], loss: 0.12843 acc: 0.98667 val_loss: 0.13836, val_acc: 0.96000\n",
      "Epoch [9770/10000], loss: 0.12837 acc: 0.98667 val_loss: 0.13831, val_acc: 0.96000\n",
      "Epoch [9780/10000], loss: 0.12831 acc: 0.98667 val_loss: 0.13826, val_acc: 0.96000\n",
      "Epoch [9790/10000], loss: 0.12825 acc: 0.98667 val_loss: 0.13821, val_acc: 0.96000\n",
      "Epoch [9800/10000], loss: 0.12819 acc: 0.98667 val_loss: 0.13816, val_acc: 0.96000\n",
      "Epoch [9810/10000], loss: 0.12813 acc: 0.98667 val_loss: 0.13811, val_acc: 0.96000\n",
      "Epoch [9820/10000], loss: 0.12808 acc: 0.98667 val_loss: 0.13806, val_acc: 0.96000\n",
      "Epoch [9830/10000], loss: 0.12802 acc: 0.98667 val_loss: 0.13801, val_acc: 0.96000\n",
      "Epoch [9840/10000], loss: 0.12796 acc: 0.98667 val_loss: 0.13796, val_acc: 0.96000\n",
      "Epoch [9850/10000], loss: 0.12790 acc: 0.98667 val_loss: 0.13791, val_acc: 0.96000\n",
      "Epoch [9860/10000], loss: 0.12784 acc: 0.98667 val_loss: 0.13786, val_acc: 0.96000\n",
      "Epoch [9870/10000], loss: 0.12778 acc: 0.98667 val_loss: 0.13782, val_acc: 0.96000\n",
      "Epoch [9880/10000], loss: 0.12772 acc: 0.98667 val_loss: 0.13777, val_acc: 0.96000\n",
      "Epoch [9890/10000], loss: 0.12767 acc: 0.98667 val_loss: 0.13772, val_acc: 0.96000\n",
      "Epoch [9900/10000], loss: 0.12761 acc: 0.98667 val_loss: 0.13767, val_acc: 0.96000\n",
      "Epoch [9910/10000], loss: 0.12755 acc: 0.98667 val_loss: 0.13762, val_acc: 0.96000\n",
      "Epoch [9920/10000], loss: 0.12749 acc: 0.98667 val_loss: 0.13757, val_acc: 0.96000\n",
      "Epoch [9930/10000], loss: 0.12743 acc: 0.98667 val_loss: 0.13752, val_acc: 0.96000\n",
      "Epoch [9940/10000], loss: 0.12738 acc: 0.98667 val_loss: 0.13748, val_acc: 0.96000\n",
      "Epoch [9950/10000], loss: 0.12732 acc: 0.98667 val_loss: 0.13743, val_acc: 0.96000\n",
      "Epoch [9960/10000], loss: 0.12726 acc: 0.98667 val_loss: 0.13738, val_acc: 0.96000\n",
      "Epoch [9970/10000], loss: 0.12720 acc: 0.98667 val_loss: 0.13733, val_acc: 0.96000\n",
      "Epoch [9980/10000], loss: 0.12715 acc: 0.98667 val_loss: 0.13728, val_acc: 0.96000\n",
      "Epoch [9990/10000], loss: 0.12709 acc: 0.98667 val_loss: 0.13724, val_acc: 0.96000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # 훈련 페이즈\n",
    "\n",
    "    # 경사 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 예측 계산\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    # 여기서 로그 함수를 적용함\n",
    "    outputs2 = torch.log(outputs)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = criterion(outputs2, labels)\n",
    "\n",
    "    # 경사 계산\n",
    "    loss.backward()\n",
    "\n",
    "    # 파라미터 수정\n",
    "    optimizer.step()\n",
    "\n",
    "    # 예측 라벨 산출\n",
    "    predicted = torch.max(outputs, 1)[1]\n",
    "\n",
    "    # 손실과 정확도 계산\n",
    "    train_loss = loss.item()\n",
    "    train_acc = (predicted == labels).sum()  / len(labels)\n",
    "\n",
    "    # 예측 페이즈\n",
    "\n",
    "    # 예측 계산\n",
    "    outputs_test = net(inputs_test)\n",
    "\n",
    "    # 여기서 로그 함수를 적용함\n",
    "    outputs2_test = torch.log(outputs_test)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss_test = criterion(outputs2_test, labels_test)\n",
    "\n",
    "    # 예측 라벨 산출\n",
    "    predicted_test = torch.max(outputs_test, 1)[1]\n",
    "\n",
    "    # 손실과 정확도 계산\n",
    "    val_loss =  loss_test.item()\n",
    "    val_acc =  (predicted_test == labels_test).sum() / len(labels_test)\n",
    "\n",
    "    if ( epoch % 10 == 0):\n",
    "        print (f'Epoch [{epoch}/{num_epochs}], loss: {train_loss:.5f} acc: {train_acc:.5f} val_loss: {val_loss:.5f}, val_acc: {val_acc:.5f}')\n",
    "        item = np.array([epoch , train_loss, train_acc, val_loss, val_acc])\n",
    "        history = np.vstack((history, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1739157014639,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "Yz8_XNjW0UHB",
    "outputId": "8c8fc5e6-805a-4d62-a2f8-a74ae98df077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기상태 : 손실 : 1.09158  정확도 : 0.26667\n",
      "최종상태 : 손실 : 0.13724  정확도 : 0.96000\n"
     ]
    }
   ],
   "source": [
    "# 손실과 정확도 확인\n",
    "\n",
    "print(f'초기상태 : 손실 : {history[0,3]:.5f}  정확도 : {history[0,4]:.5f}' )\n",
    "print(f'최종상태 : 손실 : {history[-1,3]:.5f}  정확도 : {history[-1,4]:.5f}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739157014647,
     "user": {
      "displayName": "김려은",
      "userId": "11330255083274715375"
     },
     "user_tz": -540
    },
    "id": "4mqwP6qm0UHB",
    "outputId": "650049d3-6f71-49f4-f1a5-3ac71e5864a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0059 0.9056 0.0885]\n",
      " [0.0069 0.9792 0.0139]\n",
      " [0.9452 0.0548 0.    ]\n",
      " [0.     0.0404 0.9596]\n",
      " [0.0001 0.1743 0.8256]]\n"
     ]
    }
   ],
   "source": [
    "# 패턴 3 모델의 출력값\n",
    "w = outputs[:5,:].data.numpy()\n",
    "print(w)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ROCKY_4_py3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
